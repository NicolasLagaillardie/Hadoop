2018-09-17 15:02:29,759 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = lag-Predator-G3-571/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.9.1
STARTUP_MSG:   classpath = /home/lag/Downloads/hadoop-2.9.1/etc/hadoop:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/activation-1.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/xz-1.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/asm-3.2.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/hadoop-auth-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/jettison-1.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/junit-4.11.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/avro-1.7.7.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/hadoop-annotations-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/jsch-0.1.54.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1-tests.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/hadoop-nfs-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1-tests.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1-tests.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1-tests.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1-tests.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/fst-2.50.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-core-1.8.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-1.7.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/curator-framework-2.7.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/jsp-api-2.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/gson-2.2.4.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/curator-recipes-2.7.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/avro-1.7.7.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/xmlenc-0.52.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/jcip-annotations-1.0-1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/commons-digester-1.8.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/commons-net-3.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/json-smart-1.3.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/jsch-0.1.54.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-client-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-router-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-common-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-registry-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-api-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-common-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/avro-1.7.7.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/snappy-java-1.0.5.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1-tests.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.9.1.jar:/home/lag/Downloads/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.9.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e30710aea4e6e55e69372929106cf119af06fd0e; compiled by 'root' on 2018-04-16T09:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-17 15:02:29,764 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-17 15:02:29,767 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2018-09-17 15:02:29,860 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-17 15:02:29,982 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-09-17 15:02:29,982 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2018-09-17 15:02:29,997 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2018-09-17 15:02:29,999 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2018-09-17 15:02:30,091 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-09-17 15:02:30,104 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2018-09-17 15:02:30,136 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-17 15:02:30,142 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-17 15:02:30,150 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2018-09-17 15:02:30,160 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-17 15:02:30,162 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2018-09-17 15:02:30,162 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-17 15:02:30,162 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-17 15:02:30,238 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2018-09-17 15:02:30,238 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2018-09-17 15:02:30,250 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2018-09-17 15:02:30,250 INFO org.mortbay.log: jetty-6.1.26
2018-09-17 15:02:30,385 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2018-09-17 15:02:30,404 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-17 15:02:30,404 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-17 15:02:30,432 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2018-09-17 15:02:30,437 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2018-09-17 15:02:30,437 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2018-09-17 15:02:30,438 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2018-09-17 15:02:30,441 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = lag (auth:SIMPLE)
2018-09-17 15:02:30,441 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2018-09-17 15:02:30,441 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2018-09-17 15:02:30,441 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2018-09-17 15:02:30,457 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-09-17 15:02:30,466 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2018-09-17 15:02:30,466 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2018-09-17 15:02:30,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2018-09-17 15:02:30,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2018 Sep 17 15:02:30
2018-09-17 15:02:30,469 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2018-09-17 15:02:30,469 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-17 15:02:30,470 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2018-09-17 15:02:30,470 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2018-09-17 15:02:30,478 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2018-09-17 15:02:30,478 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS
2018-09-17 15:02:30,481 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2018-09-17 15:02:30,481 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2018-09-17 15:02:30,481 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2018-09-17 15:02:30,481 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2018-09-17 15:02:30,481 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2018-09-17 15:02:30,481 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2018-09-17 15:02:30,481 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2018-09-17 15:02:30,481 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2018-09-17 15:02:30,481 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2018-09-17 15:02:30,481 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2018-09-17 15:02:30,481 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2018-09-17 15:02:30,482 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2018-09-17 15:02:30,527 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2018-09-17 15:02:30,527 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-17 15:02:30,527 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2018-09-17 15:02:30,527 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2018-09-17 15:02:30,527 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2018-09-17 15:02:30,528 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2018-09-17 15:02:30,528 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2018-09-17 15:02:30,531 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false
2018-09-17 15:02:30,534 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2018-09-17 15:02:30,534 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-17 15:02:30,534 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2018-09-17 15:02:30,534 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2018-09-17 15:02:30,536 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2018-09-17 15:02:30,536 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2018-09-17 15:02:30,536 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2018-09-17 15:02:30,539 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2018-09-17 15:02:30,539 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2018-09-17 15:02:30,540 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2018-09-17 15:02:30,540 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-17 15:02:30,541 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2018-09-17 15:02:30,541 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2018-09-17 15:02:30,586 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-lag/dfs/name/in_use.lock acquired by nodename 6003@lag-Predator-G3-571
2018-09-17 15:02:30,597 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-lag/dfs/name/current
2018-09-17 15:02:30,597 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2018-09-17 15:02:30,597 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2018-09-17 15:02:30,651 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2018-09-17 15:02:30,666 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2018-09-17 15:02:30,666 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000
2018-09-17 15:02:30,668 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2018-09-17 15:02:30,669 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2018-09-17 15:02:30,801 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2018-09-17 15:02:30,801 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 259 msecs
2018-09-17 15:02:30,898 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2018-09-17 15:02:30,901 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-09-17 15:02:30,907 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2018-09-17 15:02:31,000 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2018-09-17 15:02:31,011 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2018-09-17 15:02:31,017 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues
2018-09-17 15:02:31,017 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2018-09-17 15:02:31,017 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2018-09-17 15:02:31,017 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2018-09-17 15:02:31,025 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2018-09-17 15:02:31,025 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2018-09-17 15:02:31,025 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2018-09-17 15:02:31,025 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2018-09-17 15:02:31,025 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2018-09-17 15:02:31,025 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 8 msec
2018-09-17 15:02:31,036 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-17 15:02:31,036 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2018-09-17 15:02:31,038 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2018-09-17 15:02:31,040 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2018-09-17 15:02:31,040 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2018-09-17 15:02:31,042 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 2 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2018-09-17 15:02:31,044 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2018-09-17 15:02:44,696 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=6890edfd-41c6-45f1-a056-1ab0ba514729, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-632e0267-8d2b-4c7d-af4e-fb575fbab7ea;nsid=1580621252;c=1537189322428) storage 6890edfd-41c6-45f1-a056-1ab0ba514729
2018-09-17 15:02:44,697 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2018-09-17 15:02:44,698 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 6890edfd-41c6-45f1-a056-1ab0ba514729 (127.0.0.1:50010).
2018-09-17 15:02:44,738 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-0cea8a12-cba1-47e4-b8a6-f8c88d839709 for DN 127.0.0.1:50010
2018-09-17 15:02:44,762 INFO BlockStateChange: BLOCK* processReport 0x36673a0a8e9561f5: Processing first storage report for DS-0cea8a12-cba1-47e4-b8a6-f8c88d839709 from datanode 6890edfd-41c6-45f1-a056-1ab0ba514729
2018-09-17 15:02:44,763 INFO BlockStateChange: BLOCK* processReport 0x36673a0a8e9561f5: from storage DS-0cea8a12-cba1-47e4-b8a6-f8c88d839709 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=6890edfd-41c6-45f1-a056-1ab0ba514729, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-632e0267-8d2b-4c7d-af4e-fb575fbab7ea;nsid=1580621252;c=1537189322428), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2018-09-17 15:05:46,084 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 54 
2018-09-17 15:07:05,898 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 3 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 56 
2018-09-17 15:09:23,102 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 58 
2018-09-17 15:09:23,158 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=127.0.0.1:50010 for /user/Lag/hadoop/capacity-scheduler.xml._COPYING_
2018-09-17 15:09:23,268 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741825_1001 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/hadoop/capacity-scheduler.xml._COPYING_
2018-09-17 15:09:23,672 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/capacity-scheduler.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:23,695 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=127.0.0.1:50010 for /user/Lag/hadoop/configuration.xsl._COPYING_
2018-09-17 15:09:23,705 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/configuration.xsl._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:23,728 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=127.0.0.1:50010 for /user/Lag/hadoop/container-executor.cfg._COPYING_
2018-09-17 15:09:23,744 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/container-executor.cfg._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:23,770 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=127.0.0.1:50010 for /user/Lag/hadoop/core-site.xml._COPYING_
2018-09-17 15:09:23,785 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/core-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:23,808 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=127.0.0.1:50010 for /user/Lag/hadoop/hadoop-env.cmd._COPYING_
2018-09-17 15:09:23,824 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/hadoop-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:23,849 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=127.0.0.1:50010 for /user/Lag/hadoop/hadoop-env.sh._COPYING_
2018-09-17 15:09:23,865 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/hadoop-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:23,889 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=127.0.0.1:50010 for /user/Lag/hadoop/hadoop-metrics.properties._COPYING_
2018-09-17 15:09:24,077 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/hadoop-metrics.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:24,102 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008, replicas=127.0.0.1:50010 for /user/Lag/hadoop/hadoop-metrics2.properties._COPYING_
2018-09-17 15:09:24,114 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/hadoop-metrics2.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:24,131 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009, replicas=127.0.0.1:50010 for /user/Lag/hadoop/hadoop-policy.xml._COPYING_
2018-09-17 15:09:24,143 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/hadoop-policy.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:24,161 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010, replicas=127.0.0.1:50010 for /user/Lag/hadoop/hdfs-site.xml._COPYING_
2018-09-17 15:09:24,173 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/hdfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:24,191 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011, replicas=127.0.0.1:50010 for /user/Lag/hadoop/httpfs-env.sh._COPYING_
2018-09-17 15:09:24,202 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/httpfs-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:24,220 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012, replicas=127.0.0.1:50010 for /user/Lag/hadoop/httpfs-log4j.properties._COPYING_
2018-09-17 15:09:24,231 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/httpfs-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:24,248 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013, replicas=127.0.0.1:50010 for /user/Lag/hadoop/httpfs-signature.secret._COPYING_
2018-09-17 15:09:24,263 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/httpfs-signature.secret._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:24,286 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014, replicas=127.0.0.1:50010 for /user/Lag/hadoop/httpfs-site.xml._COPYING_
2018-09-17 15:09:24,301 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/httpfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:24,324 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015, replicas=127.0.0.1:50010 for /user/Lag/hadoop/kms-acls.xml._COPYING_
2018-09-17 15:09:24,341 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/kms-acls.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:24,369 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016, replicas=127.0.0.1:50010 for /user/Lag/hadoop/kms-env.sh._COPYING_
2018-09-17 15:09:24,382 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/kms-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:24,401 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017, replicas=127.0.0.1:50010 for /user/Lag/hadoop/kms-log4j.properties._COPYING_
2018-09-17 15:09:24,414 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/kms-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:24,433 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018, replicas=127.0.0.1:50010 for /user/Lag/hadoop/kms-site.xml._COPYING_
2018-09-17 15:09:24,446 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/kms-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:24,463 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019, replicas=127.0.0.1:50010 for /user/Lag/hadoop/log4j.properties._COPYING_
2018-09-17 15:09:24,476 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741843_1019 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/hadoop/log4j.properties._COPYING_
2018-09-17 15:09:24,879 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:24,899 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020, replicas=127.0.0.1:50010 for /user/Lag/hadoop/mapred-env.cmd._COPYING_
2018-09-17 15:09:24,908 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/mapred-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:24,922 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021, replicas=127.0.0.1:50010 for /user/Lag/hadoop/mapred-env.sh._COPYING_
2018-09-17 15:09:24,929 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/mapred-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:24,951 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022, replicas=127.0.0.1:50010 for /user/Lag/hadoop/mapred-queues.xml.template._COPYING_
2018-09-17 15:09:24,964 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/mapred-queues.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:24,985 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023, replicas=127.0.0.1:50010 for /user/Lag/hadoop/mapred-site.xml.template._COPYING_
2018-09-17 15:09:24,999 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/mapred-site.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:25,016 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024, replicas=127.0.0.1:50010 for /user/Lag/hadoop/slaves._COPYING_
2018-09-17 15:09:25,030 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/slaves._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:25,050 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025, replicas=127.0.0.1:50010 for /user/Lag/hadoop/ssl-client.xml.example._COPYING_
2018-09-17 15:09:25,060 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/ssl-client.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:25,076 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741850_1026, replicas=127.0.0.1:50010 for /user/Lag/hadoop/ssl-server.xml.example._COPYING_
2018-09-17 15:09:25,086 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/ssl-server.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:25,101 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741851_1027, replicas=127.0.0.1:50010 for /user/Lag/hadoop/yarn-env.cmd._COPYING_
2018-09-17 15:09:25,110 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/yarn-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:25,125 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741852_1028, replicas=127.0.0.1:50010 for /user/Lag/hadoop/yarn-env.sh._COPYING_
2018-09-17 15:09:25,135 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/yarn-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:09:25,151 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741853_1029, replicas=127.0.0.1:50010 for /user/Lag/hadoop/yarn-site.xml._COPYING_
2018-09-17 15:09:25,161 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/yarn-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_274734246_1
2018-09-17 15:11:10,139 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 182 Total time for transactions(ms): 18 Number of transactions batched in Syncs: 29 Number of syncs: 150 SyncTimes(ms): 526 
2018-09-17 15:11:10,261 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#4 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:36442: java.io.FileNotFoundException: Path is not a file: /user/Lag/hadoop
2018-09-17 15:14:16,354 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 185 Total time for transactions(ms): 18 Number of transactions batched in Syncs: 32 Number of syncs: 153 SyncTimes(ms): 543 
2018-09-17 15:14:16,400 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741854_1030, replicas=127.0.0.1:50010 for /user/lag/input/capacity-scheduler.xml._COPYING_
2018-09-17 15:14:16,466 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/capacity-scheduler.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:16,538 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741855_1031, replicas=127.0.0.1:50010 for /user/lag/input/configuration.xsl._COPYING_
2018-09-17 15:14:16,550 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/configuration.xsl._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:16,568 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741856_1032, replicas=127.0.0.1:50010 for /user/lag/input/container-executor.cfg._COPYING_
2018-09-17 15:14:16,580 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/container-executor.cfg._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:16,599 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741857_1033, replicas=127.0.0.1:50010 for /user/lag/input/core-site.xml._COPYING_
2018-09-17 15:14:16,610 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/core-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:16,630 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741858_1034, replicas=127.0.0.1:50010 for /user/lag/input/hadoop-env.cmd._COPYING_
2018-09-17 15:14:16,640 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:16,653 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741859_1035, replicas=127.0.0.1:50010 for /user/lag/input/hadoop-env.sh._COPYING_
2018-09-17 15:14:16,661 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:16,674 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741860_1036, replicas=127.0.0.1:50010 for /user/lag/input/hadoop-metrics.properties._COPYING_
2018-09-17 15:14:16,683 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop-metrics.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:16,696 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741861_1037, replicas=127.0.0.1:50010 for /user/lag/input/hadoop-metrics2.properties._COPYING_
2018-09-17 15:14:16,704 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop-metrics2.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:16,718 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741862_1038, replicas=127.0.0.1:50010 for /user/lag/input/hadoop-policy.xml._COPYING_
2018-09-17 15:14:16,726 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop-policy.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:16,739 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741863_1039, replicas=127.0.0.1:50010 for /user/lag/input/hdfs-site.xml._COPYING_
2018-09-17 15:14:16,748 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hdfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:16,761 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741864_1040, replicas=127.0.0.1:50010 for /user/lag/input/httpfs-env.sh._COPYING_
2018-09-17 15:14:16,769 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/httpfs-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:16,782 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741865_1041, replicas=127.0.0.1:50010 for /user/lag/input/httpfs-log4j.properties._COPYING_
2018-09-17 15:14:16,790 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/httpfs-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:16,803 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741866_1042, replicas=127.0.0.1:50010 for /user/lag/input/httpfs-signature.secret._COPYING_
2018-09-17 15:14:16,811 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/httpfs-signature.secret._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:16,823 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741867_1043, replicas=127.0.0.1:50010 for /user/lag/input/httpfs-site.xml._COPYING_
2018-09-17 15:14:16,838 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/httpfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:16,859 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741868_1044, replicas=127.0.0.1:50010 for /user/lag/input/kms-acls.xml._COPYING_
2018-09-17 15:14:16,873 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/kms-acls.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:16,898 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741869_1045, replicas=127.0.0.1:50010 for /user/lag/input/kms-env.sh._COPYING_
2018-09-17 15:14:16,925 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741869_1045 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/lag/input/kms-env.sh._COPYING_
2018-09-17 15:14:17,327 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/kms-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:17,351 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741870_1046, replicas=127.0.0.1:50010 for /user/lag/input/kms-log4j.properties._COPYING_
2018-09-17 15:14:17,364 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/kms-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:17,381 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741871_1047, replicas=127.0.0.1:50010 for /user/lag/input/kms-site.xml._COPYING_
2018-09-17 15:14:17,389 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741871_1047 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/lag/input/kms-site.xml._COPYING_
2018-09-17 15:14:17,791 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/kms-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:17,813 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741872_1048, replicas=127.0.0.1:50010 for /user/lag/input/log4j.properties._COPYING_
2018-09-17 15:14:17,997 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:18,018 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741873_1049, replicas=127.0.0.1:50010 for /user/lag/input/mapred-env.cmd._COPYING_
2018-09-17 15:14:18,033 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/mapred-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:18,054 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741874_1050, replicas=127.0.0.1:50010 for /user/lag/input/mapred-env.sh._COPYING_
2018-09-17 15:14:18,065 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/mapred-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:18,078 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741875_1051, replicas=127.0.0.1:50010 for /user/lag/input/mapred-queues.xml.template._COPYING_
2018-09-17 15:14:18,088 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/mapred-queues.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:18,100 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741876_1052, replicas=127.0.0.1:50010 for /user/lag/input/mapred-site.xml.template._COPYING_
2018-09-17 15:14:18,108 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/mapred-site.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:18,120 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741877_1053, replicas=127.0.0.1:50010 for /user/lag/input/slaves._COPYING_
2018-09-17 15:14:18,128 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/slaves._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:18,140 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741878_1054, replicas=127.0.0.1:50010 for /user/lag/input/ssl-client.xml.example._COPYING_
2018-09-17 15:14:18,148 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/ssl-client.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:18,160 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741879_1055, replicas=127.0.0.1:50010 for /user/lag/input/ssl-server.xml.example._COPYING_
2018-09-17 15:14:18,167 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/ssl-server.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:18,180 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741880_1056, replicas=127.0.0.1:50010 for /user/lag/input/yarn-env.cmd._COPYING_
2018-09-17 15:14:18,187 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/yarn-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:18,202 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741881_1057, replicas=127.0.0.1:50010 for /user/lag/input/yarn-env.sh._COPYING_
2018-09-17 15:14:18,215 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/yarn-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:18,234 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741882_1058, replicas=127.0.0.1:50010 for /user/lag/input/yarn-site.xml._COPYING_
2018-09-17 15:14:18,247 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/yarn-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_1709372373_1
2018-09-17 15:14:31,916 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741883_1059, replicas=127.0.0.1:50010 for /user/lag/grep-temp-1549789716/_temporary/0/_temporary/attempt_local1233566278_0001_r_000000_0/part-r-00000
2018-09-17 15:14:31,938 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/grep-temp-1549789716/_temporary/0/_temporary/attempt_local1233566278_0001_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_-901778237_1
2018-09-17 15:14:31,995 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/grep-temp-1549789716/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-901778237_1
2018-09-17 15:14:32,428 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741884_1060, replicas=127.0.0.1:50010 for /user/lag/output/_temporary/0/_temporary/attempt_local1167308326_0002_r_000000_0/part-r-00000
2018-09-17 15:14:32,438 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/output/_temporary/0/_temporary/attempt_local1167308326_0002_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_-901778237_1
2018-09-17 15:14:32,475 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-901778237_1
2018-09-17 16:15:44,772 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2018-09-17 16:15:44,776 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at lag-Predator-G3-571/127.0.1.1
************************************************************/
2018-09-24 15:57:52,263 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = lag-Predator-G3-571/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.9.1
STARTUP_MSG:   classpath = /home/lag/BD/Hadoop/hadoop-2.9.1/etc/hadoop:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/activation-1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xz-1.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/asm-3.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-auth-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jettison-1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/junit-4.11.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/avro-1.7.7.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-annotations-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsch-0.1.54.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-nfs-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/fst-2.50.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-core-1.8.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-1.7.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-framework-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsp-api-2.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/gson-2.2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-recipes-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/avro-1.7.7.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xmlenc-0.52.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jcip-annotations-1.0-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-digester-1.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-net-3.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-smart-1.3.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsch-0.1.54.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-client-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-router-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-common-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-registry-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-api-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-common-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/avro-1.7.7.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/snappy-java-1.0.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.9.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e30710aea4e6e55e69372929106cf119af06fd0e; compiled by 'root' on 2018-04-16T09:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-24 15:57:52,285 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-24 15:57:52,289 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2018-09-24 15:57:52,479 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-24 15:57:52,674 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-09-24 15:57:52,674 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2018-09-24 15:57:52,723 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2018-09-24 15:57:52,726 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2018-09-24 15:57:52,842 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-09-24 15:57:52,888 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2018-09-24 15:57:52,970 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-24 15:57:52,975 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-24 15:57:52,986 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2018-09-24 15:57:52,997 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-24 15:57:52,998 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2018-09-24 15:57:52,999 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-24 15:57:52,999 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-24 15:57:53,197 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2018-09-24 15:57:53,198 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2018-09-24 15:57:53,217 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2018-09-24 15:57:53,217 INFO org.mortbay.log: jetty-6.1.26
2018-09-24 15:57:53,540 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2018-09-24 15:57:53,563 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-24 15:57:53,563 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-24 15:57:53,623 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2018-09-24 15:57:53,629 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2018-09-24 15:57:53,630 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2018-09-24 15:57:53,632 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2018-09-24 15:57:53,635 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = lag (auth:SIMPLE)
2018-09-24 15:57:53,635 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2018-09-24 15:57:53,635 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2018-09-24 15:57:53,635 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2018-09-24 15:57:53,655 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-09-24 15:57:53,667 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2018-09-24 15:57:53,667 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2018-09-24 15:57:53,669 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2018-09-24 15:57:53,669 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2018 Sep 24 15:57:53
2018-09-24 15:57:53,670 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2018-09-24 15:57:53,670 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-24 15:57:53,671 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2018-09-24 15:57:53,671 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2018-09-24 15:57:53,681 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2018-09-24 15:57:53,681 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS
2018-09-24 15:57:53,684 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2018-09-24 15:57:53,684 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2018-09-24 15:57:53,684 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2018-09-24 15:57:53,684 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2018-09-24 15:57:53,684 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2018-09-24 15:57:53,684 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2018-09-24 15:57:53,685 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2018-09-24 15:57:53,685 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2018-09-24 15:57:53,685 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2018-09-24 15:57:53,685 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2018-09-24 15:57:53,685 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2018-09-24 15:57:53,686 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2018-09-24 15:57:53,743 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2018-09-24 15:57:53,743 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-24 15:57:53,743 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2018-09-24 15:57:53,743 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2018-09-24 15:57:53,744 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2018-09-24 15:57:53,744 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2018-09-24 15:57:53,744 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2018-09-24 15:57:53,747 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false
2018-09-24 15:57:53,750 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2018-09-24 15:57:53,750 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-24 15:57:53,751 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2018-09-24 15:57:53,751 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2018-09-24 15:57:53,753 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2018-09-24 15:57:53,753 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2018-09-24 15:57:53,753 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2018-09-24 15:57:53,756 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2018-09-24 15:57:53,756 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2018-09-24 15:57:53,758 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2018-09-24 15:57:53,758 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-24 15:57:53,758 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2018-09-24 15:57:53,758 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2018-09-24 15:57:53,778 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-lag/dfs/name/in_use.lock acquired by nodename 7179@lag-Predator-G3-571
2018-09-24 15:57:53,791 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-lag/dfs/name/current
2018-09-24 15:57:53,791 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2018-09-24 15:57:53,792 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2018-09-24 15:57:53,854 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2018-09-24 15:57:53,872 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2018-09-24 15:57:53,872 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000
2018-09-24 15:57:53,875 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2018-09-24 15:57:53,876 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2018-09-24 15:57:54,041 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2018-09-24 15:57:54,041 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 281 msecs
2018-09-24 15:57:54,356 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2018-09-24 15:57:54,372 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-09-24 15:57:54,384 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for active state
2018-09-24 15:57:54,385 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1, 1
2018-09-24 15:57:54,420 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 101 
2018-09-24 15:57:54,421 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000001-0000000000000000002
2018-09-24 15:57:54,446 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: FSEditLogAsync was interrupted, exiting
2018-09-24 15:57:54,464 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for active state
2018-09-24 15:57:54,464 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state
2018-09-24 15:57:54,466 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2018-09-24 15:57:54,566 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2018-09-24 15:57:54,567 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2018-09-24 15:57:54,568 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2018-09-24 15:57:54,580 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
java.net.BindException: Problem binding to [localhost:9000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:824)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:735)
	at org.apache.hadoop.ipc.Server.bind(Server.java:561)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1037)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2738)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:958)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:420)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:341)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:800)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:431)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:803)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:730)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:953)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:932)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1741)
Caused by: java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.apache.hadoop.ipc.Server.bind(Server.java:544)
	... 13 more
2018-09-24 15:57:54,587 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1: java.net.BindException: Problem binding to [localhost:9000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
2018-09-24 15:57:54,590 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at lag-Predator-G3-571/127.0.1.1
************************************************************/
2018-09-24 16:00:44,159 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = lag-Predator-G3-571/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.9.1
STARTUP_MSG:   classpath = /home/lag/BD/Hadoop/hadoop-2.9.1/etc/hadoop:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/activation-1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xz-1.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/asm-3.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-auth-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jettison-1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/junit-4.11.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/avro-1.7.7.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-annotations-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsch-0.1.54.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-nfs-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/fst-2.50.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-core-1.8.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-1.7.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-framework-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsp-api-2.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/gson-2.2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-recipes-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/avro-1.7.7.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xmlenc-0.52.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jcip-annotations-1.0-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-digester-1.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-net-3.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-smart-1.3.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsch-0.1.54.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-client-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-router-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-common-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-registry-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-api-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-common-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/avro-1.7.7.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/snappy-java-1.0.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.9.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e30710aea4e6e55e69372929106cf119af06fd0e; compiled by 'root' on 2018-04-16T09:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-24 16:00:44,166 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-24 16:00:44,169 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2018-09-24 16:00:44,279 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-24 16:00:44,347 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-09-24 16:00:44,347 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2018-09-24 16:00:44,364 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2018-09-24 16:00:44,366 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2018-09-24 16:00:44,477 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-09-24 16:00:44,488 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2018-09-24 16:00:44,526 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-24 16:00:44,532 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-24 16:00:44,542 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2018-09-24 16:00:44,552 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-24 16:00:44,554 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2018-09-24 16:00:44,554 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-24 16:00:44,554 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-24 16:00:44,648 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2018-09-24 16:00:44,648 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2018-09-24 16:00:44,658 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2018-09-24 16:00:44,658 INFO org.mortbay.log: jetty-6.1.26
2018-09-24 16:00:44,812 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2018-09-24 16:00:44,831 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-24 16:00:44,831 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-24 16:00:44,864 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2018-09-24 16:00:44,870 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2018-09-24 16:00:44,871 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2018-09-24 16:00:44,872 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2018-09-24 16:00:44,876 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = lag (auth:SIMPLE)
2018-09-24 16:00:44,876 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2018-09-24 16:00:44,876 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2018-09-24 16:00:44,876 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2018-09-24 16:00:44,895 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-09-24 16:00:44,907 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2018-09-24 16:00:44,907 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2018-09-24 16:00:44,909 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2018-09-24 16:00:44,910 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2018 Sep 24 16:00:44
2018-09-24 16:00:44,911 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2018-09-24 16:00:44,911 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-24 16:00:44,912 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2018-09-24 16:00:44,912 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2018-09-24 16:00:44,922 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2018-09-24 16:00:44,922 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS
2018-09-24 16:00:44,925 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2018-09-24 16:00:44,925 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2018-09-24 16:00:44,925 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2018-09-24 16:00:44,925 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2018-09-24 16:00:44,925 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2018-09-24 16:00:44,925 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2018-09-24 16:00:44,925 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2018-09-24 16:00:44,925 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2018-09-24 16:00:44,925 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2018-09-24 16:00:44,925 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2018-09-24 16:00:44,925 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2018-09-24 16:00:44,926 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2018-09-24 16:00:44,979 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2018-09-24 16:00:44,979 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-24 16:00:44,979 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2018-09-24 16:00:44,979 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2018-09-24 16:00:44,980 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2018-09-24 16:00:44,980 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2018-09-24 16:00:44,980 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2018-09-24 16:00:44,984 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false
2018-09-24 16:00:44,987 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2018-09-24 16:00:44,988 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-24 16:00:44,988 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2018-09-24 16:00:44,988 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2018-09-24 16:00:44,990 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2018-09-24 16:00:44,990 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2018-09-24 16:00:44,990 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2018-09-24 16:00:44,993 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2018-09-24 16:00:44,993 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2018-09-24 16:00:44,995 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2018-09-24 16:00:44,995 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-24 16:00:44,995 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2018-09-24 16:00:44,995 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2018-09-24 16:00:45,017 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-lag/dfs/name/in_use.lock acquired by nodename 7867@lag-Predator-G3-571
2018-09-24 16:00:45,029 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-lag/dfs/name/current
2018-09-24 16:00:45,034 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2018-09-24 16:00:45,096 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2018-09-24 16:00:45,114 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2018-09-24 16:00:45,114 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000
2018-09-24 16:00:45,117 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@73163d48 expecting start txid #1
2018-09-24 16:00:45,117 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000001-0000000000000000002
2018-09-24 16:00:45,120 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream '/tmp/hadoop-lag/dfs/name/current/edits_0000000000000000001-0000000000000000002' to transaction ID 1
2018-09-24 16:00:45,129 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000001-0000000000000000002 of size 42 edits # 2 loaded in 0 seconds
2018-09-24 16:00:45,130 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2018-09-24 16:00:45,130 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2018-09-24 16:00:45,265 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2018-09-24 16:00:45,265 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 268 msecs
2018-09-24 16:00:45,384 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2018-09-24 16:00:45,389 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-09-24 16:00:45,396 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2018-09-24 16:00:45,433 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2018-09-24 16:00:45,448 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2018-09-24 16:00:45,455 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues
2018-09-24 16:00:45,455 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2018-09-24 16:00:45,455 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2018-09-24 16:00:45,455 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2018-09-24 16:00:45,462 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2018-09-24 16:00:45,462 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2018-09-24 16:00:45,462 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2018-09-24 16:00:45,462 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2018-09-24 16:00:45,462 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2018-09-24 16:00:45,462 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 7 msec
2018-09-24 16:00:45,473 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-24 16:00:45,473 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2018-09-24 16:00:45,475 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2018-09-24 16:00:45,477 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2018-09-24 16:00:45,477 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2018-09-24 16:00:45,479 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 3 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2018-09-24 16:00:45,481 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2018-09-24 16:00:50,267 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=916da39d-1bd1-46c6-97b7-33d886cf34c5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-d46bcbf5-42c2-408a-a95e-5958aa5ec944;nsid=1438813254;c=1537797164589) storage 916da39d-1bd1-46c6-97b7-33d886cf34c5
2018-09-24 16:00:50,268 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2018-09-24 16:00:50,268 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 916da39d-1bd1-46c6-97b7-33d886cf34c5 (127.0.0.1:50010).
2018-09-24 16:00:50,330 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-d1df1055-40f5-4056-920c-be574d20c567 for DN 127.0.0.1:50010
2018-09-24 16:00:50,359 INFO BlockStateChange: BLOCK* processReport 0x842dda8b4e77d9dc: Processing first storage report for DS-d1df1055-40f5-4056-920c-be574d20c567 from datanode 916da39d-1bd1-46c6-97b7-33d886cf34c5
2018-09-24 16:00:50,360 INFO BlockStateChange: BLOCK* processReport 0x842dda8b4e77d9dc: from storage DS-d1df1055-40f5-4056-920c-be574d20c567 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=916da39d-1bd1-46c6-97b7-33d886cf34c5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-d46bcbf5-42c2-408a-a95e-5958aa5ec944;nsid=1438813254;c=1537797164589), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2018-09-24 16:01:50,672 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 3 Total time for transactions(ms): 14 Number of transactions batched in Syncs: 2 Number of syncs: 3 SyncTimes(ms): 47 
2018-09-24 16:06:23,861 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 5 Total time for transactions(ms): 14 Number of transactions batched in Syncs: 2 Number of syncs: 5 SyncTimes(ms): 52 
2018-09-24 16:06:23,926 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=127.0.0.1:50010 for /hadoop/capacity-scheduler.xml._COPYING_
2018-09-24 16:06:24,077 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741825_1001 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /hadoop/capacity-scheduler.xml._COPYING_
2018-09-24 16:06:24,482 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/capacity-scheduler.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:24,505 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=127.0.0.1:50010 for /hadoop/configuration.xsl._COPYING_
2018-09-24 16:06:24,521 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/configuration.xsl._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:24,535 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=127.0.0.1:50010 for /hadoop/container-executor.cfg._COPYING_
2018-09-24 16:06:24,544 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/container-executor.cfg._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:24,558 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=127.0.0.1:50010 for /hadoop/core-site.xml._COPYING_
2018-09-24 16:06:24,567 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/core-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:24,581 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=127.0.0.1:50010 for /hadoop/hadoop-env.cmd._COPYING_
2018-09-24 16:06:24,591 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/hadoop-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:24,605 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=127.0.0.1:50010 for /hadoop/hadoop-env.sh._COPYING_
2018-09-24 16:06:24,614 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/hadoop-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:24,627 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=127.0.0.1:50010 for /hadoop/hadoop-metrics.properties._COPYING_
2018-09-24 16:06:24,637 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/hadoop-metrics.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:24,650 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008, replicas=127.0.0.1:50010 for /hadoop/hadoop-metrics2.properties._COPYING_
2018-09-24 16:06:24,659 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741832_1008 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /hadoop/hadoop-metrics2.properties._COPYING_
2018-09-24 16:06:25,061 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/hadoop-metrics2.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:25,097 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009, replicas=127.0.0.1:50010 for /hadoop/hadoop-policy.xml._COPYING_
2018-09-24 16:06:25,109 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/hadoop-policy.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:25,127 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010, replicas=127.0.0.1:50010 for /hadoop/hdfs-site.xml._COPYING_
2018-09-24 16:06:25,140 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/hdfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:25,167 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011, replicas=127.0.0.1:50010 for /hadoop/httpfs-env.sh._COPYING_
2018-09-24 16:06:25,181 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/httpfs-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:25,200 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012, replicas=127.0.0.1:50010 for /hadoop/httpfs-log4j.properties._COPYING_
2018-09-24 16:06:25,213 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/httpfs-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:25,232 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013, replicas=127.0.0.1:50010 for /hadoop/httpfs-signature.secret._COPYING_
2018-09-24 16:06:25,244 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741837_1013 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /hadoop/httpfs-signature.secret._COPYING_
2018-09-24 16:06:25,648 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/httpfs-signature.secret._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:25,663 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014, replicas=127.0.0.1:50010 for /hadoop/httpfs-site.xml._COPYING_
2018-09-24 16:06:25,675 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741838_1014 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /hadoop/httpfs-site.xml._COPYING_
2018-09-24 16:06:26,078 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/httpfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:26,102 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015, replicas=127.0.0.1:50010 for /hadoop/kms-acls.xml._COPYING_
2018-09-24 16:06:26,118 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/kms-acls.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:26,143 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016, replicas=127.0.0.1:50010 for /hadoop/kms-env.sh._COPYING_
2018-09-24 16:06:26,159 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/kms-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:26,180 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017, replicas=127.0.0.1:50010 for /hadoop/kms-log4j.properties._COPYING_
2018-09-24 16:06:26,192 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741841_1017 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /hadoop/kms-log4j.properties._COPYING_
2018-09-24 16:06:26,594 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/kms-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:26,616 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018, replicas=127.0.0.1:50010 for /hadoop/kms-site.xml._COPYING_
2018-09-24 16:06:26,632 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/kms-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:26,653 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019, replicas=127.0.0.1:50010 for /hadoop/log4j.properties._COPYING_
2018-09-24 16:06:26,669 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:26,691 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020, replicas=127.0.0.1:50010 for /hadoop/mapred-env.cmd._COPYING_
2018-09-24 16:06:26,706 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/mapred-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:26,728 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021, replicas=127.0.0.1:50010 for /hadoop/mapred-env.sh._COPYING_
2018-09-24 16:06:26,742 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/mapred-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:26,759 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022, replicas=127.0.0.1:50010 for /hadoop/mapred-queues.xml.template._COPYING_
2018-09-24 16:06:26,770 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/mapred-queues.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:26,790 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023, replicas=127.0.0.1:50010 for /hadoop/mapred-site.xml.template._COPYING_
2018-09-24 16:06:26,803 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741847_1023 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /hadoop/mapred-site.xml.template._COPYING_
2018-09-24 16:06:27,206 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/mapred-site.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:27,230 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024, replicas=127.0.0.1:50010 for /hadoop/slaves._COPYING_
2018-09-24 16:06:27,246 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/slaves._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:27,270 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025, replicas=127.0.0.1:50010 for /hadoop/ssl-client.xml.example._COPYING_
2018-09-24 16:06:27,286 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/ssl-client.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:27,311 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741850_1026, replicas=127.0.0.1:50010 for /hadoop/ssl-server.xml.example._COPYING_
2018-09-24 16:06:27,324 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/ssl-server.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:27,343 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741851_1027, replicas=127.0.0.1:50010 for /hadoop/yarn-env.cmd._COPYING_
2018-09-24 16:06:27,354 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/yarn-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:27,373 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741852_1028, replicas=127.0.0.1:50010 for /hadoop/yarn-env.sh._COPYING_
2018-09-24 16:06:27,388 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/yarn-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:27,411 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741853_1029, replicas=127.0.0.1:50010 for /hadoop/yarn-site.xml._COPYING_
2018-09-24 16:06:27,426 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hadoop/yarn-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_857183074_1
2018-09-24 16:06:50,831 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#4 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:57472: java.io.FileNotFoundException: Path is not a file: /hadoop
2018-09-24 16:06:50,906 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#5 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:57472: java.io.FileNotFoundException: Path is not a file: /user
2018-09-24 16:06:52,106 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/output/_temporary/0/_temporary/attempt_local1836395736_0002_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_2031943810_1
2018-09-24 16:06:52,185 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_2031943810_1
2018-09-24 16:08:12,507 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 198 Total time for transactions(ms): 27 Number of transactions batched in Syncs: 37 Number of syncs: 163 SyncTimes(ms): 596 
2018-09-24 16:08:12,555 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741854_1030, replicas=127.0.0.1:50010 for /Lag/capacity-scheduler.xml._COPYING_
2018-09-24 16:08:12,633 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/capacity-scheduler.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:12,652 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741855_1031, replicas=127.0.0.1:50010 for /Lag/configuration.xsl._COPYING_
2018-09-24 16:08:12,660 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/configuration.xsl._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:12,680 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741856_1032, replicas=127.0.0.1:50010 for /Lag/container-executor.cfg._COPYING_
2018-09-24 16:08:12,694 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/container-executor.cfg._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:12,715 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741857_1033, replicas=127.0.0.1:50010 for /Lag/core-site.xml._COPYING_
2018-09-24 16:08:12,728 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/core-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:12,752 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741858_1034, replicas=127.0.0.1:50010 for /Lag/hadoop-env.cmd._COPYING_
2018-09-24 16:08:12,763 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/hadoop-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:12,784 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741859_1035, replicas=127.0.0.1:50010 for /Lag/hadoop-env.sh._COPYING_
2018-09-24 16:08:12,798 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/hadoop-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:12,819 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741860_1036, replicas=127.0.0.1:50010 for /Lag/hadoop-metrics.properties._COPYING_
2018-09-24 16:08:12,830 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/hadoop-metrics.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:12,843 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741861_1037, replicas=127.0.0.1:50010 for /Lag/hadoop-metrics2.properties._COPYING_
2018-09-24 16:08:12,851 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/hadoop-metrics2.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:12,870 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741862_1038, replicas=127.0.0.1:50010 for /Lag/hadoop-policy.xml._COPYING_
2018-09-24 16:08:12,879 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/hadoop-policy.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:12,892 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741863_1039, replicas=127.0.0.1:50010 for /Lag/hdfs-site.xml._COPYING_
2018-09-24 16:08:12,901 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/hdfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:12,915 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741864_1040, replicas=127.0.0.1:50010 for /Lag/httpfs-env.sh._COPYING_
2018-09-24 16:08:12,923 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/httpfs-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:12,939 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741865_1041, replicas=127.0.0.1:50010 for /Lag/httpfs-log4j.properties._COPYING_
2018-09-24 16:08:12,953 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/httpfs-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:12,971 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741866_1042, replicas=127.0.0.1:50010 for /Lag/httpfs-signature.secret._COPYING_
2018-09-24 16:08:12,983 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/httpfs-signature.secret._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:13,002 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741867_1043, replicas=127.0.0.1:50010 for /Lag/httpfs-site.xml._COPYING_
2018-09-24 16:08:13,015 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/httpfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:13,035 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741868_1044, replicas=127.0.0.1:50010 for /Lag/kms-acls.xml._COPYING_
2018-09-24 16:08:13,048 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/kms-acls.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:13,071 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741869_1045, replicas=127.0.0.1:50010 for /Lag/kms-env.sh._COPYING_
2018-09-24 16:08:13,082 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/kms-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:13,096 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741870_1046, replicas=127.0.0.1:50010 for /Lag/kms-log4j.properties._COPYING_
2018-09-24 16:08:13,109 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/kms-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:13,131 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741871_1047, replicas=127.0.0.1:50010 for /Lag/kms-site.xml._COPYING_
2018-09-24 16:08:13,145 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/kms-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:13,164 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741872_1048, replicas=127.0.0.1:50010 for /Lag/log4j.properties._COPYING_
2018-09-24 16:08:13,177 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:13,195 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741873_1049, replicas=127.0.0.1:50010 for /Lag/mapred-env.cmd._COPYING_
2018-09-24 16:08:13,206 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/mapred-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:13,222 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741874_1050, replicas=127.0.0.1:50010 for /Lag/mapred-env.sh._COPYING_
2018-09-24 16:08:13,233 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/mapred-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:13,250 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741875_1051, replicas=127.0.0.1:50010 for /Lag/mapred-queues.xml.template._COPYING_
2018-09-24 16:08:13,263 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/mapred-queues.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:13,282 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741876_1052, replicas=127.0.0.1:50010 for /Lag/mapred-site.xml.template._COPYING_
2018-09-24 16:08:13,291 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/mapred-site.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:13,305 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741877_1053, replicas=127.0.0.1:50010 for /Lag/slaves._COPYING_
2018-09-24 16:08:13,315 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/slaves._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:13,328 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741878_1054, replicas=127.0.0.1:50010 for /Lag/ssl-client.xml.example._COPYING_
2018-09-24 16:08:13,337 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/ssl-client.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:13,352 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741879_1055, replicas=127.0.0.1:50010 for /Lag/ssl-server.xml.example._COPYING_
2018-09-24 16:08:13,361 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/ssl-server.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:13,552 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741880_1056, replicas=127.0.0.1:50010 for /Lag/yarn-env.cmd._COPYING_
2018-09-24 16:08:13,565 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/yarn-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:13,584 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741881_1057, replicas=127.0.0.1:50010 for /Lag/yarn-env.sh._COPYING_
2018-09-24 16:08:13,597 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/yarn-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:13,616 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741882_1058, replicas=127.0.0.1:50010 for /Lag/yarn-site.xml._COPYING_
2018-09-24 16:08:13,629 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Lag/yarn-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1800379114_1
2018-09-24 16:08:28,556 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741883_1059, replicas=127.0.0.1:50010 for /user/lag/grep-temp-1917940720/_temporary/0/_temporary/attempt_local1425633903_0001_r_000000_0/part-r-00000
2018-09-24 16:08:28,586 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/grep-temp-1917940720/_temporary/0/_temporary/attempt_local1425633903_0001_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_461593478_1
2018-09-24 16:08:28,643 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/grep-temp-1917940720/_SUCCESS is closed by DFSClient_NONMAPREDUCE_461593478_1
2018-09-24 16:09:28,101 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 389 Total time for transactions(ms): 43 Number of transactions batched in Syncs: 72 Number of syncs: 319 SyncTimes(ms): 1120 
2018-09-24 16:09:28,151 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741884_1060, replicas=127.0.0.1:50010 for /user/lag/input/capacity-scheduler.xml._COPYING_
2018-09-24 16:09:28,232 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/capacity-scheduler.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,251 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741885_1061, replicas=127.0.0.1:50010 for /user/lag/input/configuration.xsl._COPYING_
2018-09-24 16:09:28,259 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/configuration.xsl._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,278 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741886_1062, replicas=127.0.0.1:50010 for /user/lag/input/container-executor.cfg._COPYING_
2018-09-24 16:09:28,290 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/container-executor.cfg._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,305 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741887_1063, replicas=127.0.0.1:50010 for /user/lag/input/core-site.xml._COPYING_
2018-09-24 16:09:28,315 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/core-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,332 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741888_1064, replicas=127.0.0.1:50010 for /user/lag/input/hadoop-env.cmd._COPYING_
2018-09-24 16:09:28,341 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,353 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741889_1065, replicas=127.0.0.1:50010 for /user/lag/input/hadoop-env.sh._COPYING_
2018-09-24 16:09:28,362 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,374 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741890_1066, replicas=127.0.0.1:50010 for /user/lag/input/hadoop-metrics.properties._COPYING_
2018-09-24 16:09:28,383 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop-metrics.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,397 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741891_1067, replicas=127.0.0.1:50010 for /user/lag/input/hadoop-metrics2.properties._COPYING_
2018-09-24 16:09:28,409 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop-metrics2.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,429 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741892_1068, replicas=127.0.0.1:50010 for /user/lag/input/hadoop-policy.xml._COPYING_
2018-09-24 16:09:28,442 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop-policy.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,462 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741893_1069, replicas=127.0.0.1:50010 for /user/lag/input/hdfs-site.xml._COPYING_
2018-09-24 16:09:28,658 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hdfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,679 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741894_1070, replicas=127.0.0.1:50010 for /user/lag/input/httpfs-env.sh._COPYING_
2018-09-24 16:09:28,692 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/httpfs-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,711 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741895_1071, replicas=127.0.0.1:50010 for /user/lag/input/httpfs-log4j.properties._COPYING_
2018-09-24 16:09:28,723 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/httpfs-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,743 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741896_1072, replicas=127.0.0.1:50010 for /user/lag/input/httpfs-signature.secret._COPYING_
2018-09-24 16:09:28,755 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/httpfs-signature.secret._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,774 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741897_1073, replicas=127.0.0.1:50010 for /user/lag/input/httpfs-site.xml._COPYING_
2018-09-24 16:09:28,786 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/httpfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,800 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741898_1074, replicas=127.0.0.1:50010 for /user/lag/input/kms-acls.xml._COPYING_
2018-09-24 16:09:28,811 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/kms-acls.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,833 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741899_1075, replicas=127.0.0.1:50010 for /user/lag/input/kms-env.sh._COPYING_
2018-09-24 16:09:28,842 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/kms-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,857 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741900_1076, replicas=127.0.0.1:50010 for /user/lag/input/kms-log4j.properties._COPYING_
2018-09-24 16:09:28,866 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/kms-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,881 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741901_1077, replicas=127.0.0.1:50010 for /user/lag/input/kms-site.xml._COPYING_
2018-09-24 16:09:28,890 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/kms-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,905 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741902_1078, replicas=127.0.0.1:50010 for /user/lag/input/log4j.properties._COPYING_
2018-09-24 16:09:28,917 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,936 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741903_1079, replicas=127.0.0.1:50010 for /user/lag/input/mapred-env.cmd._COPYING_
2018-09-24 16:09:28,948 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/mapred-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,967 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741904_1080, replicas=127.0.0.1:50010 for /user/lag/input/mapred-env.sh._COPYING_
2018-09-24 16:09:28,977 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/mapred-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:28,994 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741905_1081, replicas=127.0.0.1:50010 for /user/lag/input/mapred-queues.xml.template._COPYING_
2018-09-24 16:09:29,005 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/mapred-queues.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:29,020 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741906_1082, replicas=127.0.0.1:50010 for /user/lag/input/mapred-site.xml.template._COPYING_
2018-09-24 16:09:29,031 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/mapred-site.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:29,047 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741907_1083, replicas=127.0.0.1:50010 for /user/lag/input/slaves._COPYING_
2018-09-24 16:09:29,057 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/slaves._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:29,073 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741908_1084, replicas=127.0.0.1:50010 for /user/lag/input/ssl-client.xml.example._COPYING_
2018-09-24 16:09:29,083 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/ssl-client.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:29,096 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741909_1085, replicas=127.0.0.1:50010 for /user/lag/input/ssl-server.xml.example._COPYING_
2018-09-24 16:09:29,104 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/ssl-server.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:29,116 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741910_1086, replicas=127.0.0.1:50010 for /user/lag/input/yarn-env.cmd._COPYING_
2018-09-24 16:09:29,124 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/yarn-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:29,137 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741911_1087, replicas=127.0.0.1:50010 for /user/lag/input/yarn-env.sh._COPYING_
2018-09-24 16:09:29,144 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741911_1087 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/lag/input/yarn-env.sh._COPYING_
2018-09-24 16:09:29,546 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/yarn-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:29,572 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741912_1088, replicas=127.0.0.1:50010 for /user/lag/input/yarn-site.xml._COPYING_
2018-09-24 16:09:29,584 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/yarn-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_1942902694_1
2018-09-24 16:09:46,511 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741913_1089, replicas=127.0.0.1:50010 for /user/lag/grep-temp-293572281/_temporary/0/_temporary/attempt_local753802890_0001_r_000000_0/part-r-00000
2018-09-24 16:09:46,533 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/grep-temp-293572281/_temporary/0/_temporary/attempt_local753802890_0001_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_-2110156692_1
2018-09-24 16:09:46,590 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/grep-temp-293572281/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-2110156692_1
2018-09-24 16:11:07,070 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 580 Total time for transactions(ms): 54 Number of transactions batched in Syncs: 107 Number of syncs: 475 SyncTimes(ms): 1667 
2018-09-24 16:11:28,114 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741914_1090, replicas=127.0.0.1:50010 for /user/Test/hadoop/capacity-scheduler.xml._COPYING_
2018-09-24 16:11:28,195 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/capacity-scheduler.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,215 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741915_1091, replicas=127.0.0.1:50010 for /user/Test/hadoop/configuration.xsl._COPYING_
2018-09-24 16:11:28,223 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/configuration.xsl._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,243 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741916_1092, replicas=127.0.0.1:50010 for /user/Test/hadoop/container-executor.cfg._COPYING_
2018-09-24 16:11:28,255 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/container-executor.cfg._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,276 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741917_1093, replicas=127.0.0.1:50010 for /user/Test/hadoop/core-site.xml._COPYING_
2018-09-24 16:11:28,288 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/core-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,308 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741918_1094, replicas=127.0.0.1:50010 for /user/Test/hadoop/hadoop-env.cmd._COPYING_
2018-09-24 16:11:28,321 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/hadoop-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,341 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741919_1095, replicas=127.0.0.1:50010 for /user/Test/hadoop/hadoop-env.sh._COPYING_
2018-09-24 16:11:28,355 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/hadoop-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,375 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741920_1096, replicas=127.0.0.1:50010 for /user/Test/hadoop/hadoop-metrics.properties._COPYING_
2018-09-24 16:11:28,384 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/hadoop-metrics.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,397 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741921_1097, replicas=127.0.0.1:50010 for /user/Test/hadoop/hadoop-metrics2.properties._COPYING_
2018-09-24 16:11:28,405 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/hadoop-metrics2.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,419 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741922_1098, replicas=127.0.0.1:50010 for /user/Test/hadoop/hadoop-policy.xml._COPYING_
2018-09-24 16:11:28,428 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/hadoop-policy.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,440 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741923_1099, replicas=127.0.0.1:50010 for /user/Test/hadoop/hdfs-site.xml._COPYING_
2018-09-24 16:11:28,447 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/hdfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,460 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741924_1100, replicas=127.0.0.1:50010 for /user/Test/hadoop/httpfs-env.sh._COPYING_
2018-09-24 16:11:28,468 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/httpfs-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,480 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741925_1101, replicas=127.0.0.1:50010 for /user/Test/hadoop/httpfs-log4j.properties._COPYING_
2018-09-24 16:11:28,488 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/httpfs-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,499 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741926_1102, replicas=127.0.0.1:50010 for /user/Test/hadoop/httpfs-signature.secret._COPYING_
2018-09-24 16:11:28,506 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/httpfs-signature.secret._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,520 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741927_1103, replicas=127.0.0.1:50010 for /user/Test/hadoop/httpfs-site.xml._COPYING_
2018-09-24 16:11:28,533 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/httpfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,552 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741928_1104, replicas=127.0.0.1:50010 for /user/Test/hadoop/kms-acls.xml._COPYING_
2018-09-24 16:11:28,564 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/kms-acls.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,587 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741929_1105, replicas=127.0.0.1:50010 for /user/Test/hadoop/kms-env.sh._COPYING_
2018-09-24 16:11:28,600 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/kms-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,620 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741930_1106, replicas=127.0.0.1:50010 for /user/Test/hadoop/kms-log4j.properties._COPYING_
2018-09-24 16:11:28,633 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/kms-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,652 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741931_1107, replicas=127.0.0.1:50010 for /user/Test/hadoop/kms-site.xml._COPYING_
2018-09-24 16:11:28,664 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/kms-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,683 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741932_1108, replicas=127.0.0.1:50010 for /user/Test/hadoop/log4j.properties._COPYING_
2018-09-24 16:11:28,696 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,715 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741933_1109, replicas=127.0.0.1:50010 for /user/Test/hadoop/mapred-env.cmd._COPYING_
2018-09-24 16:11:28,728 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/mapred-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,747 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741934_1110, replicas=127.0.0.1:50010 for /user/Test/hadoop/mapred-env.sh._COPYING_
2018-09-24 16:11:28,757 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/mapred-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,770 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741935_1111, replicas=127.0.0.1:50010 for /user/Test/hadoop/mapred-queues.xml.template._COPYING_
2018-09-24 16:11:28,778 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/mapred-queues.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:28,792 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741936_1112, replicas=127.0.0.1:50010 for /user/Test/hadoop/mapred-site.xml.template._COPYING_
2018-09-24 16:11:28,800 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741936_1112 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Test/hadoop/mapred-site.xml.template._COPYING_
2018-09-24 16:11:29,202 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/mapred-site.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:29,221 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741937_1113, replicas=127.0.0.1:50010 for /user/Test/hadoop/slaves._COPYING_
2018-09-24 16:11:29,233 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/slaves._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:29,253 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741938_1114, replicas=127.0.0.1:50010 for /user/Test/hadoop/ssl-client.xml.example._COPYING_
2018-09-24 16:11:29,437 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/ssl-client.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:29,455 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741939_1115, replicas=127.0.0.1:50010 for /user/Test/hadoop/ssl-server.xml.example._COPYING_
2018-09-24 16:11:29,468 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/ssl-server.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:29,486 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741940_1116, replicas=127.0.0.1:50010 for /user/Test/hadoop/yarn-env.cmd._COPYING_
2018-09-24 16:11:29,496 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/yarn-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:29,510 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741941_1117, replicas=127.0.0.1:50010 for /user/Test/hadoop/yarn-env.sh._COPYING_
2018-09-24 16:11:29,519 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/yarn-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:29,532 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741942_1118, replicas=127.0.0.1:50010 for /user/Test/hadoop/yarn-site.xml._COPYING_
2018-09-24 16:11:29,542 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Test/hadoop/yarn-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2073014715_1
2018-09-24 16:11:48,215 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#4 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:57736: java.io.FileNotFoundException: Path is not a file: /user/Test/hadoop
2018-09-24 16:12:25,733 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 763 Total time for transactions(ms): 60 Number of transactions batched in Syncs: 138 Number of syncs: 625 SyncTimes(ms): 2150 
2018-09-24 16:12:25,861 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#4 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:57742: java.io.FileNotFoundException: Path is not a file: /user/Test/hadoop
2018-09-24 16:17:03,605 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 766 Total time for transactions(ms): 60 Number of transactions batched in Syncs: 140 Number of syncs: 628 SyncTimes(ms): 2165 
2018-09-24 16:17:12,404 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741943_1119, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/capacity-scheduler.xml._COPYING_
2018-09-24 16:17:12,481 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/capacity-scheduler.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:12,500 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741944_1120, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/configuration.xsl._COPYING_
2018-09-24 16:17:12,507 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/configuration.xsl._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:12,520 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741945_1121, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/container-executor.cfg._COPYING_
2018-09-24 16:17:12,527 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/container-executor.cfg._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:12,541 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741946_1122, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/core-site.xml._COPYING_
2018-09-24 16:17:12,549 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/core-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:12,561 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741947_1123, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/hadoop-env.cmd._COPYING_
2018-09-24 16:17:12,569 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/hadoop-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:12,582 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741948_1124, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/hadoop-env.sh._COPYING_
2018-09-24 16:17:12,590 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/hadoop-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:12,603 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741949_1125, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/hadoop-metrics.properties._COPYING_
2018-09-24 16:17:12,611 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/hadoop-metrics.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:12,623 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741950_1126, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/hadoop-metrics2.properties._COPYING_
2018-09-24 16:17:12,631 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/hadoop-metrics2.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:12,644 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741951_1127, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/hadoop-policy.xml._COPYING_
2018-09-24 16:17:12,652 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/hadoop-policy.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:12,664 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741952_1128, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/hdfs-site.xml._COPYING_
2018-09-24 16:17:12,678 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/hdfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:12,696 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741953_1129, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/httpfs-env.sh._COPYING_
2018-09-24 16:17:12,709 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/httpfs-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:12,726 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741954_1130, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/httpfs-log4j.properties._COPYING_
2018-09-24 16:17:12,734 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/httpfs-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:12,746 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741955_1131, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/httpfs-signature.secret._COPYING_
2018-09-24 16:17:12,753 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/httpfs-signature.secret._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:12,770 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741956_1132, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/httpfs-site.xml._COPYING_
2018-09-24 16:17:12,782 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/httpfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:12,973 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741957_1133, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/kms-acls.xml._COPYING_
2018-09-24 16:17:12,986 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/kms-acls.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:13,010 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741958_1134, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/kms-env.sh._COPYING_
2018-09-24 16:17:13,022 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/kms-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:13,042 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741959_1135, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/kms-log4j.properties._COPYING_
2018-09-24 16:17:13,054 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/kms-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:13,072 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741960_1136, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/kms-site.xml._COPYING_
2018-09-24 16:17:13,083 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/kms-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:13,101 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741961_1137, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/log4j.properties._COPYING_
2018-09-24 16:17:13,112 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:13,131 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741962_1138, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/mapred-env.cmd._COPYING_
2018-09-24 16:17:13,143 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/mapred-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:13,159 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741963_1139, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/mapred-env.sh._COPYING_
2018-09-24 16:17:13,170 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/mapred-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:13,186 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741964_1140, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/mapred-queues.xml.template._COPYING_
2018-09-24 16:17:13,196 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/mapred-queues.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:13,213 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741965_1141, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/mapred-site.xml.template._COPYING_
2018-09-24 16:17:13,225 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/mapred-site.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:13,243 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741966_1142, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/slaves._COPYING_
2018-09-24 16:17:13,254 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/slaves._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:13,273 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741967_1143, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/ssl-client.xml.example._COPYING_
2018-09-24 16:17:13,282 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/ssl-client.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:13,294 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741968_1144, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/ssl-server.xml.example._COPYING_
2018-09-24 16:17:13,302 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/ssl-server.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:13,313 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741969_1145, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/yarn-env.cmd._COPYING_
2018-09-24 16:17:13,321 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/yarn-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:13,339 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741970_1146, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/yarn-env.sh._COPYING_
2018-09-24 16:17:13,348 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/yarn-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:13,364 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741971_1147, replicas=127.0.0.1:50010 for /user/lag/input/hadoop/yarn-site.xml._COPYING_
2018-09-24 16:17:13,374 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/input/hadoop/yarn-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2071920537_1
2018-09-24 16:17:37,240 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#63 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:57846: java.io.FileNotFoundException: Path is not a file: /user/lag/input/hadoop
2018-09-24 16:25:36,482 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 949 Total time for transactions(ms): 71 Number of transactions batched in Syncs: 171 Number of syncs: 778 SyncTimes(ms): 2645 
2018-09-24 16:25:39,129 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#63 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:57914: java.io.FileNotFoundException: Path is not a file: /user/lag/input/hadoop
2018-09-24 16:31:22,540 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-09-24 16:31:22,540 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-09-24 16:31:22,540 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 3, 953
2018-09-24 16:31:22,541 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 952 Total time for transactions(ms): 71 Number of transactions batched in Syncs: 173 Number of syncs: 781 SyncTimes(ms): 2660 
2018-09-24 16:31:22,543 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 952 Total time for transactions(ms): 71 Number of transactions batched in Syncs: 173 Number of syncs: 782 SyncTimes(ms): 2663 
2018-09-24 16:31:22,545 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000000003 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000003-0000000000000000954
2018-09-24 16:31:22,572 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 955
2018-09-24 16:31:23,198 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000, fileSize: 320. Sent total: 320 bytes. Size of last segment intended to send: -1 bytes.
2018-09-24 16:31:23,237 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000001-0000000000000000002, fileSize: 42. Sent total: 42 bytes. Size of last segment intended to send: -1 bytes.
2018-09-24 16:31:23,252 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000003-0000000000000000954, fileSize: 93540. Sent total: 93540 bytes. Size of last segment intended to send: -1 bytes.
2018-09-24 16:31:23,571 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Combined time for fsimage download and fsync to all disks took 0.01s. The fsimage download took 0.00s at 3000.00 KB/s. Synchronous (fsync) write to disk of /tmp/hadoop-lag/dfs/name/current/fsimage.ckpt_0000000000000000954 took 0.00s.
2018-09-24 16:31:23,572 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000954 size 12333 bytes.
2018-09-24 16:31:23,584 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2018-09-24 15:31:23,500 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-09-24 15:31:23,500 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-09-24 15:31:23,500 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 955, 955
2018-09-24 15:31:23,501 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 56 
2018-09-24 15:31:23,503 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 57 
2018-09-24 15:31:23,504 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000000955 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000955-0000000000000000956
2018-09-24 15:31:23,504 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 957
2018-09-24 15:31:23,589 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000955-0000000000000000956, fileSize: 42. Sent total: 42 bytes. Size of last segment intended to send: -1 bytes.
2018-09-24 15:31:23,659 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Combined time for fsimage download and fsync to all disks took 0.00s. The fsimage download took 0.00s at 12000.00 KB/s. Synchronous (fsync) write to disk of /tmp/hadoop-lag/dfs/name/current/fsimage.ckpt_0000000000000000956 took 0.00s.
2018-09-24 15:31:23,659 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000956 size 12333 bytes.
2018-09-24 15:31:23,668 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 954
2018-09-24 15:31:23,668 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2018-09-24 16:03:05,790 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#2598 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 127.0.0.1:58572: org.apache.hadoop.security.AccessControlException: Permission denied: user=dr.who, access=WRITE, inode="/user":lag:supergroup:drwxr-xr-x
2018-09-24 16:03:28,547 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2018-09-24 16:03:28,548 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at lag-Predator-G3-571/127.0.1.1
************************************************************/
2018-09-24 16:04:26,710 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = lag-Predator-G3-571/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.9.1
STARTUP_MSG:   classpath = /home/lag/BD/Hadoop/hadoop-2.9.1/etc/hadoop:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/activation-1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xz-1.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/asm-3.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-auth-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jettison-1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/junit-4.11.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/avro-1.7.7.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-annotations-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsch-0.1.54.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-nfs-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/fst-2.50.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-core-1.8.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-1.7.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-framework-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsp-api-2.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/gson-2.2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-recipes-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/avro-1.7.7.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xmlenc-0.52.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jcip-annotations-1.0-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-digester-1.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-net-3.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-smart-1.3.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsch-0.1.54.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-client-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-router-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-common-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-registry-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-api-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-common-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/avro-1.7.7.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/snappy-java-1.0.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.9.1.jar:/usr/lib/jvm/java-8-openjdk-amd64//lib/tools.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e30710aea4e6e55e69372929106cf119af06fd0e; compiled by 'root' on 2018-04-16T09:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-24 16:04:26,719 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-24 16:04:26,721 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2018-09-24 16:04:26,860 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-24 16:04:26,938 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-09-24 16:04:26,938 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2018-09-24 16:04:26,957 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2018-09-24 16:04:26,959 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2018-09-24 16:04:27,090 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-09-24 16:04:27,102 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2018-09-24 16:04:27,150 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-24 16:04:27,158 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-24 16:04:27,171 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2018-09-24 16:04:27,182 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-24 16:04:27,184 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2018-09-24 16:04:27,184 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-24 16:04:27,184 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-24 16:04:27,280 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2018-09-24 16:04:27,280 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2018-09-24 16:04:27,289 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2018-09-24 16:04:27,289 INFO org.mortbay.log: jetty-6.1.26
2018-09-24 16:04:27,432 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2018-09-24 16:04:27,450 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-24 16:04:27,450 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-24 16:04:27,485 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2018-09-24 16:04:27,491 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2018-09-24 16:04:27,492 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2018-09-24 16:04:27,493 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2018-09-24 16:04:27,496 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = lag (auth:SIMPLE)
2018-09-24 16:04:27,496 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2018-09-24 16:04:27,496 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2018-09-24 16:04:27,497 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2018-09-24 16:04:27,516 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-09-24 16:04:27,528 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2018-09-24 16:04:27,528 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2018-09-24 16:04:27,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2018-09-24 16:04:27,531 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2018 Sep 24 16:04:27
2018-09-24 16:04:27,532 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2018-09-24 16:04:27,532 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-24 16:04:27,533 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2018-09-24 16:04:27,533 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2018-09-24 16:04:27,542 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2018-09-24 16:04:27,543 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS
2018-09-24 16:04:27,546 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2018-09-24 16:04:27,546 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2018-09-24 16:04:27,546 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2018-09-24 16:04:27,546 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2018-09-24 16:04:27,546 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2018-09-24 16:04:27,546 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2018-09-24 16:04:27,546 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2018-09-24 16:04:27,546 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2018-09-24 16:04:27,546 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2018-09-24 16:04:27,546 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2018-09-24 16:04:27,546 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2018-09-24 16:04:27,547 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2018-09-24 16:04:27,589 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2018-09-24 16:04:27,589 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-24 16:04:27,589 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2018-09-24 16:04:27,589 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2018-09-24 16:04:27,590 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2018-09-24 16:04:27,590 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2018-09-24 16:04:27,590 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2018-09-24 16:04:27,593 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false
2018-09-24 16:04:27,596 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2018-09-24 16:04:27,596 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-24 16:04:27,597 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2018-09-24 16:04:27,597 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2018-09-24 16:04:27,599 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2018-09-24 16:04:27,599 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2018-09-24 16:04:27,599 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2018-09-24 16:04:27,602 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2018-09-24 16:04:27,602 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2018-09-24 16:04:27,604 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2018-09-24 16:04:27,604 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-24 16:04:27,604 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2018-09-24 16:04:27,604 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2018-09-24 16:04:27,626 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-lag/dfs/name/in_use.lock acquired by nodename 15688@lag-Predator-G3-571
2018-09-24 16:04:27,639 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-lag/dfs/name/current
2018-09-24 16:04:27,657 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000000957 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000957-0000000000000000957
2018-09-24 16:04:27,673 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000956, cpktTxId=0000000000000000956)
2018-09-24 16:04:27,740 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 160 INodes.
2018-09-24 16:04:27,776 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2018-09-24 16:04:27,776 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 956 from /tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000956
2018-09-24 16:04:27,776 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@53812a9b expecting start txid #957
2018-09-24 16:04:27,776 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000957-0000000000000000957
2018-09-24 16:04:27,777 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream '/tmp/hadoop-lag/dfs/name/current/edits_0000000000000000957-0000000000000000957' to transaction ID 957
2018-09-24 16:04:27,785 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000957-0000000000000000957 of size 1048576 edits # 1 loaded in 0 seconds
2018-09-24 16:04:27,785 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2018-09-24 16:04:27,786 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 958
2018-09-24 16:04:28,134 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2018-09-24 16:04:28,134 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 528 msecs
2018-09-24 16:04:28,269 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2018-09-24 16:04:28,274 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-09-24 16:04:28,282 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2018-09-24 16:04:28,319 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2018-09-24 16:04:28,325 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2018-09-24 16:04:28,331 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 144 blocks to reach the threshold 0.9990 of total blocks 145.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2018-09-24 16:04:28,349 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-24 16:04:28,350 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2018-09-24 16:04:28,358 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2018-09-24 16:04:28,360 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2018-09-24 16:04:28,360 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2018-09-24 16:04:28,365 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 5 milliseconds
name space=160
storage space=449210
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2018-09-24 16:04:28,368 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2018-09-24 16:04:32,490 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=916da39d-1bd1-46c6-97b7-33d886cf34c5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-d46bcbf5-42c2-408a-a95e-5958aa5ec944;nsid=1438813254;c=1537797164589) storage 916da39d-1bd1-46c6-97b7-33d886cf34c5
2018-09-24 16:04:32,492 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2018-09-24 16:04:32,492 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 916da39d-1bd1-46c6-97b7-33d886cf34c5 (127.0.0.1:50010).
2018-09-24 16:04:32,538 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-d1df1055-40f5-4056-920c-be574d20c567 for DN 127.0.0.1:50010
2018-09-24 16:04:32,568 INFO BlockStateChange: BLOCK* processReport 0xdc95c4739cfb6832: Processing first storage report for DS-d1df1055-40f5-4056-920c-be574d20c567 from datanode 916da39d-1bd1-46c6-97b7-33d886cf34c5
2018-09-24 16:04:32,572 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues
2018-09-24 16:04:32,572 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 144 has reached the threshold 0.9990 of total blocks 145. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2018-09-24 16:04:32,573 INFO BlockStateChange: BLOCK* processReport 0xdc95c4739cfb6832: from storage DS-d1df1055-40f5-4056-920c-be574d20c567 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=916da39d-1bd1-46c6-97b7-33d886cf34c5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-d46bcbf5-42c2-408a-a95e-5958aa5ec944;nsid=1438813254;c=1537797164589), blocks: 145, hasStaleStorage: false, processing time: 5 msecs, invalidatedBlocks: 0
2018-09-24 16:04:32,575 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 145
2018-09-24 16:04:32,575 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2018-09-24 16:04:32,575 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2018-09-24 16:04:32,575 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2018-09-24 16:04:32,575 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2018-09-24 16:04:32,575 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 3 msec
2018-09-24 16:04:52,578 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 145 has reached the threshold 0.9990 of total blocks 145. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2018-09-24 16:04:54,825 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#11 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from 127.0.0.1:58614: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/user/WordCount.java. Name node is in safe mode.
The reported blocks 145 has reached the threshold 0.9990 of total blocks 145. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 7 seconds. NamenodeHostName:localhost
2018-09-24 16:05:02,581 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2018-09-24 16:05:02,582 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 34 secs
2018-09-24 16:05:02,582 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2018-09-24 16:05:02,582 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2018-09-24 16:05:13,718 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2018-09-24 16:05:13,723 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at lag-Predator-G3-571/127.0.1.1
************************************************************/
2018-09-24 16:06:01,935 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = lag-Predator-G3-571/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.9.1
STARTUP_MSG:   classpath = /home/lag/BD/Hadoop/hadoop-2.9.1/etc/hadoop:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/activation-1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xz-1.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/asm-3.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-auth-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jettison-1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/junit-4.11.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/avro-1.7.7.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-annotations-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsch-0.1.54.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-nfs-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/fst-2.50.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-core-1.8.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-1.7.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-framework-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsp-api-2.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/gson-2.2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-recipes-2.7.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/avro-1.7.7.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xmlenc-0.52.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jcip-annotations-1.0-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-digester-1.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-net-3.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-smart-1.3.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsch-0.1.54.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-client-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-router-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-common-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-registry-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-api-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-common-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/avro-1.7.7.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/snappy-java-1.0.5.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1-tests.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.9.1.jar:/home/lag/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.9.1.jar:/usr/lib/jvm/java-8-openjdk-amd64//lib/tools.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e30710aea4e6e55e69372929106cf119af06fd0e; compiled by 'root' on 2018-04-16T09:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-24 16:06:01,942 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-24 16:06:01,945 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2018-09-24 16:06:02,057 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-24 16:06:02,126 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-09-24 16:06:02,126 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2018-09-24 16:06:02,144 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2018-09-24 16:06:02,146 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2018-09-24 16:06:02,255 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-09-24 16:06:02,266 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2018-09-24 16:06:02,304 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-24 16:06:02,310 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-24 16:06:02,320 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2018-09-24 16:06:02,332 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-24 16:06:02,334 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2018-09-24 16:06:02,334 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-24 16:06:02,334 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-24 16:06:02,424 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2018-09-24 16:06:02,424 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2018-09-24 16:06:02,434 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2018-09-24 16:06:02,434 INFO org.mortbay.log: jetty-6.1.26
2018-09-24 16:06:02,576 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2018-09-24 16:06:02,594 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-24 16:06:02,594 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-24 16:06:02,630 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2018-09-24 16:06:02,636 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2018-09-24 16:06:02,637 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2018-09-24 16:06:02,638 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2018-09-24 16:06:02,642 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = lag (auth:SIMPLE)
2018-09-24 16:06:02,642 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2018-09-24 16:06:02,642 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2018-09-24 16:06:02,642 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2018-09-24 16:06:02,662 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-09-24 16:06:02,674 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2018-09-24 16:06:02,674 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2018-09-24 16:06:02,676 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2018-09-24 16:06:02,676 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2018 Sep 24 16:06:02
2018-09-24 16:06:02,677 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2018-09-24 16:06:02,677 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-24 16:06:02,678 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2018-09-24 16:06:02,678 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2018-09-24 16:06:02,688 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2018-09-24 16:06:02,688 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS
2018-09-24 16:06:02,691 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2018-09-24 16:06:02,691 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2018-09-24 16:06:02,691 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2018-09-24 16:06:02,691 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2018-09-24 16:06:02,691 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2018-09-24 16:06:02,691 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2018-09-24 16:06:02,691 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2018-09-24 16:06:02,691 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2018-09-24 16:06:02,691 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2018-09-24 16:06:02,691 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2018-09-24 16:06:02,691 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2018-09-24 16:06:02,692 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2018-09-24 16:06:02,735 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2018-09-24 16:06:02,735 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-24 16:06:02,735 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2018-09-24 16:06:02,735 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2018-09-24 16:06:02,735 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2018-09-24 16:06:02,735 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2018-09-24 16:06:02,736 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2018-09-24 16:06:02,738 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false
2018-09-24 16:06:02,742 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2018-09-24 16:06:02,742 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-24 16:06:02,742 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2018-09-24 16:06:02,742 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2018-09-24 16:06:02,745 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2018-09-24 16:06:02,745 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2018-09-24 16:06:02,745 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2018-09-24 16:06:02,748 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2018-09-24 16:06:02,748 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2018-09-24 16:06:02,749 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2018-09-24 16:06:02,749 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-24 16:06:02,749 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2018-09-24 16:06:02,749 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2018-09-24 16:06:02,763 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-lag/dfs/name/in_use.lock acquired by nodename 16854@lag-Predator-G3-571
2018-09-24 16:06:02,776 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-lag/dfs/name/current
2018-09-24 16:06:02,794 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000000958 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000958-0000000000000000958
2018-09-24 16:06:02,813 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000956, cpktTxId=0000000000000000956)
2018-09-24 16:06:02,878 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 160 INodes.
2018-09-24 16:06:02,914 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2018-09-24 16:06:02,914 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 956 from /tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000956
2018-09-24 16:06:02,914 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@75e91545 expecting start txid #957
2018-09-24 16:06:02,915 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000957-0000000000000000957
2018-09-24 16:06:02,916 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream '/tmp/hadoop-lag/dfs/name/current/edits_0000000000000000957-0000000000000000957' to transaction ID 957
2018-09-24 16:06:02,923 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000957-0000000000000000957 of size 1048576 edits # 1 loaded in 0 seconds
2018-09-24 16:06:02,923 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@60d1a32f expecting start txid #958
2018-09-24 16:06:02,923 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000958-0000000000000000958
2018-09-24 16:06:02,923 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream '/tmp/hadoop-lag/dfs/name/current/edits_0000000000000000958-0000000000000000958' to transaction ID 957
2018-09-24 16:06:02,924 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000958-0000000000000000958 of size 1048576 edits # 1 loaded in 0 seconds
2018-09-24 16:06:02,924 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2018-09-24 16:06:02,925 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 959
2018-09-24 16:06:03,079 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2018-09-24 16:06:03,079 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 328 msecs
2018-09-24 16:06:03,211 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2018-09-24 16:06:03,215 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-09-24 16:06:03,223 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2018-09-24 16:06:03,260 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2018-09-24 16:06:03,266 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2018-09-24 16:06:03,272 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 144 blocks to reach the threshold 0.9990 of total blocks 145.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2018-09-24 16:06:03,291 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-24 16:06:03,291 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2018-09-24 16:06:03,298 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2018-09-24 16:06:03,301 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2018-09-24 16:06:03,301 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2018-09-24 16:06:03,306 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 4 milliseconds
name space=160
storage space=449210
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2018-09-24 16:06:03,309 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2018-09-24 16:06:07,598 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=916da39d-1bd1-46c6-97b7-33d886cf34c5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-d46bcbf5-42c2-408a-a95e-5958aa5ec944;nsid=1438813254;c=1537797164589) storage 916da39d-1bd1-46c6-97b7-33d886cf34c5
2018-09-24 16:06:07,599 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2018-09-24 16:06:07,600 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 916da39d-1bd1-46c6-97b7-33d886cf34c5 (127.0.0.1:50010).
2018-09-24 16:06:07,639 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-d1df1055-40f5-4056-920c-be574d20c567 for DN 127.0.0.1:50010
2018-09-24 16:06:07,666 INFO BlockStateChange: BLOCK* processReport 0x5477fc05cd251a8f: Processing first storage report for DS-d1df1055-40f5-4056-920c-be574d20c567 from datanode 916da39d-1bd1-46c6-97b7-33d886cf34c5
2018-09-24 16:06:07,670 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues
2018-09-24 16:06:07,671 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 144 has reached the threshold 0.9990 of total blocks 145. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 30 seconds.
2018-09-24 16:06:07,672 INFO BlockStateChange: BLOCK* processReport 0x5477fc05cd251a8f: from storage DS-d1df1055-40f5-4056-920c-be574d20c567 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=916da39d-1bd1-46c6-97b7-33d886cf34c5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-d46bcbf5-42c2-408a-a95e-5958aa5ec944;nsid=1438813254;c=1537797164589), blocks: 145, hasStaleStorage: false, processing time: 5 msecs, invalidatedBlocks: 0
2018-09-24 16:06:07,674 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 145
2018-09-24 16:06:07,674 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2018-09-24 16:06:07,674 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2018-09-24 16:06:07,674 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2018-09-24 16:06:07,674 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2018-09-24 16:06:07,674 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 4 msec
2018-09-24 16:06:27,679 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 145 has reached the threshold 0.9990 of total blocks 145. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2018-09-24 16:06:37,683 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2018-09-24 16:06:37,684 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 34 secs
2018-09-24 16:06:37,684 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2018-09-24 16:06:37,684 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2018-09-24 16:07:11,925 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-09-24 16:07:11,925 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-09-24 16:07:11,925 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 959, 959
2018-09-24 16:07:11,925 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 958 Number of syncs: 2 SyncTimes(ms): 51 
2018-09-24 16:07:11,937 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 958 Number of syncs: 3 SyncTimes(ms): 63 
2018-09-24 16:07:11,938 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000000959 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000959-0000000000000000960
2018-09-24 16:07:11,939 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 961
2018-09-24 16:07:12,340 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000956, fileSize: 12333. Sent total: 12333 bytes. Size of last segment intended to send: -1 bytes.
2018-09-24 16:07:12,380 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000957-0000000000000000957, fileSize: 1048576. Sent total: 1048576 bytes. Size of last segment intended to send: -1 bytes.
2018-09-24 16:07:12,420 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000958-0000000000000000958, fileSize: 1048576. Sent total: 1048576 bytes. Size of last segment intended to send: -1 bytes.
2018-09-24 16:07:12,456 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000959-0000000000000000960, fileSize: 42. Sent total: 42 bytes. Size of last segment intended to send: -1 bytes.
2018-09-24 16:07:12,679 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Combined time for fsimage download and fsync to all disks took 0.00s. The fsimage download took 0.00s at 12000.00 KB/s. Synchronous (fsync) write to disk of /tmp/hadoop-lag/dfs/name/current/fsimage.ckpt_0000000000000000960 took 0.00s.
2018-09-24 16:07:12,679 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000960 size 12333 bytes.
2018-09-24 16:07:12,697 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 956
2018-09-24 16:07:12,697 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000954, cpktTxId=0000000000000000954)
2018-09-24 16:11:35,575 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 63 
2018-09-24 16:11:35,635 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741972_1148, replicas=127.0.0.1:50010 for /user/Lag/input/capacity-scheduler.xml._COPYING_
2018-09-24 16:11:35,777 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741972_1148 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/input/capacity-scheduler.xml._COPYING_
2018-09-24 16:11:36,182 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/capacity-scheduler.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:36,205 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741973_1149, replicas=127.0.0.1:50010 for /user/Lag/input/configuration.xsl._COPYING_
2018-09-24 16:11:36,232 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/configuration.xsl._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:36,256 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741974_1150, replicas=127.0.0.1:50010 for /user/Lag/input/container-executor.cfg._COPYING_
2018-09-24 16:11:36,276 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/container-executor.cfg._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:36,304 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741975_1151, replicas=127.0.0.1:50010 for /user/Lag/input/core-site.xml._COPYING_
2018-09-24 16:11:36,315 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/core-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:36,336 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741976_1152, replicas=127.0.0.1:50010 for /user/Lag/input/hadoop-env.cmd._COPYING_
2018-09-24 16:11:36,349 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/hadoop-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:36,375 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741977_1153, replicas=127.0.0.1:50010 for /user/Lag/input/hadoop-env.sh._COPYING_
2018-09-24 16:11:36,390 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/hadoop-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:36,422 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741978_1154, replicas=127.0.0.1:50010 for /user/Lag/input/hadoop-metrics.properties._COPYING_
2018-09-24 16:11:36,438 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/hadoop-metrics.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:36,469 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741979_1155, replicas=127.0.0.1:50010 for /user/Lag/input/hadoop-metrics2.properties._COPYING_
2018-09-24 16:11:36,485 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/hadoop-metrics2.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:36,514 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741980_1156, replicas=127.0.0.1:50010 for /user/Lag/input/hadoop-policy.xml._COPYING_
2018-09-24 16:11:36,528 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741980_1156 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/input/hadoop-policy.xml._COPYING_
2018-09-24 16:11:36,931 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/hadoop-policy.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:36,956 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741981_1157, replicas=127.0.0.1:50010 for /user/Lag/input/hdfs-site.xml._COPYING_
2018-09-24 16:11:36,971 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/hdfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:36,996 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741982_1158, replicas=127.0.0.1:50010 for /user/Lag/input/httpfs-env.sh._COPYING_
2018-09-24 16:11:37,012 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/httpfs-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:37,035 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741983_1159, replicas=127.0.0.1:50010 for /user/Lag/input/httpfs-log4j.properties._COPYING_
2018-09-24 16:11:37,057 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/httpfs-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:37,081 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741984_1160, replicas=127.0.0.1:50010 for /user/Lag/input/httpfs-signature.secret._COPYING_
2018-09-24 16:11:37,269 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741984_1160 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/input/httpfs-signature.secret._COPYING_
2018-09-24 16:11:37,673 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/httpfs-signature.secret._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:37,696 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741985_1161, replicas=127.0.0.1:50010 for /user/Lag/input/httpfs-site.xml._COPYING_
2018-09-24 16:11:37,711 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741985_1161 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/input/httpfs-site.xml._COPYING_
2018-09-24 16:11:38,114 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/httpfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:38,135 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741986_1162, replicas=127.0.0.1:50010 for /user/Lag/input/kms-acls.xml._COPYING_
2018-09-24 16:11:38,148 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/kms-acls.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:38,170 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741987_1163, replicas=127.0.0.1:50010 for /user/Lag/input/kms-env.sh._COPYING_
2018-09-24 16:11:38,188 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/kms-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:38,210 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741988_1164, replicas=127.0.0.1:50010 for /user/Lag/input/kms-log4j.properties._COPYING_
2018-09-24 16:11:38,234 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/kms-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:38,260 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741989_1165, replicas=127.0.0.1:50010 for /user/Lag/input/kms-site.xml._COPYING_
2018-09-24 16:11:38,277 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/kms-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:38,307 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741990_1166, replicas=127.0.0.1:50010 for /user/Lag/input/log4j.properties._COPYING_
2018-09-24 16:11:38,326 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:38,357 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741991_1167, replicas=127.0.0.1:50010 for /user/Lag/input/mapred-env.cmd._COPYING_
2018-09-24 16:11:38,374 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/mapred-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:38,397 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741992_1168, replicas=127.0.0.1:50010 for /user/Lag/input/mapred-env.sh._COPYING_
2018-09-24 16:11:38,408 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/mapred-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:38,427 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741993_1169, replicas=127.0.0.1:50010 for /user/Lag/input/mapred-queues.xml.template._COPYING_
2018-09-24 16:11:38,445 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/mapred-queues.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:38,475 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741994_1170, replicas=127.0.0.1:50010 for /user/Lag/input/mapred-site.xml.template._COPYING_
2018-09-24 16:11:38,494 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/mapred-site.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:38,517 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741995_1171, replicas=127.0.0.1:50010 for /user/Lag/input/slaves._COPYING_
2018-09-24 16:11:38,532 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/slaves._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:38,553 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741996_1172, replicas=127.0.0.1:50010 for /user/Lag/input/ssl-client.xml.example._COPYING_
2018-09-24 16:11:38,568 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/ssl-client.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:38,593 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741997_1173, replicas=127.0.0.1:50010 for /user/Lag/input/ssl-server.xml.example._COPYING_
2018-09-24 16:11:38,607 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/ssl-server.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:38,631 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741998_1174, replicas=127.0.0.1:50010 for /user/Lag/input/yarn-env.cmd._COPYING_
2018-09-24 16:11:38,651 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/yarn-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:38,686 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741999_1175, replicas=127.0.0.1:50010 for /user/Lag/input/yarn-env.sh._COPYING_
2018-09-24 16:11:38,706 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/yarn-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:11:38,730 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742000_1176, replicas=127.0.0.1:50010 for /user/Lag/input/yarn-site.xml._COPYING_
2018-09-24 16:11:38,746 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/input/yarn-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_571641344_1
2018-09-24 16:12:20,578 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742001_1177, replicas=127.0.0.1:50010 for /user/lag/grep-temp-1664522989/_temporary/0/_temporary/attempt_local1142704713_0001_r_000000_0/part-r-00000
2018-09-24 16:12:20,602 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/grep-temp-1664522989/_temporary/0/_temporary/attempt_local1142704713_0001_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_-710489514_1
2018-09-24 16:12:20,653 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/grep-temp-1664522989/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-710489514_1
2018-09-24 16:12:21,649 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742002_1178, replicas=127.0.0.1:50010 for /user/Lag/output/_temporary/0/_temporary/attempt_local795194312_0002_r_000000_0/part-r-00000
2018-09-24 16:12:21,665 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/output/_temporary/0/_temporary/attempt_local795194312_0002_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_-710489514_1
2018-09-24 16:12:21,712 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-710489514_1
2018-09-24 16:16:25,686 INFO BlockStateChange: BLOCK* processReport 0x5477fc05cd251a90: from storage DS-d1df1055-40f5-4056-920c-be574d20c567 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=916da39d-1bd1-46c6-97b7-33d886cf34c5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-d46bcbf5-42c2-408a-a95e-5958aa5ec944;nsid=1438813254;c=1537797164589), blocks: 175, hasStaleStorage: false, processing time: 3 msecs, invalidatedBlocks: 0
2018-09-24 16:40:18,184 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2018-09-24 16:40:18,186 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at lag-Predator-G3-571/127.0.1.1
************************************************************/
2018-09-28 13:33:01,415 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = lag-Predator-G3-571/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.9.1
STARTUP_MSG:   classpath = /home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/etc/hadoop:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/activation-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-auth-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jettison-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/junit-4.11.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-annotations-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsch-0.1.54.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-nfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/fst-2.50.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-core-1.8.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-1.7.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-framework-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsp-api-2.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/gson-2.2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-recipes-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jcip-annotations-1.0-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-digester-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-net-3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-smart-1.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsch-0.1.54.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-router-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-registry-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-api-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.9.1.jar:/usr/lib/jvm/java-8-openjdk-amd64//lib/tools.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e30710aea4e6e55e69372929106cf119af06fd0e; compiled by 'root' on 2018-04-16T09:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-28 13:33:01,438 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-28 13:33:01,442 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2018-09-28 13:33:01,595 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-28 13:33:01,832 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-09-28 13:33:01,832 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2018-09-28 13:33:01,898 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2018-09-28 13:33:01,902 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2018-09-28 13:33:01,933 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2018-09-28 13:33:02,001 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-09-28 13:33:02,047 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2018-09-28 13:33:02,099 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-28 13:33:02,104 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-28 13:33:02,191 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2018-09-28 13:33:02,201 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-28 13:33:02,203 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2018-09-28 13:33:02,203 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-28 13:33:02,203 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-28 13:33:02,327 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2018-09-28 13:33:02,327 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2018-09-28 13:33:02,362 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2018-09-28 13:33:02,362 INFO org.mortbay.log: jetty-6.1.26
2018-09-28 13:33:02,594 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2018-09-28 13:33:02,703 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-28 13:33:02,703 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-28 13:33:02,749 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2018-09-28 13:33:02,783 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2018-09-28 13:33:02,784 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2018-09-28 13:33:02,785 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2018-09-28 13:33:02,787 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = lag (auth:SIMPLE)
2018-09-28 13:33:02,787 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2018-09-28 13:33:02,787 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2018-09-28 13:33:02,787 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2018-09-28 13:33:02,819 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-09-28 13:33:02,837 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2018-09-28 13:33:02,837 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2018-09-28 13:33:02,838 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2018-09-28 13:33:02,838 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2018 Sep 28 13:33:02
2018-09-28 13:33:02,839 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2018-09-28 13:33:02,839 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-28 13:33:02,841 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2018-09-28 13:33:02,841 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2018-09-28 13:33:02,860 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2018-09-28 13:33:02,860 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS
2018-09-28 13:33:02,864 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2018-09-28 13:33:02,865 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2018-09-28 13:33:02,865 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2018-09-28 13:33:02,865 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2018-09-28 13:33:02,865 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2018-09-28 13:33:02,865 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2018-09-28 13:33:02,865 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2018-09-28 13:33:02,865 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2018-09-28 13:33:02,865 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2018-09-28 13:33:02,865 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2018-09-28 13:33:02,865 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2018-09-28 13:33:02,866 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2018-09-28 13:33:03,083 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2018-09-28 13:33:03,083 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-28 13:33:03,084 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2018-09-28 13:33:03,084 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2018-09-28 13:33:03,084 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2018-09-28 13:33:03,084 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2018-09-28 13:33:03,084 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2018-09-28 13:33:03,087 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false
2018-09-28 13:33:03,091 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2018-09-28 13:33:03,091 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-28 13:33:03,091 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2018-09-28 13:33:03,091 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2018-09-28 13:33:03,094 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2018-09-28 13:33:03,094 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2018-09-28 13:33:03,094 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2018-09-28 13:33:03,097 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2018-09-28 13:33:03,097 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2018-09-28 13:33:03,099 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2018-09-28 13:33:03,099 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-28 13:33:03,099 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2018-09-28 13:33:03,099 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2018-09-28 13:33:03,101 WARN org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-lag/dfs/name does not exist
2018-09-28 13:33:03,102 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-lag/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:375)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:226)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1048)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:681)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:666)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:728)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:953)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:932)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1741)
2018-09-28 13:33:03,115 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2018-09-28 13:33:03,215 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2018-09-28 13:33:03,216 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2018-09-28 13:33:03,217 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2018-09-28 13:33:03,217 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-lag/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:375)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:226)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1048)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:681)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:666)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:728)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:953)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:932)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1741)
2018-09-28 13:33:03,220 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1: org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-lag/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
2018-09-28 13:33:03,222 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at lag-Predator-G3-571/127.0.1.1
************************************************************/
2018-09-28 13:57:36,670 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = lag-Predator-G3-571/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.9.1
STARTUP_MSG:   classpath = /home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/etc/hadoop:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/activation-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-auth-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jettison-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/junit-4.11.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-annotations-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsch-0.1.54.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-nfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/fst-2.50.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-core-1.8.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-1.7.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-framework-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsp-api-2.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/gson-2.2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-recipes-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jcip-annotations-1.0-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-digester-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-net-3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-smart-1.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsch-0.1.54.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-router-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-registry-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-api-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.9.1.jar:/usr/lib/jvm/java-8-openjdk-amd64//lib/tools.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e30710aea4e6e55e69372929106cf119af06fd0e; compiled by 'root' on 2018-04-16T09:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-28 13:57:36,676 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-28 13:57:36,678 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2018-09-28 13:57:36,770 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-28 13:57:36,829 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-09-28 13:57:36,829 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2018-09-28 13:57:36,843 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2018-09-28 13:57:36,845 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2018-09-28 13:57:36,935 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-09-28 13:57:36,942 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2018-09-28 13:57:36,973 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-28 13:57:36,977 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-28 13:57:36,992 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2018-09-28 13:57:36,995 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-28 13:57:36,997 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2018-09-28 13:57:36,997 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-28 13:57:36,997 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-28 13:57:37,067 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2018-09-28 13:57:37,067 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2018-09-28 13:57:37,075 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2018-09-28 13:57:37,075 INFO org.mortbay.log: jetty-6.1.26
2018-09-28 13:57:37,185 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2018-09-28 13:57:37,200 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-28 13:57:37,200 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-28 13:57:37,228 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2018-09-28 13:57:37,233 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2018-09-28 13:57:37,234 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2018-09-28 13:57:37,235 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2018-09-28 13:57:37,238 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = lag (auth:SIMPLE)
2018-09-28 13:57:37,238 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2018-09-28 13:57:37,238 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2018-09-28 13:57:37,238 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2018-09-28 13:57:37,254 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-09-28 13:57:37,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2018-09-28 13:57:37,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2018-09-28 13:57:37,265 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2018-09-28 13:57:37,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2018 Sep 28 13:57:37
2018-09-28 13:57:37,267 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2018-09-28 13:57:37,267 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-28 13:57:37,268 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2018-09-28 13:57:37,268 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2018-09-28 13:57:37,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2018-09-28 13:57:37,276 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS
2018-09-28 13:57:37,278 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2018-09-28 13:57:37,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2018-09-28 13:57:37,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2018-09-28 13:57:37,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2018-09-28 13:57:37,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2018-09-28 13:57:37,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2018-09-28 13:57:37,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2018-09-28 13:57:37,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2018-09-28 13:57:37,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2018-09-28 13:57:37,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2018-09-28 13:57:37,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2018-09-28 13:57:37,279 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2018-09-28 13:57:37,316 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2018-09-28 13:57:37,316 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-28 13:57:37,316 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2018-09-28 13:57:37,316 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2018-09-28 13:57:37,316 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2018-09-28 13:57:37,316 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2018-09-28 13:57:37,316 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2018-09-28 13:57:37,319 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false
2018-09-28 13:57:37,322 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2018-09-28 13:57:37,322 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-28 13:57:37,322 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2018-09-28 13:57:37,322 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2018-09-28 13:57:37,324 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2018-09-28 13:57:37,324 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2018-09-28 13:57:37,324 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2018-09-28 13:57:37,327 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2018-09-28 13:57:37,327 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2018-09-28 13:57:37,328 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2018-09-28 13:57:37,328 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-28 13:57:37,328 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2018-09-28 13:57:37,328 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2018-09-28 13:57:37,330 WARN org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-lag/dfs/name does not exist
2018-09-28 13:57:37,331 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-lag/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:375)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:226)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1048)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:681)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:666)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:728)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:953)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:932)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1741)
2018-09-28 13:57:37,333 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2018-09-28 13:57:37,434 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2018-09-28 13:57:37,435 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2018-09-28 13:57:37,435 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2018-09-28 13:57:37,435 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-lag/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:375)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:226)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1048)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:681)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:666)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:728)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:953)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:932)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1741)
2018-09-28 13:57:37,439 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1: org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-lag/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
2018-09-28 13:57:37,441 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at lag-Predator-G3-571/127.0.1.1
************************************************************/
2018-09-28 14:00:12,841 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = lag-Predator-G3-571/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.9.1
STARTUP_MSG:   classpath = /home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/etc/hadoop:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/activation-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-auth-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jettison-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/junit-4.11.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-annotations-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsch-0.1.54.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-nfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/fst-2.50.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-core-1.8.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-1.7.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-framework-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsp-api-2.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/gson-2.2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-recipes-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jcip-annotations-1.0-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-digester-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-net-3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-smart-1.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsch-0.1.54.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-router-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-registry-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-api-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.9.1.jar:/usr/lib/jvm/java-8-openjdk-amd64//lib/tools.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e30710aea4e6e55e69372929106cf119af06fd0e; compiled by 'root' on 2018-04-16T09:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-28 14:00:12,847 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-28 14:00:12,849 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2018-09-28 14:00:12,942 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-28 14:00:13,001 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-09-28 14:00:13,001 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2018-09-28 14:00:13,015 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2018-09-28 14:00:13,017 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2018-09-28 14:00:13,106 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-09-28 14:00:13,114 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2018-09-28 14:00:13,144 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-28 14:00:13,149 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-28 14:00:13,164 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2018-09-28 14:00:13,167 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-28 14:00:13,169 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2018-09-28 14:00:13,169 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-28 14:00:13,169 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-28 14:00:13,250 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2018-09-28 14:00:13,250 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2018-09-28 14:00:13,259 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2018-09-28 14:00:13,259 INFO org.mortbay.log: jetty-6.1.26
2018-09-28 14:00:13,408 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2018-09-28 14:00:13,422 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-28 14:00:13,422 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-28 14:00:13,450 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2018-09-28 14:00:13,455 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2018-09-28 14:00:13,456 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2018-09-28 14:00:13,457 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2018-09-28 14:00:13,460 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = lag (auth:SIMPLE)
2018-09-28 14:00:13,460 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2018-09-28 14:00:13,460 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2018-09-28 14:00:13,460 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2018-09-28 14:00:13,477 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-09-28 14:00:13,486 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2018-09-28 14:00:13,486 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2018-09-28 14:00:13,488 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2018-09-28 14:00:13,488 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2018 Sep 28 14:00:13
2018-09-28 14:00:13,489 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2018-09-28 14:00:13,489 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-28 14:00:13,490 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2018-09-28 14:00:13,490 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2018-09-28 14:00:13,497 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2018-09-28 14:00:13,497 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS
2018-09-28 14:00:13,499 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2018-09-28 14:00:13,500 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2018-09-28 14:00:13,500 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2018-09-28 14:00:13,500 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2018-09-28 14:00:13,500 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2018-09-28 14:00:13,500 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2018-09-28 14:00:13,500 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2018-09-28 14:00:13,500 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2018-09-28 14:00:13,500 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2018-09-28 14:00:13,500 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2018-09-28 14:00:13,500 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2018-09-28 14:00:13,501 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2018-09-28 14:00:13,537 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2018-09-28 14:00:13,537 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-28 14:00:13,537 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2018-09-28 14:00:13,537 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2018-09-28 14:00:13,537 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2018-09-28 14:00:13,537 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2018-09-28 14:00:13,537 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2018-09-28 14:00:13,540 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false
2018-09-28 14:00:13,543 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2018-09-28 14:00:13,543 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-28 14:00:13,543 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2018-09-28 14:00:13,543 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2018-09-28 14:00:13,545 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2018-09-28 14:00:13,545 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2018-09-28 14:00:13,545 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2018-09-28 14:00:13,548 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2018-09-28 14:00:13,548 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2018-09-28 14:00:13,549 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2018-09-28 14:00:13,549 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-28 14:00:13,549 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2018-09-28 14:00:13,549 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2018-09-28 14:00:13,551 WARN org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-lag/dfs/name does not exist
2018-09-28 14:00:13,552 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-lag/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:375)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:226)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1048)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:681)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:666)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:728)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:953)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:932)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1741)
2018-09-28 14:00:13,554 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2018-09-28 14:00:13,655 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2018-09-28 14:00:13,656 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2018-09-28 14:00:13,656 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2018-09-28 14:00:13,656 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-lag/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:375)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:226)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1048)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:681)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:666)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:728)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:953)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:932)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1741)
2018-09-28 14:00:13,659 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1: org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-lag/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
2018-09-28 14:00:13,662 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at lag-Predator-G3-571/127.0.1.1
************************************************************/
2018-09-28 14:03:15,198 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = lag-Predator-G3-571/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.9.1
STARTUP_MSG:   classpath = /home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/etc/hadoop:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/activation-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-auth-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jettison-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/junit-4.11.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-annotations-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsch-0.1.54.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-nfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/fst-2.50.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-core-1.8.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-1.7.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-framework-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsp-api-2.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/gson-2.2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-recipes-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jcip-annotations-1.0-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-digester-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-net-3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-smart-1.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsch-0.1.54.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-router-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-registry-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-api-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.9.1.jar:/usr/lib/jvm/java-8-openjdk-amd64//lib/tools.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e30710aea4e6e55e69372929106cf119af06fd0e; compiled by 'root' on 2018-04-16T09:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-28 14:03:15,206 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-28 14:03:15,209 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2018-09-28 14:03:15,322 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-28 14:03:15,383 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-09-28 14:03:15,383 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2018-09-28 14:03:15,397 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2018-09-28 14:03:15,399 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2018-09-28 14:03:15,497 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-09-28 14:03:15,504 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2018-09-28 14:03:15,534 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-28 14:03:15,539 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-28 14:03:15,554 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2018-09-28 14:03:15,557 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-28 14:03:15,559 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2018-09-28 14:03:15,559 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-28 14:03:15,559 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-28 14:03:15,629 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2018-09-28 14:03:15,629 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2018-09-28 14:03:15,638 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2018-09-28 14:03:15,638 INFO org.mortbay.log: jetty-6.1.26
2018-09-28 14:03:15,747 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2018-09-28 14:03:15,760 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-28 14:03:15,760 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-28 14:03:15,797 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2018-09-28 14:03:15,802 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2018-09-28 14:03:15,802 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2018-09-28 14:03:15,803 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2018-09-28 14:03:15,806 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = lag (auth:SIMPLE)
2018-09-28 14:03:15,806 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2018-09-28 14:03:15,806 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2018-09-28 14:03:15,806 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2018-09-28 14:03:15,821 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-09-28 14:03:15,831 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2018-09-28 14:03:15,831 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2018-09-28 14:03:15,833 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2018-09-28 14:03:15,833 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2018 Sep 28 14:03:15
2018-09-28 14:03:15,834 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2018-09-28 14:03:15,834 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-28 14:03:15,834 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2018-09-28 14:03:15,835 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2018-09-28 14:03:15,842 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2018-09-28 14:03:15,842 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS
2018-09-28 14:03:15,844 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2018-09-28 14:03:15,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2018-09-28 14:03:15,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2018-09-28 14:03:15,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2018-09-28 14:03:15,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2018-09-28 14:03:15,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2018-09-28 14:03:15,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2018-09-28 14:03:15,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2018-09-28 14:03:15,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2018-09-28 14:03:15,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2018-09-28 14:03:15,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2018-09-28 14:03:15,846 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2018-09-28 14:03:15,882 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2018-09-28 14:03:15,882 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-28 14:03:15,882 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2018-09-28 14:03:15,882 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2018-09-28 14:03:15,882 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2018-09-28 14:03:15,882 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2018-09-28 14:03:15,883 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2018-09-28 14:03:15,885 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false
2018-09-28 14:03:15,888 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2018-09-28 14:03:15,888 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-28 14:03:15,888 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2018-09-28 14:03:15,888 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2018-09-28 14:03:15,890 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2018-09-28 14:03:15,890 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2018-09-28 14:03:15,890 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2018-09-28 14:03:15,892 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2018-09-28 14:03:15,893 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2018-09-28 14:03:15,894 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2018-09-28 14:03:15,894 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-28 14:03:15,894 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2018-09-28 14:03:15,894 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2018-09-28 14:03:15,915 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-lag/dfs/name/in_use.lock acquired by nodename 8481@lag-Predator-G3-571
2018-09-28 14:03:15,929 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-lag/dfs/name/current
2018-09-28 14:03:15,929 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2018-09-28 14:03:15,930 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2018-09-28 14:03:15,987 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2018-09-28 14:03:16,002 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2018-09-28 14:03:16,002 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000
2018-09-28 14:03:16,005 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2018-09-28 14:03:16,005 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2018-09-28 14:03:16,131 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2018-09-28 14:03:16,131 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 235 msecs
2018-09-28 14:03:16,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2018-09-28 14:03:16,394 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-09-28 14:03:16,400 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2018-09-28 14:03:16,432 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2018-09-28 14:03:16,443 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2018-09-28 14:03:16,449 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues
2018-09-28 14:03:16,449 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2018-09-28 14:03:16,449 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2018-09-28 14:03:16,449 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2018-09-28 14:03:16,456 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2018-09-28 14:03:16,456 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2018-09-28 14:03:16,456 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2018-09-28 14:03:16,456 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2018-09-28 14:03:16,456 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2018-09-28 14:03:16,456 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 7 msec
2018-09-28 14:03:16,467 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-28 14:03:16,467 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2018-09-28 14:03:16,470 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2018-09-28 14:03:16,472 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2018-09-28 14:03:16,472 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2018-09-28 14:03:16,474 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 2 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2018-09-28 14:03:16,476 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2018-09-28 14:03:20,824 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=84d002ea-f1b0-4ced-bf0b-ca0a4c752928, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-d70c2304-fa4f-4780-840f-ce1e822c5a3d;nsid=1348204911;c=1538136165972) storage 84d002ea-f1b0-4ced-bf0b-ca0a4c752928
2018-09-28 14:03:20,825 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2018-09-28 14:03:20,825 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 84d002ea-f1b0-4ced-bf0b-ca0a4c752928 (127.0.0.1:50010).
2018-09-28 14:03:20,870 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-908b0096-0ea6-4ced-be41-5dcf2d33a7f1 for DN 127.0.0.1:50010
2018-09-28 14:03:20,893 INFO BlockStateChange: BLOCK* processReport 0x52fb1f6ed7e5dd2c: Processing first storage report for DS-908b0096-0ea6-4ced-be41-5dcf2d33a7f1 from datanode 84d002ea-f1b0-4ced-bf0b-ca0a4c752928
2018-09-28 14:03:20,894 INFO BlockStateChange: BLOCK* processReport 0x52fb1f6ed7e5dd2c: from storage DS-908b0096-0ea6-4ced-be41-5dcf2d33a7f1 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=84d002ea-f1b0-4ced-bf0b-ca0a4c752928, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-d70c2304-fa4f-4780-840f-ce1e822c5a3d;nsid=1348204911;c=1538136165972), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2018-09-28 14:26:25,378 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-09-28 14:26:25,379 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-09-28 14:26:25,379 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1, 1
2018-09-28 14:26:25,380 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 42 
2018-09-28 14:26:25,382 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 44 
2018-09-28 14:26:25,384 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000001-0000000000000000002
2018-09-28 14:26:25,418 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2018-09-28 14:26:26,108 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000, fileSize: 320. Sent total: 320 bytes. Size of last segment intended to send: -1 bytes.
2018-09-28 14:26:26,130 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000001-0000000000000000002, fileSize: 42. Sent total: 42 bytes. Size of last segment intended to send: -1 bytes.
2018-09-28 14:26:26,329 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Combined time for fsimage download and fsync to all disks took 0.00s. The fsimage download took 0.00s at 0.00 KB/s. Synchronous (fsync) write to disk of /tmp/hadoop-lag/dfs/name/current/fsimage.ckpt_0000000000000000002 took 0.00s.
2018-09-28 14:26:26,329 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 320 bytes.
2018-09-28 14:26:26,337 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2018-09-28 14:43:55,315 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 8 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 56 
2018-09-28 14:49:21,275 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 8 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 59 
2018-09-28 14:53:39,174 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 5 Total time for transactions(ms): 8 Number of transactions batched in Syncs: 0 Number of syncs: 5 SyncTimes(ms): 61 
2018-09-28 14:53:39,276 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/adelaide2.txt._COPYING_
2018-09-28 14:53:39,389 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741825_1001 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/adelaide2.txt._COPYING_
2018-09-28 14:53:39,794 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/adelaide2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:39,822 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/aiglon1.txt._COPYING_
2018-09-28 14:53:39,840 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/aiglon1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:39,857 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/amoursjaunes1.txt._COPYING_
2018-09-28 14:53:39,873 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/amoursjaunes1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:39,893 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/andromaque1.txt._COPYING_
2018-09-28 14:53:39,910 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/andromaque1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:39,931 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/ane1.txt._COPYING_
2018-09-28 14:53:39,958 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/ane1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:40,001 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/annee1.txt._COPYING_
2018-09-28 14:53:40,019 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/annee1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:40,040 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/antiquites1.txt._COPYING_
2018-09-28 14:53:40,061 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/antiquites1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:40,082 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/archiduc1.txt._COPYING_
2018-09-28 14:53:40,287 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/archiduc1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:40,312 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/arebours1.txt._COPYING_
2018-09-28 14:53:40,332 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/arebours1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:40,352 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/argent1.txt._COPYING_
2018-09-28 14:53:40,372 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/argent1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:40,391 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/armance2.txt._COPYING_
2018-09-28 14:53:40,416 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/armance2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:40,462 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/arriamar2.txt._COPYING_
2018-09-28 14:53:40,483 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/arriamar2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:40,504 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/artgrdp1.txt._COPYING_
2018-09-28 14:53:40,529 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/artgrdp1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:40,553 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/athalie2.txt._COPYING_
2018-09-28 14:53:40,576 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/athalie2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:40,600 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/avare2.txt._COPYING_
2018-09-28 14:53:40,626 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/avare2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:40,648 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/aziyade1.txt._COPYING_
2018-09-28 14:53:40,676 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/aziyade1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:40,700 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/ballades1.txt._COPYING_
2018-09-28 14:53:40,724 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/ballades1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:40,750 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/ballon1.txt._COPYING_
2018-09-28 14:53:40,776 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/ballon1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:40,818 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/becass2.txt._COPYING_
2018-09-28 14:53:40,843 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/becass2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:40,868 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/begum2.txt._COPYING_
2018-09-28 14:53:40,894 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/begum2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:40,920 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/belami2.txt._COPYING_
2018-09-28 14:53:40,951 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/belami2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:40,985 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/bertris1.txt._COPYING_
2018-09-28 14:53:41,006 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/bertris1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,029 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/blocus3.txt._COPYING_
2018-09-28 14:53:41,050 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/blocus3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,073 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/boule2.txt._COPYING_
2018-09-28 14:53:41,093 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/boule2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,116 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/bounty1.txt._COPYING_
2018-09-28 14:53:41,137 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/bounty1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,163 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741850_1026, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/bouvard2.txt._COPYING_
2018-09-28 14:53:41,187 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/bouvard2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,210 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741851_1027, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/bovary3.txt._COPYING_
2018-09-28 14:53:41,236 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/bovary3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,258 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741852_1028, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/bretagne1.txt._COPYING_
2018-09-28 14:53:41,280 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/bretagne1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,305 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741853_1029, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/britannicus1.txt._COPYING_
2018-09-28 14:53:41,327 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/britannicus1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,374 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741854_1030, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/bugjarg1.txt._COPYING_
2018-09-28 14:53:41,396 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/bugjarg1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,419 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741855_1031, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/candide3.txt._COPYING_
2018-09-28 14:53:41,438 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/candide3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,463 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741856_1032, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/caribou1.txt._COPYING_
2018-09-28 14:53:41,484 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/caribou1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,509 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741857_1033, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/centurion1.txt._COPYING_
2018-09-28 14:53:41,531 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/centurion1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,554 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741858_1034, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/chabert3.txt._COPYING_
2018-09-28 14:53:41,573 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/chabert3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,594 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741859_1035, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/chambre2.txt._COPYING_
2018-09-28 14:53:41,609 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/chambre2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,628 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741860_1036, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/champi1.txt._COPYING_
2018-09-28 14:53:41,645 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/champi1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,664 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741861_1037, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/chartre1.txt._COPYING_
2018-09-28 14:53:41,685 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/chartre1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,704 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741862_1038, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/chef2.txt._COPYING_
2018-09-28 14:53:41,721 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/chef2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,741 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741863_1039, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/cinna2.txt._COPYING_
2018-09-28 14:53:41,761 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/cinna2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,805 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741864_1040, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/cleves2.txt._COPYING_
2018-09-28 14:53:41,828 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/cleves2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,852 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741865_1041, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/cocuage2.txt._COPYING_
2018-09-28 14:53:41,871 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/cocuage2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,896 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741866_1042, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/colliergriffes1.txt._COPYING_
2018-09-28 14:53:41,915 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/colliergriffes1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,939 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741867_1043, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/colomba1.txt._COPYING_
2018-09-28 14:53:41,960 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/colomba1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:41,984 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741868_1044, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/commerce1.txt._COPYING_
2018-09-28 14:53:42,003 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/commerce1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:42,027 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741869_1045, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/confessions1.txt._COPYING_
2018-09-28 14:53:42,060 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/confessions1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:42,092 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741870_1046, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/conscrit2.txt._COPYING_
2018-09-28 14:53:42,114 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/conscrit2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:42,137 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741871_1047, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/consider1.txt._COPYING_
2018-09-28 14:53:42,157 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/consider1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:42,180 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741872_1048, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/contemplA2.txt._COPYING_
2018-09-28 14:53:42,200 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/contemplA2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:42,392 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741873_1049, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/contemplB2.txt._COPYING_
2018-09-28 14:53:42,413 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/contemplB2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:42,437 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741874_1050, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/contrat1.txt._COPYING_
2018-09-28 14:53:42,457 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/contrat1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:42,476 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741875_1051, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/crainque1.txt._COPYING_
2018-09-28 14:53:42,495 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/crainque1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:42,513 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741876_1052, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/curee2.txt._COPYING_
2018-09-28 14:53:42,537 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/curee2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:42,579 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741877_1053, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/cyrano1.txt._COPYING_
2018-09-28 14:53:42,599 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/cyrano1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:42,623 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741878_1054, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/daphnis1.txt._COPYING_
2018-09-28 14:53:42,642 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/daphnis1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:42,662 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741879_1055, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/ddhc3.txt._COPYING_
2018-09-28 14:53:42,681 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/ddhc3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:42,704 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741880_1056, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/desespere1.txt._COPYING_
2018-09-28 14:53:42,726 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/desespere1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:42,744 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741881_1057, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/diableam3.txt._COPYING_
2018-09-28 14:53:42,762 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/diableam3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:42,784 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741882_1058, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/diablecor2.txt._COPYING_
2018-09-28 14:53:42,803 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/diablecor2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:42,824 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741883_1059, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/dicohisto1.txt._COPYING_
2018-09-28 14:53:42,840 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/dicohisto1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:42,860 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741884_1060, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/dicotypo1.txt._COPYING_
2018-09-28 14:53:42,879 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/dicotypo1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:42,903 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741885_1061, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/domi3.txt._COPYING_
2018-09-28 14:53:42,925 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/domi3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:42,968 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741886_1062, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/domjuan2.txt._COPYING_
2018-09-28 14:53:42,986 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/domjuan2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,009 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741887_1063, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/drageoir2.txt._COPYING_
2018-09-28 14:53:43,029 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/drageoir2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,050 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741888_1064, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/drole2.txt._COPYING_
2018-09-28 14:53:43,073 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/drole2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,096 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741889_1065, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/ecole1.txt._COPYING_
2018-09-28 14:53:43,110 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/ecole1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,128 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741890_1066, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/educati1.txt._COPYING_
2018-09-28 14:53:43,146 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/educati1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,164 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741891_1067, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/elixir2.txt._COPYING_
2018-09-28 14:53:43,178 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/elixir2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,196 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741892_1068, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/erenouv1.txt._COPYING_
2018-09-28 14:53:43,216 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/erenouv1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,235 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741893_1069, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/espece1.txt._COPYING_
2018-09-28 14:53:43,265 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/espece1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,305 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741894_1070, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/etuit1.txt._COPYING_
2018-09-28 14:53:43,325 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/etuit1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,351 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741895_1071, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/excentlang1.txt._COPYING_
2018-09-28 14:53:43,367 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/excentlang1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,385 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741896_1072, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/fabulistes1.txt._COPYING_
2018-09-28 14:53:43,409 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/fabulistes1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,449 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741897_1073, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/fadette1.txt._COPYING_
2018-09-28 14:53:43,469 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/fadette1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,494 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741898_1074, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/farcefran1.txt._COPYING_
2018-09-28 14:53:43,513 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/farcefran1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,537 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741899_1075, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/femtin1.txt._COPYING_
2018-09-28 14:53:43,557 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/femtin1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,579 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741900_1076, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/feuilles1.txt._COPYING_
2018-09-28 14:53:43,597 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/feuilles1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,620 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741901_1077, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/figchose1.txt._COPYING_
2018-09-28 14:53:43,637 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/figchose1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,656 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741902_1078, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/flandeux1.txt._COPYING_
2018-09-28 14:53:43,675 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/flandeux1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,696 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741903_1079, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/football1.txt._COPYING_
2018-09-28 14:53:43,715 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/football1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,733 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741904_1080, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/gargantua2.txt._COPYING_
2018-09-28 14:53:43,746 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/gargantua2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,765 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741905_1081, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/gaspard2.txt._COPYING_
2018-09-28 14:53:43,777 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/gaspard2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,796 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741906_1082, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/germinal1.txt._COPYING_
2018-09-28 14:53:43,813 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/germinal1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,840 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741907_1083, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/germinie3.txt._COPYING_
2018-09-28 14:53:43,887 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/germinie3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,907 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741908_1084, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/gilblas1.txt._COPYING_
2018-09-28 14:53:43,928 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/gilblas1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:43,945 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741909_1085, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/guizeur1.txt._COPYING_
2018-09-28 14:53:43,980 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/guizeur1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:44,011 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741910_1086, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/hamlet1.txt._COPYING_
2018-09-28 14:53:44,041 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/hamlet1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:44,059 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741911_1087, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/harmonie1.txt._COPYING_
2018-09-28 14:53:44,077 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/harmonie1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:44,099 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741912_1088, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/historiettes2.txt._COPYING_
2018-09-28 14:53:44,123 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741912_1088 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/historiettes2.txt._COPYING_
2018-09-28 14:53:44,524 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/historiettes2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:44,732 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741913_1089, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/homecu1.txt._COPYING_
2018-09-28 14:53:44,749 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741913_1089 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/homecu1.txt._COPYING_
2018-09-28 14:53:45,150 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/homecu1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:45,176 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741914_1090, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/horla3.txt._COPYING_
2018-09-28 14:53:45,195 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/horla3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:45,214 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741915_1091, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/hugoshak1.txt._COPYING_
2018-09-28 14:53:45,229 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/hugoshak1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:45,251 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741916_1092, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/humilite3.txt._COPYING_
2018-09-28 14:53:45,270 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/humilite3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:45,292 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741917_1093, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/ideolo1.txt._COPYING_
2018-09-28 14:53:45,310 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/ideolo1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:45,331 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741918_1094, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/illusion1.txt._COPYING_
2018-09-28 14:53:45,343 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/illusion1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:45,360 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741919_1095, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/inquisit2.txt._COPYING_
2018-09-28 14:53:45,369 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/inquisit2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:45,382 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741920_1096, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/interpret2.txt._COPYING_
2018-09-28 14:53:45,389 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/interpret2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:45,402 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741921_1097, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/iphigenie1.txt._COPYING_
2018-09-28 14:53:45,410 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/iphigenie1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:45,426 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741922_1098, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/irnois2.txt._COPYING_
2018-09-28 14:53:45,437 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/irnois2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:45,454 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741923_1099, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/jaccuse3.txt._COPYING_
2018-09-28 14:53:45,462 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/jaccuse3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:45,475 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741924_1100, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/jacques1.txt._COPYING_
2018-09-28 14:53:45,489 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/jacques1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:45,515 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741925_1101, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/journalism1.txt._COPYING_
2018-09-28 14:53:45,535 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/journalism1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:45,553 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741926_1102, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/journbloy1.txt._COPYING_
2018-09-28 14:53:45,593 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/journbloy1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:45,610 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741927_1103, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/justice1.txt._COPYING_
2018-09-28 14:53:45,624 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/justice1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:45,641 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741928_1104, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/lafille2.txt._COPYING_
2018-09-28 14:53:45,656 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741928_1104 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/lafille2.txt._COPYING_
2018-09-28 14:53:46,057 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/lafille2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:46,081 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741929_1105, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/lecid1.txt._COPYING_
2018-09-28 14:53:46,099 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/lecid1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:46,122 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741930_1106, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/legend1.txt._COPYING_
2018-09-28 14:53:46,136 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/legend1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:46,157 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741931_1107, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/legendet21.txt._COPYING_
2018-09-28 14:53:46,173 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/legendet21.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:46,195 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741932_1108, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/leibnitzdiv1.txt._COPYING_
2018-09-28 14:53:46,212 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/leibnitzdiv1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:46,235 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741933_1109, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/lejeu1.txt._COPYING_
2018-09-28 14:53:46,251 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/lejeu1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:46,269 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741934_1110, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/lepetitchose1.txt._COPYING_
2018-09-28 14:53:46,284 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/lepetitchose1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:46,302 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741935_1111, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/lessoirees1.txt._COPYING_
2018-09-28 14:53:46,357 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/lessoirees1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:46,402 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741936_1112, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/letphi1.txt._COPYING_
2018-09-28 14:53:46,420 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/letphi1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:46,442 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741937_1113, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/lettresecrites1.txt._COPYING_
2018-09-28 14:53:46,462 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/lettresecrites1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:46,483 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741938_1114, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/lettresjuives11.txt._COPYING_
2018-09-28 14:53:46,507 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741938_1114 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/lettresjuives11.txt._COPYING_
2018-09-28 14:53:46,909 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/lettresjuives11.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:46,931 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741939_1115, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/lettresjuives231.txt._COPYING_
2018-09-28 14:53:46,960 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/lettresjuives231.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:46,990 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741940_1116, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/lettresjuives451.txt._COPYING_
2018-09-28 14:53:47,038 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/lettresjuives451.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:47,060 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741941_1117, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/lettresjuives6781.txt._COPYING_
2018-09-28 14:53:47,077 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741941_1117 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/lettresjuives6781.txt._COPYING_
2018-09-28 14:53:47,478 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/lettresjuives6781.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:47,501 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741942_1118, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/lettresreli2.txt._COPYING_
2018-09-28 14:53:47,518 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/lettresreli2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:47,536 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741943_1119, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/levieux2.txt._COPYING_
2018-09-28 14:53:47,551 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/levieux2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:47,569 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741944_1120, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/liaisons3.txt._COPYING_
2018-09-28 14:53:47,595 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/liaisons3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:47,629 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741945_1121, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/licong1.txt._COPYING_
2018-09-28 14:53:47,648 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/licong1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:47,667 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741946_1122, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/lmain3.txt._COPYING_
2018-09-28 14:53:47,678 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/lmain3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:47,692 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741947_1123, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/lutrin1.txt._COPYING_
2018-09-28 14:53:47,699 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/lutrin1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:47,710 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741948_1124, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/m702douai2.txt._COPYING_
2018-09-28 14:53:47,718 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/m702douai2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:47,730 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741949_1125, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/machine3.txt._COPYING_
2018-09-28 14:53:47,736 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741949_1125 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/machine3.txt._COPYING_
2018-09-28 14:53:48,138 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/machine3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:48,155 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741950_1126, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/manifeste2.txt._COPYING_
2018-09-28 14:53:48,167 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/manifeste2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:48,184 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741951_1127, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/manon2.txt._COPYING_
2018-09-28 14:53:48,195 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/manon2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:48,383 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741952_1128, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/marie1.txt._COPYING_
2018-09-28 14:53:48,406 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/marie1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:48,444 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741953_1129, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/maximes2.txt._COPYING_
2018-09-28 14:53:48,460 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/maximes2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:48,481 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741954_1130, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/medipoet1.txt._COPYING_
2018-09-28 14:53:48,499 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/medipoet1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:48,517 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741955_1131, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/medit3.txt._COPYING_
2018-09-28 14:53:48,535 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/medit3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:48,557 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741956_1132, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/medita1.txt._COPYING_
2018-09-28 14:53:48,576 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/medita1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:48,597 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741957_1133, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/methode3.txt._COPYING_
2018-09-28 14:53:48,612 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/methode3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:48,628 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741958_1134, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/micromeg3.txt._COPYING_
2018-09-28 14:53:48,643 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741958_1134 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/micromeg3.txt._COPYING_
2018-09-28 14:53:49,044 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/micromeg3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:49,065 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741959_1135, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/monadologie1.txt._COPYING_
2018-09-28 14:53:49,082 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/monadologie1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:49,103 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741960_1136, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/monde1.txt._COPYING_
2018-09-28 14:53:49,121 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/monde1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:49,143 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741961_1137, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/montbard1.txt._COPYING_
2018-09-28 14:53:49,160 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/montbard1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:49,179 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741962_1138, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/morte2.txt._COPYING_
2018-09-28 14:53:49,195 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/morte2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:49,216 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741963_1139, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/mousque1.txt._COPYING_
2018-09-28 14:53:49,239 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/mousque1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:49,260 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741964_1140, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/nddp1.txt._COPYING_
2018-09-28 14:53:49,313 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/nddp1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:49,360 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741965_1141, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/neveu2.txt._COPYING_
2018-09-28 14:53:49,378 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/neveu2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:49,399 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741966_1142, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/nouvmedi1.txt._COPYING_
2018-09-28 14:53:49,417 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/nouvmedi1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:49,443 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741967_1143, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/odespolit1.txt._COPYING_
2018-09-28 14:53:49,461 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/odespolit1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:49,479 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741968_1144, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/opinions3.txt._COPYING_
2018-09-28 14:53:49,493 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741968_1144 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/opinions3.txt._COPYING_
2018-09-28 14:53:49,895 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/opinions3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:49,918 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741969_1145, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/oriental1.txt._COPYING_
2018-09-28 14:53:49,934 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/oriental1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:49,952 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741970_1146, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/paresse3.txt._COPYING_
2018-09-28 14:53:49,966 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741970_1146 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/paresse3.txt._COPYING_
2018-09-28 14:53:50,367 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/paresse3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:50,388 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741971_1147, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/partiecamp2.txt._COPYING_
2018-09-28 14:53:50,404 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741971_1147 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/partiecamp2.txt._COPYING_
2018-09-28 14:53:50,806 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/partiecamp2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:50,827 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741972_1148, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/pascaldiv1.txt._COPYING_
2018-09-28 14:53:50,844 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/pascaldiv1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:50,866 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741973_1149, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/pascalpetits1.txt._COPYING_
2018-09-28 14:53:50,883 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/pascalpetits1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:50,905 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741974_1150, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/penseesXX1.txt._COPYING_
2018-09-28 14:53:50,921 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/penseesXX1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:50,939 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741975_1151, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/pensepict1.txt._COPYING_
2018-09-28 14:53:50,955 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/pensepict1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:50,972 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741976_1152, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/phedre2.txt._COPYING_
2018-09-28 14:53:50,988 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/phedre2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:51,005 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741977_1153, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/plural3.txt._COPYING_
2018-09-28 14:53:51,021 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/plural3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:51,038 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741978_1154, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/poemallar1.txt._COPYING_
2018-09-28 14:53:51,052 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741978_1154 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/poemallar1.txt._COPYING_
2018-09-28 14:53:51,453 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/poemallar1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:51,477 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741979_1155, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/poemesadv1.txt._COPYING_
2018-09-28 14:53:51,492 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/poemesadv1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:51,510 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741980_1156, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/presseguerre1.txt._COPYING_
2018-09-28 14:53:51,524 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/presseguerre1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:51,541 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741981_1157, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/preuves1.txt._COPYING_
2018-09-28 14:53:51,556 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741981_1157 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/preuves1.txt._COPYING_
2018-09-28 14:53:51,957 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/preuves1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:51,973 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741982_1158, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/propriete1.txt._COPYING_
2018-09-28 14:53:51,989 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741982_1158 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/propriete1.txt._COPYING_
2018-09-28 14:53:52,390 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/propriete1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:52,411 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741983_1159, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/pucelle1.txt._COPYING_
2018-09-28 14:53:52,430 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741983_1159 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/pucelle1.txt._COPYING_
2018-09-28 14:53:52,831 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/pucelle1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:52,848 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741984_1160, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/quatrevt1.txt._COPYING_
2018-09-28 14:53:52,871 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/quatrevt1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:52,890 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741985_1161, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/raison2.txt._COPYING_
2018-09-28 14:53:52,905 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/raison2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:52,921 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741986_1162, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/rayons1.txt._COPYING_
2018-09-28 14:53:52,936 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741986_1162 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/rayons1.txt._COPYING_
2018-09-28 14:53:53,337 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/rayons1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:53,356 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741987_1163, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/ren05101.txt._COPYING_
2018-09-28 14:53:53,374 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741987_1163 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/ren05101.txt._COPYING_
2018-09-28 14:53:53,776 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/ren05101.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:53,796 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741988_1164, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/ren87922.txt._COPYING_
2018-09-28 14:53:53,814 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/ren87922.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:53,835 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741989_1165, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/ren93982.txt._COPYING_
2018-09-28 14:53:53,855 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741989_1165 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/ren93982.txt._COPYING_
2018-09-28 14:53:54,256 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/ren93982.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:54,276 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741990_1166, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/ren99041.txt._COPYING_
2018-09-28 14:53:54,296 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/ren99041.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:54,313 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741991_1167, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/reveries3.txt._COPYING_
2018-09-28 14:53:54,327 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741991_1167 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/reveries3.txt._COPYING_
2018-09-28 14:53:54,729 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/reveries3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:54,749 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741992_1168, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/rldased3.txt._COPYING_
2018-09-28 14:53:54,766 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/rldased3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:54,787 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741993_1169, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/robur1.txt._COPYING_
2018-09-28 14:53:54,805 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/robur1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:54,827 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741994_1170, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/roland2.txt._COPYING_
2018-09-28 14:53:54,841 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/roland2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:54,857 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741995_1171, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/roletrav2.txt._COPYING_
2018-09-28 14:53:54,872 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741995_1171 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/roletrav2.txt._COPYING_
2018-09-28 14:53:55,273 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/roletrav2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:55,294 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741996_1172, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/rouge1.txt._COPYING_
2018-09-28 14:53:55,318 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741996_1172 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/rouge1.txt._COPYING_
2018-09-28 14:53:55,720 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/rouge1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:55,741 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741997_1173, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/rousgene1.txt._COPYING_
2018-09-28 14:53:55,759 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/rousgene1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:55,779 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741998_1174, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/ruesboi1.txt._COPYING_
2018-09-28 14:53:55,797 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/ruesboi1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:55,817 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741999_1175, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/salalion1.txt._COPYING_
2018-09-28 14:53:55,831 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/salalion1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:55,848 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742000_1176, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/salammb1.txt._COPYING_
2018-09-28 14:53:55,866 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/salammb1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:55,883 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742001_1177, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/satan1.txt._COPYING_
2018-09-28 14:53:55,901 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/satan1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:55,922 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742002_1178, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/scapin2.txt._COPYING_
2018-09-28 14:53:55,936 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742002_1178 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/scapin2.txt._COPYING_
2018-09-28 14:53:56,337 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/scapin2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:56,355 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742003_1179, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/scihyp2.txt._COPYING_
2018-09-28 14:53:56,395 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/scihyp2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:56,417 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742004_1180, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/septfem2.txt._COPYING_
2018-09-28 14:53:56,431 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742004_1180 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/septfem2.txt._COPYING_
2018-09-28 14:53:56,832 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/septfem2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:57,011 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742005_1181, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/smarra1.txt._COPYING_
2018-09-28 14:53:57,029 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742005_1181 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/smarra1.txt._COPYING_
2018-09-28 14:53:57,430 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/smarra1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:57,448 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742006_1182, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/souveni1.txt._COPYING_
2018-09-28 14:53:57,599 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/souveni1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:57,616 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742007_1183, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/supplem2.txt._COPYING_
2018-09-28 14:53:57,633 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/supplem2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:57,654 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742008_1184, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/tartuf2.txt._COPYING_
2018-09-28 14:53:57,671 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/tartuf2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:57,691 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742009_1185, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/tdm80j2.txt._COPYING_
2018-09-28 14:53:57,710 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742009_1185 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/tdm80j2.txt._COPYING_
2018-09-28 14:53:58,112 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/tdm80j2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:58,134 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742010_1186, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/theodicee1.txt._COPYING_
2018-09-28 14:53:58,291 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/theodicee1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:58,312 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742011_1187, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/theoriephys1.txt._COPYING_
2018-09-28 14:53:58,332 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742011_1187 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/theoriephys1.txt._COPYING_
2018-09-28 14:53:58,733 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/theoriephys1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:58,755 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742012_1188, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/thotri1.txt._COPYING_
2018-09-28 14:53:58,897 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/thotri1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:58,916 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742013_1189, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/tlun3.txt._COPYING_
2018-09-28 14:53:58,932 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742013_1189 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/tlun3.txt._COPYING_
2018-09-28 14:53:59,333 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/tlun3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:59,350 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742014_1190, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/tome1malherbe1.txt._COPYING_
2018-09-28 14:53:59,569 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742014_1190 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/tome1malherbe1.txt._COPYING_
2018-09-28 14:53:59,970 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/tome1malherbe1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:53:59,989 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742015_1191, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/troylion1.txt._COPYING_
2018-09-28 14:54:01,276 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742015_1191 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/troylion1.txt._COPYING_
2018-09-28 14:54:01,678 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/troylion1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:54:01,695 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742016_1192, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/ulenspiegel1.txt._COPYING_
2018-09-28 14:54:01,718 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/ulenspiegel1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:54:01,736 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742017_1193, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/uncoeur3.txt._COPYING_
2018-09-28 14:54:01,750 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/uncoeur3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:54:01,767 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742018_1194, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/unevie2.txt._COPYING_
2018-09-28 14:54:01,782 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/unevie2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:54:01,799 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742019_1195, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/utopique2.txt._COPYING_
2018-09-28 14:54:01,817 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/utopique2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:54:01,837 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742020_1196, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/verhafin1.txt._COPYING_
2018-09-28 14:54:01,858 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/verhafin1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:54:01,878 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742021_1197, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/viemars3.txt._COPYING_
2018-09-28 14:54:01,895 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/viemars3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:54:01,915 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742022_1198, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/vignypoesie1.txt._COPYING_
2018-09-28 14:54:01,934 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742022_1198 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/vignypoesie1.txt._COPYING_
2018-09-28 14:54:02,335 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/vignypoesie1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:54:02,356 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742023_1199, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/volpofin1.txt._COPYING_
2018-09-28 14:54:03,751 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/volpofin1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:54:03,772 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742024_1200, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/voltgene1.txt._COPYING_
2018-09-28 14:54:03,786 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/voltgene1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:54:03,804 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742025_1201, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/voyfran1.txt._COPYING_
2018-09-28 14:54:03,820 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742025_1201 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/voyfran1.txt._COPYING_
2018-09-28 14:54:04,221 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/voyfran1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:54:04,238 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742026_1202, replicas=127.0.0.1:50010 for /user/Lag/ABU/ABU/voylun3.txt._COPYING_
2018-09-28 14:54:05,534 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742026_1202 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ABU/voylun3.txt._COPYING_
2018-09-28 14:54:05,936 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ABU/voylun3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-819198910_1
2018-09-28 14:54:37,423 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#4 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:49854: java.io.FileNotFoundException: Path is not a file: /user/Lag/ABU/ABU
2018-09-28 15:10:16,427 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1225 Total time for transactions(ms): 95 Number of transactions batched in Syncs: 209 Number of syncs: 1014 SyncTimes(ms): 9338 
2018-09-28 15:10:29,296 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742027_1203, replicas=127.0.0.1:50010 for /user/lag/grep-temp-1164231544/_temporary/0/_temporary/attempt_local1822758675_0001_r_000000_0/part-r-00000
2018-09-28 15:10:29,315 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/grep-temp-1164231544/_temporary/0/_temporary/attempt_local1822758675_0001_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_-1904905987_1
2018-09-28 15:10:29,374 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/lag/grep-temp-1164231544/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1904905987_1
2018-09-28 15:12:21,074 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1239 Total time for transactions(ms): 95 Number of transactions batched in Syncs: 213 Number of syncs: 1026 SyncTimes(ms): 9375 
2018-09-28 15:12:21,153 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742028_1204, replicas=127.0.0.1:50010 for /user/Lag/hadoop/capacity-scheduler.xml._COPYING_
2018-09-28 15:12:21,200 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/capacity-scheduler.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,218 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742029_1205, replicas=127.0.0.1:50010 for /user/Lag/hadoop/configuration.xsl._COPYING_
2018-09-28 15:12:21,232 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/configuration.xsl._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,249 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742030_1206, replicas=127.0.0.1:50010 for /user/Lag/hadoop/container-executor.cfg._COPYING_
2018-09-28 15:12:21,263 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/container-executor.cfg._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,291 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742031_1207, replicas=127.0.0.1:50010 for /user/Lag/hadoop/core-site.xml._COPYING_
2018-09-28 15:12:21,302 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/core-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,319 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742032_1208, replicas=127.0.0.1:50010 for /user/Lag/hadoop/hadoop-env.cmd._COPYING_
2018-09-28 15:12:21,327 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/hadoop-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,340 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742033_1209, replicas=127.0.0.1:50010 for /user/Lag/hadoop/hadoop-env.sh._COPYING_
2018-09-28 15:12:21,347 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/hadoop-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,360 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742034_1210, replicas=127.0.0.1:50010 for /user/Lag/hadoop/hadoop-metrics.properties._COPYING_
2018-09-28 15:12:21,368 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/hadoop-metrics.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,381 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742035_1211, replicas=127.0.0.1:50010 for /user/Lag/hadoop/hadoop-metrics2.properties._COPYING_
2018-09-28 15:12:21,389 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/hadoop-metrics2.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,402 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742036_1212, replicas=127.0.0.1:50010 for /user/Lag/hadoop/hadoop-policy.xml._COPYING_
2018-09-28 15:12:21,409 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/hadoop-policy.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,429 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742037_1213, replicas=127.0.0.1:50010 for /user/Lag/hadoop/hdfs-site.xml._COPYING_
2018-09-28 15:12:21,442 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/hdfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,463 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742038_1214, replicas=127.0.0.1:50010 for /user/Lag/hadoop/httpfs-env.sh._COPYING_
2018-09-28 15:12:21,477 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/httpfs-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,498 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742039_1215, replicas=127.0.0.1:50010 for /user/Lag/hadoop/httpfs-log4j.properties._COPYING_
2018-09-28 15:12:21,508 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/httpfs-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,523 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742040_1216, replicas=127.0.0.1:50010 for /user/Lag/hadoop/httpfs-signature.secret._COPYING_
2018-09-28 15:12:21,531 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/httpfs-signature.secret._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,544 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742041_1217, replicas=127.0.0.1:50010 for /user/Lag/hadoop/httpfs-site.xml._COPYING_
2018-09-28 15:12:21,555 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/httpfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,569 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742042_1218, replicas=127.0.0.1:50010 for /user/Lag/hadoop/kms-acls.xml._COPYING_
2018-09-28 15:12:21,577 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/kms-acls.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,774 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742043_1219, replicas=127.0.0.1:50010 for /user/Lag/hadoop/kms-env.sh._COPYING_
2018-09-28 15:12:21,787 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/kms-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,809 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742044_1220, replicas=127.0.0.1:50010 for /user/Lag/hadoop/kms-log4j.properties._COPYING_
2018-09-28 15:12:21,818 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/kms-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,831 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742045_1221, replicas=127.0.0.1:50010 for /user/Lag/hadoop/kms-site.xml._COPYING_
2018-09-28 15:12:21,839 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/kms-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,851 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742046_1222, replicas=127.0.0.1:50010 for /user/Lag/hadoop/log4j.properties._COPYING_
2018-09-28 15:12:21,864 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,885 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742047_1223, replicas=127.0.0.1:50010 for /user/Lag/hadoop/mapred-env.cmd._COPYING_
2018-09-28 15:12:21,898 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/mapred-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,919 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742048_1224, replicas=127.0.0.1:50010 for /user/Lag/hadoop/mapred-env.sh._COPYING_
2018-09-28 15:12:21,928 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/mapred-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,942 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742049_1225, replicas=127.0.0.1:50010 for /user/Lag/hadoop/mapred-queues.xml.template._COPYING_
2018-09-28 15:12:21,953 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/mapred-queues.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,970 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742050_1226, replicas=127.0.0.1:50010 for /user/Lag/hadoop/mapred-site.xml.template._COPYING_
2018-09-28 15:12:21,981 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/mapred-site.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:21,996 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742051_1227, replicas=127.0.0.1:50010 for /user/Lag/hadoop/slaves._COPYING_
2018-09-28 15:12:22,004 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/slaves._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:22,017 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742052_1228, replicas=127.0.0.1:50010 for /user/Lag/hadoop/ssl-client.xml.example._COPYING_
2018-09-28 15:12:22,026 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/ssl-client.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:22,039 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742053_1229, replicas=127.0.0.1:50010 for /user/Lag/hadoop/ssl-server.xml.example._COPYING_
2018-09-28 15:12:22,047 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/ssl-server.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:22,060 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742054_1230, replicas=127.0.0.1:50010 for /user/Lag/hadoop/yarn-env.cmd._COPYING_
2018-09-28 15:12:22,069 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/yarn-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:22,083 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742055_1231, replicas=127.0.0.1:50010 for /user/Lag/hadoop/yarn-env.sh._COPYING_
2018-09-28 15:12:22,091 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/yarn-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:22,104 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742056_1232, replicas=127.0.0.1:50010 for /user/Lag/hadoop/yarn-site.xml._COPYING_
2018-09-28 15:12:22,117 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/hadoop/yarn-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-718155720_1
2018-09-28 15:12:38,065 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#4 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:50426: java.io.FileNotFoundException: Path is not a file: /user/Lag/ABU
2018-09-28 15:12:38,121 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#5 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:50426: java.io.FileNotFoundException: Path is not a file: /user/Lag/hadoop
2018-09-28 15:12:59,253 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2018-09-28 15:12:59,257 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at lag-Predator-G3-571/127.0.1.1
************************************************************/
2018-09-28 15:16:02,349 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = lag-Predator-G3-571/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.9.1
STARTUP_MSG:   classpath = /home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/etc/hadoop:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/activation-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-auth-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jettison-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/junit-4.11.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-annotations-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsch-0.1.54.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-nfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/fst-2.50.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-core-1.8.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-1.7.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-framework-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsp-api-2.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/gson-2.2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-recipes-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jcip-annotations-1.0-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-digester-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-net-3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-smart-1.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsch-0.1.54.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-router-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-registry-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-api-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.9.1.jar:/usr/lib/jvm/java-11-openjdk-amd64//lib/tools.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e30710aea4e6e55e69372929106cf119af06fd0e; compiled by 'root' on 2018-04-16T09:33Z
STARTUP_MSG:   java = 10.0.2
************************************************************/
2018-09-28 15:16:02,355 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-28 15:16:02,357 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2018-09-28 15:16:02,454 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-28 15:16:02,732 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-09-28 15:16:02,732 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2018-09-28 15:16:02,750 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2018-09-28 15:16:02,752 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2018-09-28 15:16:02,923 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-09-28 15:16:02,937 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2018-09-28 15:16:02,994 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-28 15:16:03,002 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-28 15:16:03,015 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2018-09-28 15:16:03,022 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-28 15:16:03,023 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2018-09-28 15:16:03,023 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-28 15:16:03,024 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-28 15:16:03,146 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2018-09-28 15:16:03,146 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2018-09-28 15:16:03,192 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2018-09-28 15:16:03,192 INFO org.mortbay.log: jetty-6.1.26
2018-09-28 15:16:03,303 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2018-09-28 15:16:03,321 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-28 15:16:03,321 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-28 15:16:03,344 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2018-09-28 15:16:03,351 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2018-09-28 15:16:03,352 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2018-09-28 15:16:03,353 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2018-09-28 15:16:03,377 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = lag (auth:SIMPLE)
2018-09-28 15:16:03,377 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2018-09-28 15:16:03,377 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2018-09-28 15:16:03,378 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2018-09-28 15:16:03,398 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-09-28 15:16:03,409 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2018-09-28 15:16:03,409 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2018-09-28 15:16:03,413 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2018-09-28 15:16:03,413 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2018 Sep 28 15:16:03
2018-09-28 15:16:03,414 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2018-09-28 15:16:03,415 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-28 15:16:03,415 INFO org.apache.hadoop.util.GSet: 2.0% max memory 1000 MB = 20 MB
2018-09-28 15:16:03,415 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2018-09-28 15:16:03,428 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2018-09-28 15:16:03,429 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS
2018-09-28 15:16:03,432 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2018-09-28 15:16:03,432 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2018-09-28 15:16:03,432 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2018-09-28 15:16:03,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2018-09-28 15:16:03,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2018-09-28 15:16:03,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2018-09-28 15:16:03,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2018-09-28 15:16:03,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2018-09-28 15:16:03,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2018-09-28 15:16:03,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2018-09-28 15:16:03,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2018-09-28 15:16:03,434 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2018-09-28 15:16:03,472 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2018-09-28 15:16:03,472 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-28 15:16:03,472 INFO org.apache.hadoop.util.GSet: 1.0% max memory 1000 MB = 10 MB
2018-09-28 15:16:03,472 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2018-09-28 15:16:03,474 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2018-09-28 15:16:03,474 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2018-09-28 15:16:03,474 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2018-09-28 15:16:03,476 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false
2018-09-28 15:16:03,480 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2018-09-28 15:16:03,480 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-28 15:16:03,481 INFO org.apache.hadoop.util.GSet: 0.25% max memory 1000 MB = 2.5 MB
2018-09-28 15:16:03,481 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2018-09-28 15:16:03,483 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2018-09-28 15:16:03,483 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2018-09-28 15:16:03,484 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2018-09-28 15:16:03,486 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2018-09-28 15:16:03,486 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2018-09-28 15:16:03,488 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2018-09-28 15:16:03,488 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-28 15:16:03,488 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 1000 MB = 307.2 KB
2018-09-28 15:16:03,488 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2018-09-28 15:16:03,517 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-lag/dfs/name/in_use.lock acquired by nodename 15940@lag-Predator-G3-571
2018-09-28 15:16:03,535 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-lag/dfs/name/current
2018-09-28 15:16:03,536 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2018-09-28 15:16:03,536 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2018-09-28 15:16:03,572 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2018-09-28 15:16:03,597 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2018-09-28 15:16:03,597 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000
2018-09-28 15:16:03,602 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2018-09-28 15:16:03,602 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2018-09-28 15:16:03,736 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2018-09-28 15:16:03,736 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 246 msecs
2018-09-28 15:16:03,861 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2018-09-28 15:16:03,865 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-09-28 15:16:03,896 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2018-09-28 15:16:03,970 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2018-09-28 15:16:03,977 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2018-09-28 15:16:03,983 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues
2018-09-28 15:16:03,983 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2018-09-28 15:16:03,983 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2018-09-28 15:16:03,983 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2018-09-28 15:16:03,992 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2018-09-28 15:16:03,992 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2018-09-28 15:16:03,993 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2018-09-28 15:16:03,993 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2018-09-28 15:16:03,993 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2018-09-28 15:16:03,993 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 9 msec
2018-09-28 15:16:04,005 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-28 15:16:04,005 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2018-09-28 15:16:04,007 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2018-09-28 15:16:04,012 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2018-09-28 15:16:04,013 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2018-09-28 15:16:04,015 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 2 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2018-09-28 15:16:04,023 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2018-09-28 15:17:12,907 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-09-28 15:17:12,908 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-09-28 15:17:12,908 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1, 3
2018-09-28 15:17:12,908 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 226 
2018-09-28 15:17:12,910 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 0 Number of syncs: 5 SyncTimes(ms): 228 
2018-09-28 15:17:12,911 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000001-0000000000000000004
2018-09-28 15:17:12,931 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 5
2018-09-28 15:17:23,989 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#9 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/adelaide2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,021 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#16 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/aiglon1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,039 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#23 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/amoursjaunes1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,056 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#30 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/andromaque1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,072 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#37 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/ane1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,087 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#44 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/annee1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,103 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#51 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/antiquites1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,117 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#58 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/archiduc1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,132 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#65 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/arebours1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,145 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#72 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/argent1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,159 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#79 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/armance2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,173 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#86 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/arriamar2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,188 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#93 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/artgrdp1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,200 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#100 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/athalie2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,217 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#107 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/avare2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,258 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#114 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/aziyade1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,290 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#121 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/ballades1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,320 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#128 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/ballon1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,346 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#135 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/becass2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,373 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#142 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/begum2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,401 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#149 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/belami2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,427 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#156 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/bertris1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,453 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#163 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/blocus3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,478 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#170 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/boule2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,506 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#177 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/bounty1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,534 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#184 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/bouvard2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,564 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#191 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/bovary3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,591 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#198 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/bretagne1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,616 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#205 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/britannicus1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,642 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#212 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/bugjarg1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,667 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#219 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/candide3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,693 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#226 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/caribou1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,721 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#233 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/centurion1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,751 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#240 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/chabert3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,778 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#247 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/chambre2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,799 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#254 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/champi1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,818 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#261 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/chartre1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,842 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#268 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/chef2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,863 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#275 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/cinna2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,885 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#282 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/cleves2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,907 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#289 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/cocuage2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,927 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#296 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/colliergriffes1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,947 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#303 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/colomba1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,967 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#310 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/commerce1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:24,987 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#317 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/confessions1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,183 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#324 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/conscrit2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,209 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#331 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/consider1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,234 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#338 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/contemplA2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,257 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#345 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/contemplB2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,284 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#352 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/contrat1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,300 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#359 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/crainque1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,313 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#366 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/curee2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,327 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#373 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/cyrano1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,340 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#380 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/daphnis1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,353 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#387 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/ddhc3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,365 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#394 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/desespere1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,378 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#401 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/diableam3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,390 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#408 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/diablecor2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,403 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#415 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/dicohisto1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,416 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#422 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/dicotypo1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,428 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#429 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/domi3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,441 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#436 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/domjuan2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,454 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#443 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/drageoir2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,466 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#450 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/drole2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,482 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#457 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/ecole1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,494 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#464 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/educati1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,506 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#471 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/elixir2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,518 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#478 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/erenouv1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,529 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#485 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/espece1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,540 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#492 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/etuit1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,552 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#499 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/excentlang1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,565 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#506 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/fabulistes1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,577 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#513 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/fadette1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,589 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#520 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/farcefran1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,601 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#527 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/femtin1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,612 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#534 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/feuilles1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,624 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#541 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/figchose1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,636 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#548 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/flandeux1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,648 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#555 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/football1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,659 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#562 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/gargantua2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,670 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#569 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/gaspard2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,682 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#576 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/germinal1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,693 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#583 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/germinie3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,706 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#590 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/gilblas1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,717 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#597 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/guizeur1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,728 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#604 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/hamlet1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,742 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#611 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/harmonie1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,762 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#618 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/historiettes2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,782 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#625 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/homecu1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,802 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#632 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/horla3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,824 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#639 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/hugoshak1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,846 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#646 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/humilite3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,866 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#653 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/ideolo1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,885 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#660 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/illusion1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,905 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#667 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/inquisit2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,928 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#674 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/interpret2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,949 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#681 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/iphigenie1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,970 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#688 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/irnois2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:25,989 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#695 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/jaccuse3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,008 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#702 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/jacques1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,026 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#709 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/journalism1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,045 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#716 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/journbloy1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,065 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#723 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/justice1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,085 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#730 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/lafille2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,105 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#737 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/lecid1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,126 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#744 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/legend1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,145 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#751 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/legendet21.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,165 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#758 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/leibnitzdiv1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,185 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#765 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/lejeu1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,214 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#772 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/lepetitchose1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,235 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#779 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/lessoirees1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,254 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#786 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/letphi1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,272 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#793 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/lettresecrites1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,291 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#800 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/lettresjuives11.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,309 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#807 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/lettresjuives231.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,330 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#814 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/lettresjuives451.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,349 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#821 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/lettresjuives6781.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,369 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#828 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/lettresreli2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,388 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#835 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/levieux2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,407 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#842 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/liaisons3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,426 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#849 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/licong1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,445 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#856 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/lmain3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,464 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#863 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/lutrin1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,485 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#870 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/m702douai2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,503 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#877 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/machine3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,523 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#884 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/manifeste2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,544 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#891 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/manon2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,567 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#898 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/marie1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,589 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#905 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/maximes2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,605 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#912 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/medipoet1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,617 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#919 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/medit3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,628 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#926 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/medita1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,639 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#933 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/methode3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,649 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#940 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/micromeg3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,661 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#947 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/monadologie1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,672 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#954 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/monde1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,683 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#961 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/montbard1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,696 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#968 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/morte2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,707 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#975 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/mousque1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,719 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#982 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/nddp1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,730 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#989 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/neveu2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,742 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#996 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/nouvmedi1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,752 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#1003 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/odespolit1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,763 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#1010 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/opinions3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,774 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#1017 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/oriental1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,785 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#1024 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/paresse3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,972 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#1031 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/partiecamp2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:26,989 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#1038 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/pascaldiv1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,001 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#1045 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/pascalpetits1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,014 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#1052 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/penseesXX1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,023 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#1059 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/pensepict1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,032 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#1066 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/phedre2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,041 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#1073 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/plural3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,051 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#1080 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/poemallar1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,060 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#1087 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/poemesadv1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,069 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#1094 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/presseguerre1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,080 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#1101 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/preuves1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,097 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#1108 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/propriete1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,113 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#1115 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/pucelle1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,131 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#1122 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/quatrevt1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,147 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#1129 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/raison2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,159 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#1136 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/rayons1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,170 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#1143 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/ren05101.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,180 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#1150 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/ren87922.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,192 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#1157 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/ren93982.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,205 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#1164 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/ren99041.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,215 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#1171 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/reveries3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,225 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#1178 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/rldased3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,235 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#1185 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/robur1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,246 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#1192 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/roland2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,256 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#1199 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/roletrav2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,267 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#1206 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/rouge1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,277 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#1213 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/rousgene1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,287 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#1220 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/ruesboi1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,297 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#1227 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/salalion1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,313 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#1234 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/salammb1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,330 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#1241 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/satan1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,346 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#1248 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/scapin2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,362 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#1255 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/scihyp2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,379 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#1262 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/septfem2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,395 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#1269 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/smarra1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,411 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#1276 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/souveni1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,428 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#1283 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/supplem2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,445 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#1290 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/tartuf2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,462 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#1297 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/tdm80j2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,479 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#1304 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/theodicee1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,498 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#1311 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/theoriephys1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,515 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#1318 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/thotri1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,531 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#1325 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/tlun3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,550 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#1332 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/tome1malherbe1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,567 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#1339 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/troylion1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,581 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#1346 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/ulenspiegel1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,594 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#1353 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/uncoeur3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,604 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#1360 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/unevie2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,620 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#1367 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/utopique2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,633 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#1374 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/verhafin1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,644 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#1381 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/viemars3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,652 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#1388 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/vignypoesie1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,661 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#1395 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/volpofin1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,671 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#1402 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/voltgene1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,680 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#1409 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/voyfran1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:17:27,689 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#1416 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50558
java.io.IOException: File /user/Lag/ABU/voylun3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:13,049 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-09-28 15:18:13,049 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-09-28 15:18:13,049 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 5, 410
2018-09-28 15:18:13,050 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 407 Total time for transactions(ms): 53 Number of transactions batched in Syncs: 0 Number of syncs: 407 SyncTimes(ms): 1229 
2018-09-28 15:18:13,052 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 407 Total time for transactions(ms): 53 Number of transactions batched in Syncs: 0 Number of syncs: 408 SyncTimes(ms): 1231 
2018-09-28 15:18:13,054 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000000005 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000005-0000000000000000411
2018-09-28 15:18:13,054 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 412
2018-09-28 15:18:31,190 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#7 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/adelaide2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,213 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#14 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/aiglon1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,229 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#21 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/amoursjaunes1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,242 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#28 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/andromaque1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,255 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#35 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/ane1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,268 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#42 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/annee1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,281 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#49 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/antiquites1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,293 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#56 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/archiduc1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,305 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#63 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/arebours1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,320 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#70 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/argent1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,343 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#77 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/armance2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,367 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#84 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/arriamar2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,390 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#91 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/artgrdp1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,418 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#98 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/athalie2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,444 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#105 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/avare2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,469 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#112 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/aziyade1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,487 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#119 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/ballades1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,500 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#126 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/ballon1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,512 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#133 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/becass2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,524 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#140 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/begum2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,537 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#147 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/belami2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,723 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#154 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/bertris1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,764 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#161 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/blocus3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,785 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#168 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/boule2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,806 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#175 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/bounty1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,827 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#182 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/bouvard2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,879 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#189 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/bovary3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,904 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#196 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/bretagne1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,930 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#203 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/britannicus1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,943 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#210 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/bugjarg1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,956 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#217 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/candide3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,972 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#224 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/caribou1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:31,994 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#231 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/centurion1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,015 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#238 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/chabert3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,037 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#245 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/chambre2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,059 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#252 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/champi1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,086 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#259 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/chartre1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,108 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#266 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/chef2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,130 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#273 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/cinna2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,146 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#280 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/cleves2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,158 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#287 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/cocuage2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,170 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#294 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/colliergriffes1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,180 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#301 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/colomba1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,191 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#308 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/commerce1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,202 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#315 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/confessions1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,212 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#322 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/conscrit2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,223 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#329 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/consider1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,234 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#336 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/contemplA2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,244 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#343 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/contemplB2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,255 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#350 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/contrat1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,266 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#357 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/crainque1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,276 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#364 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/curee2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,288 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#371 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/cyrano1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,299 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#378 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/daphnis1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,310 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#385 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/ddhc3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,320 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#392 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/desespere1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,330 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#399 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/diableam3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,340 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#406 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/diablecor2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,350 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#413 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/dicohisto1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,360 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#420 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/dicotypo1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,370 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#427 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/domi3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,380 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#434 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/domjuan2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,391 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#441 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/drageoir2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,401 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#448 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/drole2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,410 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#455 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/ecole1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,419 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#462 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/educati1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,429 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#469 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/elixir2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,444 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#476 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/erenouv1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,462 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#483 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/espece1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,484 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#490 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/etuit1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,502 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#497 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/excentlang1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,521 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#504 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/fabulistes1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,541 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#511 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/fadette1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,561 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#518 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/farcefran1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,580 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#525 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/femtin1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,599 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#532 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/feuilles1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,618 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#539 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/figchose1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,636 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#546 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/flandeux1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,656 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#553 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/football1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,675 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#560 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/gargantua2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,694 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#567 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/gaspard2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,712 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#574 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/germinal1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,732 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#581 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/germinie3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,756 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#588 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/gilblas1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,779 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#595 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/guizeur1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,796 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#602 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/hamlet1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,809 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#609 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/harmonie1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,820 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#616 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/historiettes2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,832 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#623 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/homecu1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,843 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#630 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/horla3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,855 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#637 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/hugoshak1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,867 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#644 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/humilite3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,878 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#651 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/ideolo1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,889 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#658 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/illusion1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,901 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#665 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/inquisit2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,913 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#672 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/interpret2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,924 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#679 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/iphigenie1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,935 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#686 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/irnois2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,947 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#693 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/jaccuse3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,958 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#700 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/jacques1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,969 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#707 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/journalism1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,980 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#714 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/journbloy1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:32,991 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#721 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/justice1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,003 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#728 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/lafille2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,018 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#735 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/lecid1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,035 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#742 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/legend1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,054 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#749 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/legendet21.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,073 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#756 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/leibnitzdiv1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,091 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#763 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/lejeu1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,111 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#770 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/lepetitchose1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,129 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#777 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/lessoirees1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,146 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#784 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/letphi1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,165 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#791 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/lettresecrites1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,182 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#798 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/lettresjuives11.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,200 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#805 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/lettresjuives231.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,219 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#812 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/lettresjuives451.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,237 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#819 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/lettresjuives6781.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,255 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#826 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/lettresreli2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,274 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#833 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/levieux2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,292 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#840 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/liaisons3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,310 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#847 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/licong1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,499 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#854 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/lmain3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,516 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#861 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/lutrin1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,534 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#868 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/m702douai2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,561 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#875 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/machine3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,579 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#882 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/manifeste2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,597 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#889 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/manon2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,617 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#896 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/marie1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,636 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#903 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/maximes2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,654 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#910 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/medipoet1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,671 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#917 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/medit3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,687 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#924 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/medita1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,699 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#931 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/methode3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,709 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#938 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/micromeg3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,719 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#945 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/monadologie1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,728 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#952 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/monde1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,737 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#959 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/montbard1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,746 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#966 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/morte2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,755 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#973 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/mousque1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,764 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#980 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/nddp1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,773 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#987 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/neveu2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,782 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#994 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/nouvmedi1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,791 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#1001 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/odespolit1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,799 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#1008 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/opinions3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,808 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#1015 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/oriental1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,816 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#1022 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/paresse3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,826 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#1029 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/partiecamp2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,835 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#1036 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/pascaldiv1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,843 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#1043 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/pascalpetits1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,852 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#1050 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/penseesXX1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,860 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#1057 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/pensepict1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,871 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#1064 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/phedre2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,889 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#1071 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/plural3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,905 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#1078 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/poemallar1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,921 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#1085 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/poemesadv1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,937 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#1092 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/presseguerre1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,956 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#1099 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/preuves1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,973 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#1106 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/propriete1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:33,990 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#1113 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/pucelle1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,007 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#1120 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/quatrevt1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,022 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#1127 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/raison2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,034 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#1134 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/rayons1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,047 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#1141 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/ren05101.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,060 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#1148 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/ren87922.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,073 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#1155 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/ren93982.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,088 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#1162 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/ren99041.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,102 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#1169 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/reveries3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,114 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#1176 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/rldased3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,126 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#1183 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/robur1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,139 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#1190 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/roland2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,156 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#1197 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/roletrav2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,172 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#1204 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/rouge1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,189 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#1211 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/rousgene1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,206 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#1218 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/ruesboi1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,226 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#1225 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/salalion1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,245 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#1232 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/salammb1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,261 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#1239 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/satan1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,271 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#1246 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/scapin2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,281 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#1253 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/scihyp2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,290 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#1260 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/septfem2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,300 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#1267 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/smarra1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,309 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#1274 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/souveni1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,319 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#1281 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/supplem2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,328 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#1288 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/tartuf2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,337 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#1295 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/tdm80j2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,347 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#1302 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/theodicee1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,355 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#1309 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/theoriephys1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,364 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#1316 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/thotri1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,373 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#1323 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/tlun3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,381 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#1330 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/tome1malherbe1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,389 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#1337 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/troylion1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,398 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#1344 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/ulenspiegel1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,409 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#1351 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/uncoeur3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,424 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#1358 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/unevie2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,438 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#1365 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/utopique2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,454 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#1372 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/verhafin1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,470 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#1379 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/viemars3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,493 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#1386 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/vignypoesie1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,507 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#1393 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/volpofin1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,523 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#1400 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/voltgene1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,535 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#1407 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/voyfran1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:18:34,545 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#1414 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50578
java.io.IOException: File /user/Lag/ABU/voylun3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:19:13,127 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-09-28 15:19:13,127 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-09-28 15:19:13,127 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 412, 816
2018-09-28 15:19:13,128 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 406 Total time for transactions(ms): 25 Number of transactions batched in Syncs: 0 Number of syncs: 406 SyncTimes(ms): 1238 
2018-09-28 15:19:13,130 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 406 Total time for transactions(ms): 25 Number of transactions batched in Syncs: 0 Number of syncs: 407 SyncTimes(ms): 1240 
2018-09-28 15:19:13,132 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000000412 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000412-0000000000000000817
2018-09-28 15:19:13,132 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 818
2018-09-28 15:20:13,214 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-09-28 15:20:13,214 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-09-28 15:20:13,214 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 818, 818
2018-09-28 15:20:13,214 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 63 
2018-09-28 15:20:13,216 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 65 
2018-09-28 15:20:13,218 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000000818 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000818-0000000000000000819
2018-09-28 15:20:13,218 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 820
2018-09-28 15:21:13,303 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-09-28 15:21:13,303 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-09-28 15:21:13,303 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 820, 820
2018-09-28 15:21:13,303 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 65 
2018-09-28 15:21:13,305 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 67 
2018-09-28 15:21:13,307 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000000820 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000820-0000000000000000821
2018-09-28 15:21:13,307 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 822
2018-09-28 15:21:34,388 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#7 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/adelaide2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,410 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#14 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/aiglon1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,424 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#21 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/amoursjaunes1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,437 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#28 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/andromaque1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,448 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#35 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/ane1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,461 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#42 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/annee1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,473 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#49 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/antiquites1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,485 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#56 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/archiduc1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,496 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#63 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/arebours1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,510 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#70 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/argent1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,534 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#77 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/armance2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,555 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#84 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/arriamar2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,580 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#91 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/artgrdp1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,604 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#98 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/athalie2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,624 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#105 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/avare2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,650 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#112 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/aziyade1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,674 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#119 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/ballades1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,691 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#126 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/ballon1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,708 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#133 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/becass2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,724 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#140 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/begum2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,741 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#147 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/belami2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,757 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#154 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/bertris1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,773 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#161 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/blocus3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,789 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#168 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/boule2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,806 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#175 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/bounty1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,823 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#182 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/bouvard2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,839 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#189 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/bovary3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,858 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#196 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/bretagne1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,875 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#203 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/britannicus1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,891 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#210 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/bugjarg1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,907 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#217 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/candide3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,922 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#224 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/caribou1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,938 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#231 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/centurion1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,953 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#238 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/chabert3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,969 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#245 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/chambre2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:34,985 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#252 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/champi1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,002 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#259 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/chartre1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,016 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#266 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/chef2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,031 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#273 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/cinna2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,048 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#280 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/cleves2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,069 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#287 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/cocuage2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,088 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#294 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/colliergriffes1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,108 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#301 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/colomba1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,127 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#308 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/commerce1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,146 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#315 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/confessions1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,168 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#322 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/conscrit2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,187 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#329 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/consider1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,206 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#336 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/contemplA2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,225 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#343 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/contemplB2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,244 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#350 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/contrat1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,263 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#357 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/crainque1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,286 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#364 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/curee2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,309 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#371 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/cyrano1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,329 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#378 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/daphnis1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,348 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#385 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/ddhc3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,366 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#392 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/desespere1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,385 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#399 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/diableam3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,403 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#406 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/diablecor2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,421 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#413 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/dicohisto1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,439 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#420 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/dicotypo1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,460 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#427 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/domi3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,480 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#434 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/domjuan2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,499 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#441 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/drageoir2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,514 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#448 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/drole2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,700 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#455 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/ecole1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,718 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#462 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/educati1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,746 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#469 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/elixir2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,763 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#476 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/erenouv1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,780 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#483 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/espece1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,798 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#490 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/etuit1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,815 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#497 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/excentlang1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,831 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#504 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/fabulistes1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,851 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#511 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/fadette1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,869 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#518 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/farcefran1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,887 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#525 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/femtin1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,905 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#532 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/feuilles1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,923 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#539 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/figchose1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,940 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#546 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/flandeux1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,957 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#553 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/football1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,974 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#560 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/gargantua2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:35,989 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#567 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/gaspard2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,001 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#574 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/germinal1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,015 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#581 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/germinie3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,025 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#588 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/gilblas1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,035 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#595 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/guizeur1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,046 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#602 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/hamlet1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,056 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#609 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/harmonie1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,065 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#616 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/historiettes2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,075 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#623 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/homecu1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,085 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#630 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/horla3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,095 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#637 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/hugoshak1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,104 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#644 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/humilite3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,114 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#651 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/ideolo1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,123 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#658 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/illusion1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,132 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#665 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/inquisit2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,142 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#672 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/interpret2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,151 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#679 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/iphigenie1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,161 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#686 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/irnois2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,178 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#693 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/jaccuse3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,194 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#700 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/jacques1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,212 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#707 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/journalism1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,229 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#714 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/journbloy1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,247 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#721 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/justice1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,264 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#728 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/lafille2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,284 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#735 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/lecid1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,301 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#742 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/legend1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,319 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#749 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/legendet21.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,335 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#756 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/leibnitzdiv1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,349 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#763 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/lejeu1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,362 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#770 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/lepetitchose1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,372 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#777 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/lessoirees1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,381 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#784 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/letphi1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,390 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#791 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/lettresecrites1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,399 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#798 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/lettresjuives11.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,408 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#805 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/lettresjuives231.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,418 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#812 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/lettresjuives451.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,427 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#819 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/lettresjuives6781.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,437 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#826 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/lettresreli2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,446 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#833 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/levieux2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,455 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#840 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/liaisons3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,464 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#847 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/licong1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,474 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#854 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/lmain3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,483 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#861 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/lutrin1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,492 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#868 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/m702douai2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,501 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#875 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/machine3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,510 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#882 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/manifeste2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,520 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#889 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/manon2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,533 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#896 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/marie1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,550 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#903 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/maximes2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,567 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#910 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/medipoet1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,583 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#917 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/medit3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,600 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#924 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/medita1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,617 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#931 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/methode3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,633 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#938 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/micromeg3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,650 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#945 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/monadologie1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,665 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#952 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/monde1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,682 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#959 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/montbard1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,697 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#966 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/morte2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,713 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#973 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/mousque1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,730 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#980 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/nddp1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,746 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#987 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/neveu2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,762 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#994 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/nouvmedi1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,778 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#1001 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/odespolit1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,793 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#1008 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/opinions3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,809 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#1015 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/oriental1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,825 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#1022 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/paresse3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,841 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#1029 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/partiecamp2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,858 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#1036 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/pascaldiv1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,874 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#1043 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/pascalpetits1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,892 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#1050 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/penseesXX1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,908 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#1057 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/pensepict1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,922 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#1064 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/phedre2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,932 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#1071 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/plural3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,942 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#1078 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/poemallar1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,951 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#1085 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/poemesadv1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,960 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#1092 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/presseguerre1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,970 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#1099 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/preuves1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:36,986 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#1106 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/propriete1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,002 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#1113 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/pucelle1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,018 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#1120 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/quatrevt1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,034 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#1127 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/raison2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,050 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#1134 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/rayons1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,067 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#1141 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/ren05101.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,082 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#1148 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/ren87922.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,099 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#1155 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/ren93982.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,288 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#1162 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/ren99041.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,303 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#1169 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/reveries3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,319 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#1176 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/rldased3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,334 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#1183 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/robur1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,350 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#1190 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/roland2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,366 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#1197 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/roletrav2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,382 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#1204 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/rouge1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,398 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#1211 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/rousgene1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,415 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#1218 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/ruesboi1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,430 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#1225 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/salalion1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,445 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#1232 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/salammb1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,461 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#1239 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/satan1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,476 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#1246 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/scapin2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,491 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#1253 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/scihyp2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,506 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#1260 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/septfem2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,522 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#1267 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/smarra1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,538 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#1274 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/souveni1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,554 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#1281 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/supplem2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,571 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#1288 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/tartuf2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,588 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#1295 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/tdm80j2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,604 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#1302 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/theodicee1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,619 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#1309 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/theoriephys1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,635 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#1316 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/thotri1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,650 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#1323 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/tlun3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,666 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#1330 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/tome1malherbe1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,681 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#1337 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/troylion1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,701 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#1344 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/ulenspiegel1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,721 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call Call#1351 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/uncoeur3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,734 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call Call#1358 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/unevie2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,744 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call Call#1365 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/utopique2.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,755 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#1372 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/verhafin1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,765 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call Call#1379 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/viemars3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,783 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call Call#1386 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/vignypoesie1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,794 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call Call#1393 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/volpofin1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,804 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call Call#1400 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/voltgene1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,814 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call Call#1407 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/voyfran1.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:21:37,825 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call Call#1414 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50622
java.io.IOException: File /user/Lag/ABU/voylun3.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1814)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2563)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)
2018-09-28 15:22:13,394 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-09-28 15:22:13,394 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-09-28 15:22:13,394 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 822, 1226
2018-09-28 15:22:13,394 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 406 Total time for transactions(ms): 25 Number of transactions batched in Syncs: 0 Number of syncs: 406 SyncTimes(ms): 1195 
2018-09-28 15:22:13,397 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 406 Total time for transactions(ms): 25 Number of transactions batched in Syncs: 0 Number of syncs: 407 SyncTimes(ms): 1198 
2018-09-28 15:22:13,398 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000000822 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000822-0000000000000001227
2018-09-28 15:22:13,398 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1228
2018-09-28 15:22:16,059 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2018-09-28 15:22:16,060 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at lag-Predator-G3-571/127.0.1.1
************************************************************/
2018-09-29 13:13:13,575 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = lag-Predator-G3-571/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.9.1
STARTUP_MSG:   classpath = /home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/etc/hadoop:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/activation-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-auth-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jettison-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/junit-4.11.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-annotations-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsch-0.1.54.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-nfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/fst-2.50.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-core-1.8.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-1.7.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-framework-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsp-api-2.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/gson-2.2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-recipes-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jcip-annotations-1.0-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-digester-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-net-3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-smart-1.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsch-0.1.54.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-router-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-registry-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-api-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.9.1.jar:/usr/lib/jvm/java-11-openjdk-amd64//lib/tools.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e30710aea4e6e55e69372929106cf119af06fd0e; compiled by 'root' on 2018-04-16T09:33Z
STARTUP_MSG:   java = 10.0.2
************************************************************/
2018-09-29 13:13:13,590 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-29 13:13:13,591 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2018-09-29 13:13:13,707 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-29 13:13:13,943 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-09-29 13:13:13,943 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2018-09-29 13:13:13,978 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2018-09-29 13:13:13,980 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2018-09-29 13:13:14,135 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-09-29 13:13:14,153 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2018-09-29 13:13:14,230 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-29 13:13:14,236 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-29 13:13:14,246 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2018-09-29 13:13:14,250 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-29 13:13:14,251 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2018-09-29 13:13:14,252 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-29 13:13:14,252 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-29 13:13:14,413 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2018-09-29 13:13:14,413 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2018-09-29 13:13:14,463 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2018-09-29 13:13:14,464 INFO org.mortbay.log: jetty-6.1.26
2018-09-29 13:13:14,730 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2018-09-29 13:13:14,743 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-29 13:13:14,743 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-09-29 13:13:14,766 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2018-09-29 13:13:14,772 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2018-09-29 13:13:14,772 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2018-09-29 13:13:14,773 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2018-09-29 13:13:14,792 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = lag (auth:SIMPLE)
2018-09-29 13:13:14,792 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2018-09-29 13:13:14,792 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2018-09-29 13:13:14,792 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2018-09-29 13:13:14,812 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-09-29 13:13:14,824 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2018-09-29 13:13:14,824 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2018-09-29 13:13:14,827 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2018-09-29 13:13:14,827 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2018 Sep 29 13:13:14
2018-09-29 13:13:14,828 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2018-09-29 13:13:14,828 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-29 13:13:14,829 INFO org.apache.hadoop.util.GSet: 2.0% max memory 1000 MB = 20 MB
2018-09-29 13:13:14,829 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2018-09-29 13:13:14,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2018-09-29 13:13:14,842 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS
2018-09-29 13:13:14,844 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2018-09-29 13:13:14,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2018-09-29 13:13:14,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2018-09-29 13:13:14,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2018-09-29 13:13:14,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2018-09-29 13:13:14,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2018-09-29 13:13:14,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2018-09-29 13:13:14,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2018-09-29 13:13:14,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2018-09-29 13:13:14,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2018-09-29 13:13:14,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2018-09-29 13:13:14,846 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2018-09-29 13:13:14,888 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2018-09-29 13:13:14,888 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-29 13:13:14,888 INFO org.apache.hadoop.util.GSet: 1.0% max memory 1000 MB = 10 MB
2018-09-29 13:13:14,888 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2018-09-29 13:13:14,890 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2018-09-29 13:13:14,890 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2018-09-29 13:13:14,890 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2018-09-29 13:13:14,892 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false
2018-09-29 13:13:14,896 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2018-09-29 13:13:14,896 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-29 13:13:14,897 INFO org.apache.hadoop.util.GSet: 0.25% max memory 1000 MB = 2.5 MB
2018-09-29 13:13:14,897 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2018-09-29 13:13:14,899 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2018-09-29 13:13:14,899 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2018-09-29 13:13:14,899 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2018-09-29 13:13:14,902 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2018-09-29 13:13:14,902 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2018-09-29 13:13:14,904 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2018-09-29 13:13:14,904 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-09-29 13:13:14,904 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 1000 MB = 307.2 KB
2018-09-29 13:13:14,904 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2018-09-29 13:13:14,948 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-lag/dfs/name/in_use.lock acquired by nodename 4432@lag-Predator-G3-571
2018-09-29 13:13:14,966 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-lag/dfs/name/current
2018-09-29 13:13:14,966 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2018-09-29 13:13:14,967 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2018-09-29 13:13:15,000 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2018-09-29 13:13:15,018 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2018-09-29 13:13:15,018 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000
2018-09-29 13:13:15,022 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2018-09-29 13:13:15,022 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2018-09-29 13:13:15,142 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2018-09-29 13:13:15,143 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 236 msecs
2018-09-29 13:13:15,412 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2018-09-29 13:13:15,419 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-09-29 13:13:15,459 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2018-09-29 13:13:15,562 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2018-09-29 13:13:15,577 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2018-09-29 13:13:15,583 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues
2018-09-29 13:13:15,583 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2018-09-29 13:13:15,583 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2018-09-29 13:13:15,583 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2018-09-29 13:13:15,593 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2018-09-29 13:13:15,593 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2018-09-29 13:13:15,594 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2018-09-29 13:13:15,594 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2018-09-29 13:13:15,594 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2018-09-29 13:13:15,594 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 11 msec
2018-09-29 13:13:15,605 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-29 13:13:15,605 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2018-09-29 13:13:15,606 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2018-09-29 13:13:15,615 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2018-09-29 13:13:15,615 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2018-09-29 13:13:15,617 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 2 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2018-09-29 13:13:15,621 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2018-09-29 13:13:19,670 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=e0a9757c-a277-47ac-b700-5f0e9eb7f9ae, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-88860281-4205-4492-aa60-05a152b2daf2;nsid=599153523;c=1538219587368) storage e0a9757c-a277-47ac-b700-5f0e9eb7f9ae
2018-09-29 13:13:19,671 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2018-09-29 13:13:19,672 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN e0a9757c-a277-47ac-b700-5f0e9eb7f9ae (127.0.0.1:50010).
2018-09-29 13:13:19,711 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-011e18f7-a2a2-4dea-8098-8361e16537ba for DN 127.0.0.1:50010
2018-09-29 13:13:19,746 INFO BlockStateChange: BLOCK* processReport 0x647d9f1e6fd4b243: Processing first storage report for DS-011e18f7-a2a2-4dea-8098-8361e16537ba from datanode e0a9757c-a277-47ac-b700-5f0e9eb7f9ae
2018-09-29 13:13:19,748 INFO BlockStateChange: BLOCK* processReport 0x647d9f1e6fd4b243: from storage DS-011e18f7-a2a2-4dea-8098-8361e16537ba node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=e0a9757c-a277-47ac-b700-5f0e9eb7f9ae, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-88860281-4205-4492-aa60-05a152b2daf2;nsid=599153523;c=1538219587368), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2018-09-29 13:14:21,670 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 65 
2018-09-29 13:14:21,762 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=127.0.0.1:50010 for /user/Lag/ABU/adelaide2.txt._COPYING_
2018-09-29 13:14:21,863 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741825_1001 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/adelaide2.txt._COPYING_
2018-09-29 13:14:22,268 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/adelaide2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:22,329 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=127.0.0.1:50010 for /user/Lag/ABU/aiglon1.txt._COPYING_
2018-09-29 13:14:22,352 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/aiglon1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:22,374 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=127.0.0.1:50010 for /user/Lag/ABU/amoursjaunes1.txt._COPYING_
2018-09-29 13:14:22,404 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/amoursjaunes1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:22,425 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=127.0.0.1:50010 for /user/Lag/ABU/andromaque1.txt._COPYING_
2018-09-29 13:14:22,450 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/andromaque1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:22,475 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=127.0.0.1:50010 for /user/Lag/ABU/ane1.txt._COPYING_
2018-09-29 13:14:22,499 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741829_1005 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ane1.txt._COPYING_
2018-09-29 13:14:22,902 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ane1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:22,932 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=127.0.0.1:50010 for /user/Lag/ABU/annee1.txt._COPYING_
2018-09-29 13:14:22,956 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/annee1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:22,991 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=127.0.0.1:50010 for /user/Lag/ABU/antiquites1.txt._COPYING_
2018-09-29 13:14:23,008 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/antiquites1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:23,027 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008, replicas=127.0.0.1:50010 for /user/Lag/ABU/archiduc1.txt._COPYING_
2018-09-29 13:14:23,047 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/archiduc1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:23,066 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009, replicas=127.0.0.1:50010 for /user/Lag/ABU/arebours1.txt._COPYING_
2018-09-29 13:14:23,091 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741833_1009 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/arebours1.txt._COPYING_
2018-09-29 13:14:23,493 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/arebours1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:23,523 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010, replicas=127.0.0.1:50010 for /user/Lag/ABU/argent1.txt._COPYING_
2018-09-29 13:14:23,556 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/argent1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:23,592 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011, replicas=127.0.0.1:50010 for /user/Lag/ABU/armance2.txt._COPYING_
2018-09-29 13:14:23,624 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/armance2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:23,649 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012, replicas=127.0.0.1:50010 for /user/Lag/ABU/arriamar2.txt._COPYING_
2018-09-29 13:14:23,688 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/arriamar2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:23,715 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013, replicas=127.0.0.1:50010 for /user/Lag/ABU/artgrdp1.txt._COPYING_
2018-09-29 13:14:23,740 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/artgrdp1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:23,766 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014, replicas=127.0.0.1:50010 for /user/Lag/ABU/athalie2.txt._COPYING_
2018-09-29 13:14:23,789 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741838_1014 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/athalie2.txt._COPYING_
2018-09-29 13:14:24,156 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-09-29 13:14:24,156 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-09-29 13:14:24,156 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1, 86
2018-09-29 13:14:24,158 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 87 Total time for transactions(ms): 23 Number of transactions batched in Syncs: 14 Number of syncs: 74 SyncTimes(ms): 347 
2018-09-29 13:14:24,159 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000001-0000000000000000087
2018-09-29 13:14:24,179 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 88
2018-09-29 13:14:24,250 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/athalie2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:24,368 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015, replicas=127.0.0.1:50010 for /user/Lag/ABU/avare2.txt._COPYING_
2018-09-29 13:14:24,701 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/avare2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:24,755 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016, replicas=127.0.0.1:50010 for /user/Lag/ABU/aziyade1.txt._COPYING_
2018-09-29 13:14:24,772 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/aziyade1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:24,798 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017, replicas=127.0.0.1:50010 for /user/Lag/ABU/ballades1.txt._COPYING_
2018-09-29 13:14:24,826 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ballades1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:24,943 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018, replicas=127.0.0.1:50010 for /user/Lag/ABU/ballon1.txt._COPYING_
2018-09-29 13:14:24,970 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ballon1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:25,004 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019, replicas=127.0.0.1:50010 for /user/Lag/ABU/becass2.txt._COPYING_
2018-09-29 13:14:25,022 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/becass2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:25,044 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020, replicas=127.0.0.1:50010 for /user/Lag/ABU/begum2.txt._COPYING_
2018-09-29 13:14:25,062 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/begum2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:25,128 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021, replicas=127.0.0.1:50010 for /user/Lag/ABU/belami2.txt._COPYING_
2018-09-29 13:14:25,174 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/belami2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:25,197 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022, replicas=127.0.0.1:50010 for /user/Lag/ABU/bertris1.txt._COPYING_
2018-09-29 13:14:25,211 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/bertris1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:25,230 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023, replicas=127.0.0.1:50010 for /user/Lag/ABU/blocus3.txt._COPYING_
2018-09-29 13:14:25,249 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/blocus3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:25,269 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024, replicas=127.0.0.1:50010 for /user/Lag/ABU/boule2.txt._COPYING_
2018-09-29 13:14:25,284 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741848_1024 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/boule2.txt._COPYING_
2018-09-29 13:14:25,287 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000, fileSize: 320. Sent total: 320 bytes. Size of last segment intended to send: -1 bytes.
2018-09-29 13:14:25,336 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000001-0000000000000000087, fileSize: 7878. Sent total: 7878 bytes. Size of last segment intended to send: -1 bytes.
2018-09-29 13:14:25,566 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Combined time for fsimage download and fsync to all disks took 0.00s. The fsimage download took 0.00s at 1000.00 KB/s. Synchronous (fsync) write to disk of /tmp/hadoop-lag/dfs/name/current/fsimage.ckpt_0000000000000000087 took 0.00s.
2018-09-29 13:14:25,566 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000087 size 1659 bytes.
2018-09-29 13:14:25,575 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2018-09-29 13:14:25,686 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/boule2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:25,721 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025, replicas=127.0.0.1:50010 for /user/Lag/ABU/bounty1.txt._COPYING_
2018-09-29 13:14:25,743 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/bounty1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:25,767 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741850_1026, replicas=127.0.0.1:50010 for /user/Lag/ABU/bouvard2.txt._COPYING_
2018-09-29 13:14:25,796 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/bouvard2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:25,836 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741851_1027, replicas=127.0.0.1:50010 for /user/Lag/ABU/bovary3.txt._COPYING_
2018-09-29 13:14:25,866 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/bovary3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:25,892 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741852_1028, replicas=127.0.0.1:50010 for /user/Lag/ABU/bretagne1.txt._COPYING_
2018-09-29 13:14:25,913 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/bretagne1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:25,956 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741853_1029, replicas=127.0.0.1:50010 for /user/Lag/ABU/britannicus1.txt._COPYING_
2018-09-29 13:14:25,973 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/britannicus1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:25,992 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741854_1030, replicas=127.0.0.1:50010 for /user/Lag/ABU/bugjarg1.txt._COPYING_
2018-09-29 13:14:26,008 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/bugjarg1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:26,026 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741855_1031, replicas=127.0.0.1:50010 for /user/Lag/ABU/candide3.txt._COPYING_
2018-09-29 13:14:26,042 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/candide3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:26,060 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741856_1032, replicas=127.0.0.1:50010 for /user/Lag/ABU/caribou1.txt._COPYING_
2018-09-29 13:14:26,076 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/caribou1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:26,270 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741857_1033, replicas=127.0.0.1:50010 for /user/Lag/ABU/centurion1.txt._COPYING_
2018-09-29 13:14:26,305 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/centurion1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:26,324 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741858_1034, replicas=127.0.0.1:50010 for /user/Lag/ABU/chabert3.txt._COPYING_
2018-09-29 13:14:26,340 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/chabert3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:26,358 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741859_1035, replicas=127.0.0.1:50010 for /user/Lag/ABU/chambre2.txt._COPYING_
2018-09-29 13:14:26,373 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/chambre2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:26,391 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741860_1036, replicas=127.0.0.1:50010 for /user/Lag/ABU/champi1.txt._COPYING_
2018-09-29 13:14:26,407 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/champi1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:26,425 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741861_1037, replicas=127.0.0.1:50010 for /user/Lag/ABU/chartre1.txt._COPYING_
2018-09-29 13:14:26,459 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/chartre1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:26,477 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741862_1038, replicas=127.0.0.1:50010 for /user/Lag/ABU/chef2.txt._COPYING_
2018-09-29 13:14:26,492 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/chef2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:26,511 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741863_1039, replicas=127.0.0.1:50010 for /user/Lag/ABU/cinna2.txt._COPYING_
2018-09-29 13:14:26,532 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741863_1039 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/cinna2.txt._COPYING_
2018-09-29 13:14:26,934 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/cinna2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:26,971 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741864_1040, replicas=127.0.0.1:50010 for /user/Lag/ABU/cleves2.txt._COPYING_
2018-09-29 13:14:26,996 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/cleves2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:27,039 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741865_1041, replicas=127.0.0.1:50010 for /user/Lag/ABU/cocuage2.txt._COPYING_
2018-09-29 13:14:27,055 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/cocuage2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:27,073 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741866_1042, replicas=127.0.0.1:50010 for /user/Lag/ABU/colliergriffes1.txt._COPYING_
2018-09-29 13:14:27,095 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/colliergriffes1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:27,115 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741867_1043, replicas=127.0.0.1:50010 for /user/Lag/ABU/colomba1.txt._COPYING_
2018-09-29 13:14:27,131 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/colomba1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:27,150 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741868_1044, replicas=127.0.0.1:50010 for /user/Lag/ABU/commerce1.txt._COPYING_
2018-09-29 13:14:27,164 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/commerce1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:27,182 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741869_1045, replicas=127.0.0.1:50010 for /user/Lag/ABU/confessions1.txt._COPYING_
2018-09-29 13:14:27,205 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/confessions1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:27,223 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741870_1046, replicas=127.0.0.1:50010 for /user/Lag/ABU/conscrit2.txt._COPYING_
2018-09-29 13:14:27,239 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/conscrit2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:27,262 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741871_1047, replicas=127.0.0.1:50010 for /user/Lag/ABU/consider1.txt._COPYING_
2018-09-29 13:14:27,282 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/consider1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:27,307 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741872_1048, replicas=127.0.0.1:50010 for /user/Lag/ABU/contemplA2.txt._COPYING_
2018-09-29 13:14:27,329 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/contemplA2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:27,373 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741873_1049, replicas=127.0.0.1:50010 for /user/Lag/ABU/contemplB2.txt._COPYING_
2018-09-29 13:14:27,394 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/contemplB2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:27,418 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741874_1050, replicas=127.0.0.1:50010 for /user/Lag/ABU/contrat1.txt._COPYING_
2018-09-29 13:14:27,437 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/contrat1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:27,459 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741875_1051, replicas=127.0.0.1:50010 for /user/Lag/ABU/crainque1.txt._COPYING_
2018-09-29 13:14:27,477 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/crainque1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:27,499 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741876_1052, replicas=127.0.0.1:50010 for /user/Lag/ABU/curee2.txt._COPYING_
2018-09-29 13:14:27,525 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/curee2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:27,563 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741877_1053, replicas=127.0.0.1:50010 for /user/Lag/ABU/cyrano1.txt._COPYING_
2018-09-29 13:14:27,579 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/cyrano1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:27,598 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741878_1054, replicas=127.0.0.1:50010 for /user/Lag/ABU/daphnis1.txt._COPYING_
2018-09-29 13:14:27,618 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/daphnis1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:27,639 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741879_1055, replicas=127.0.0.1:50010 for /user/Lag/ABU/ddhc3.txt._COPYING_
2018-09-29 13:14:27,657 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ddhc3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:27,681 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741880_1056, replicas=127.0.0.1:50010 for /user/Lag/ABU/desespere1.txt._COPYING_
2018-09-29 13:14:27,707 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741880_1056 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/desespere1.txt._COPYING_
2018-09-29 13:14:28,109 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/desespere1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:28,143 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741881_1057, replicas=127.0.0.1:50010 for /user/Lag/ABU/diableam3.txt._COPYING_
2018-09-29 13:14:28,163 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741881_1057 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/diableam3.txt._COPYING_
2018-09-29 13:14:28,565 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/diableam3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:28,597 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741882_1058, replicas=127.0.0.1:50010 for /user/Lag/ABU/diablecor2.txt._COPYING_
2018-09-29 13:14:28,617 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/diablecor2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:28,636 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741883_1059, replicas=127.0.0.1:50010 for /user/Lag/ABU/dicohisto1.txt._COPYING_
2018-09-29 13:14:28,651 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741883_1059 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/dicohisto1.txt._COPYING_
2018-09-29 13:14:29,053 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/dicohisto1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:29,089 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741884_1060, replicas=127.0.0.1:50010 for /user/Lag/ABU/dicotypo1.txt._COPYING_
2018-09-29 13:14:29,110 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/dicotypo1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:29,129 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741885_1061, replicas=127.0.0.1:50010 for /user/Lag/ABU/domi3.txt._COPYING_
2018-09-29 13:14:29,146 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741885_1061 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/domi3.txt._COPYING_
2018-09-29 13:14:29,548 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/domi3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:29,582 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741886_1062, replicas=127.0.0.1:50010 for /user/Lag/ABU/domjuan2.txt._COPYING_
2018-09-29 13:14:29,598 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/domjuan2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:29,619 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741887_1063, replicas=127.0.0.1:50010 for /user/Lag/ABU/drageoir2.txt._COPYING_
2018-09-29 13:14:29,636 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/drageoir2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:29,656 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741888_1064, replicas=127.0.0.1:50010 for /user/Lag/ABU/drole2.txt._COPYING_
2018-09-29 13:14:29,674 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741888_1064 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/drole2.txt._COPYING_
2018-09-29 13:14:30,077 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/drole2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:30,114 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741889_1065, replicas=127.0.0.1:50010 for /user/Lag/ABU/ecole1.txt._COPYING_
2018-09-29 13:14:30,132 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741889_1065 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ecole1.txt._COPYING_
2018-09-29 13:14:30,534 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ecole1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:30,554 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741890_1066, replicas=127.0.0.1:50010 for /user/Lag/ABU/educati1.txt._COPYING_
2018-09-29 13:14:30,581 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/educati1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:30,603 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741891_1067, replicas=127.0.0.1:50010 for /user/Lag/ABU/elixir2.txt._COPYING_
2018-09-29 13:14:30,617 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741891_1067 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/elixir2.txt._COPYING_
2018-09-29 13:14:31,019 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/elixir2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:31,043 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741892_1068, replicas=127.0.0.1:50010 for /user/Lag/ABU/erenouv1.txt._COPYING_
2018-09-29 13:14:31,061 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/erenouv1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:31,252 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741893_1069, replicas=127.0.0.1:50010 for /user/Lag/ABU/espece1.txt._COPYING_
2018-09-29 13:14:31,282 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/espece1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:31,323 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741894_1070, replicas=127.0.0.1:50010 for /user/Lag/ABU/etuit1.txt._COPYING_
2018-09-29 13:14:31,342 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/etuit1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:31,364 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741895_1071, replicas=127.0.0.1:50010 for /user/Lag/ABU/excentlang1.txt._COPYING_
2018-09-29 13:14:31,384 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/excentlang1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:31,401 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741896_1072, replicas=127.0.0.1:50010 for /user/Lag/ABU/fabulistes1.txt._COPYING_
2018-09-29 13:14:31,417 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/fabulistes1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:31,436 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741897_1073, replicas=127.0.0.1:50010 for /user/Lag/ABU/fadette1.txt._COPYING_
2018-09-29 13:14:31,451 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741897_1073 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/fadette1.txt._COPYING_
2018-09-29 13:14:31,852 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/fadette1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:31,875 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741898_1074, replicas=127.0.0.1:50010 for /user/Lag/ABU/farcefran1.txt._COPYING_
2018-09-29 13:14:31,893 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/farcefran1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:31,916 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741899_1075, replicas=127.0.0.1:50010 for /user/Lag/ABU/femtin1.txt._COPYING_
2018-09-29 13:14:31,933 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/femtin1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:31,952 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741900_1076, replicas=127.0.0.1:50010 for /user/Lag/ABU/feuilles1.txt._COPYING_
2018-09-29 13:14:31,967 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/feuilles1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:31,985 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741901_1077, replicas=127.0.0.1:50010 for /user/Lag/ABU/figchose1.txt._COPYING_
2018-09-29 13:14:32,001 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/figchose1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:32,020 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741902_1078, replicas=127.0.0.1:50010 for /user/Lag/ABU/flandeux1.txt._COPYING_
2018-09-29 13:14:32,036 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741902_1078 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/flandeux1.txt._COPYING_
2018-09-29 13:14:32,437 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/flandeux1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:32,469 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741903_1079, replicas=127.0.0.1:50010 for /user/Lag/ABU/football1.txt._COPYING_
2018-09-29 13:14:32,488 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/football1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:32,508 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741904_1080, replicas=127.0.0.1:50010 for /user/Lag/ABU/gargantua2.txt._COPYING_
2018-09-29 13:14:32,520 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741904_1080 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/gargantua2.txt._COPYING_
2018-09-29 13:14:32,922 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/gargantua2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:32,942 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741905_1081, replicas=127.0.0.1:50010 for /user/Lag/ABU/gaspard2.txt._COPYING_
2018-09-29 13:14:32,952 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741905_1081 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/gaspard2.txt._COPYING_
2018-09-29 13:14:33,354 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/gaspard2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:33,373 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741906_1082, replicas=127.0.0.1:50010 for /user/Lag/ABU/germinal1.txt._COPYING_
2018-09-29 13:14:33,390 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741906_1082 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/germinal1.txt._COPYING_
2018-09-29 13:14:33,791 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/germinal1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:33,814 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741907_1083, replicas=127.0.0.1:50010 for /user/Lag/ABU/germinie3.txt._COPYING_
2018-09-29 13:14:33,842 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/germinie3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:33,880 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741908_1084, replicas=127.0.0.1:50010 for /user/Lag/ABU/gilblas1.txt._COPYING_
2018-09-29 13:14:33,911 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/gilblas1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:33,930 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741909_1085, replicas=127.0.0.1:50010 for /user/Lag/ABU/guizeur1.txt._COPYING_
2018-09-29 13:14:33,952 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741909_1085 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/guizeur1.txt._COPYING_
2018-09-29 13:14:34,353 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/guizeur1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:34,373 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741910_1086, replicas=127.0.0.1:50010 for /user/Lag/ABU/hamlet1.txt._COPYING_
2018-09-29 13:14:34,392 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741910_1086 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/hamlet1.txt._COPYING_
2018-09-29 13:14:34,794 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/hamlet1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:34,818 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741911_1087, replicas=127.0.0.1:50010 for /user/Lag/ABU/harmonie1.txt._COPYING_
2018-09-29 13:14:34,833 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741911_1087 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/harmonie1.txt._COPYING_
2018-09-29 13:14:35,235 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/harmonie1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:35,257 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741912_1088, replicas=127.0.0.1:50010 for /user/Lag/ABU/historiettes2.txt._COPYING_
2018-09-29 13:14:35,276 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/historiettes2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:35,293 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741913_1089, replicas=127.0.0.1:50010 for /user/Lag/ABU/homecu1.txt._COPYING_
2018-09-29 13:14:35,307 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741913_1089 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/homecu1.txt._COPYING_
2018-09-29 13:14:35,709 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/homecu1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:35,735 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741914_1090, replicas=127.0.0.1:50010 for /user/Lag/ABU/horla3.txt._COPYING_
2018-09-29 13:14:35,754 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741914_1090 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/horla3.txt._COPYING_
2018-09-29 13:14:36,156 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/horla3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:36,174 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741915_1091, replicas=127.0.0.1:50010 for /user/Lag/ABU/hugoshak1.txt._COPYING_
2018-09-29 13:14:36,193 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/hugoshak1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:36,215 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741916_1092, replicas=127.0.0.1:50010 for /user/Lag/ABU/humilite3.txt._COPYING_
2018-09-29 13:14:36,234 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/humilite3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:36,256 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741917_1093, replicas=127.0.0.1:50010 for /user/Lag/ABU/ideolo1.txt._COPYING_
2018-09-29 13:14:36,276 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ideolo1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:36,294 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741918_1094, replicas=127.0.0.1:50010 for /user/Lag/ABU/illusion1.txt._COPYING_
2018-09-29 13:14:36,306 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741918_1094 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/illusion1.txt._COPYING_
2018-09-29 13:14:36,707 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/illusion1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:36,727 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741919_1095, replicas=127.0.0.1:50010 for /user/Lag/ABU/inquisit2.txt._COPYING_
2018-09-29 13:14:36,739 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741919_1095 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/inquisit2.txt._COPYING_
2018-09-29 13:14:37,140 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/inquisit2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:37,159 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741920_1096, replicas=127.0.0.1:50010 for /user/Lag/ABU/interpret2.txt._COPYING_
2018-09-29 13:14:37,171 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/interpret2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:37,188 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741921_1097, replicas=127.0.0.1:50010 for /user/Lag/ABU/iphigenie1.txt._COPYING_
2018-09-29 13:14:37,196 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741921_1097 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/iphigenie1.txt._COPYING_
2018-09-29 13:14:37,597 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/iphigenie1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:37,615 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741922_1098, replicas=127.0.0.1:50010 for /user/Lag/ABU/irnois2.txt._COPYING_
2018-09-29 13:14:37,626 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741922_1098 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/irnois2.txt._COPYING_
2018-09-29 13:14:38,028 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/irnois2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:38,046 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741923_1099, replicas=127.0.0.1:50010 for /user/Lag/ABU/jaccuse3.txt._COPYING_
2018-09-29 13:14:38,057 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741923_1099 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/jaccuse3.txt._COPYING_
2018-09-29 13:14:38,460 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/jaccuse3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:38,473 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741924_1100, replicas=127.0.0.1:50010 for /user/Lag/ABU/jacques1.txt._COPYING_
2018-09-29 13:14:38,485 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741924_1100 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/jacques1.txt._COPYING_
2018-09-29 13:14:38,888 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/jacques1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:38,910 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741925_1101, replicas=127.0.0.1:50010 for /user/Lag/ABU/journalism1.txt._COPYING_
2018-09-29 13:14:38,932 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741925_1101 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/journalism1.txt._COPYING_
2018-09-29 13:14:39,334 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/journalism1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:39,355 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741926_1102, replicas=127.0.0.1:50010 for /user/Lag/ABU/journbloy1.txt._COPYING_
2018-09-29 13:14:39,375 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741926_1102 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/journbloy1.txt._COPYING_
2018-09-29 13:14:39,777 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/journbloy1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:39,799 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741927_1103, replicas=127.0.0.1:50010 for /user/Lag/ABU/justice1.txt._COPYING_
2018-09-29 13:14:39,817 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/justice1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:39,840 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741928_1104, replicas=127.0.0.1:50010 for /user/Lag/ABU/lafille2.txt._COPYING_
2018-09-29 13:14:39,855 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741928_1104 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/lafille2.txt._COPYING_
2018-09-29 13:14:40,257 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/lafille2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:40,279 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741929_1105, replicas=127.0.0.1:50010 for /user/Lag/ABU/lecid1.txt._COPYING_
2018-09-29 13:14:40,299 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/lecid1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:40,319 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741930_1106, replicas=127.0.0.1:50010 for /user/Lag/ABU/legend1.txt._COPYING_
2018-09-29 13:14:40,337 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741930_1106 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/legend1.txt._COPYING_
2018-09-29 13:14:40,738 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/legend1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:40,763 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741931_1107, replicas=127.0.0.1:50010 for /user/Lag/ABU/legendet21.txt._COPYING_
2018-09-29 13:14:40,783 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741931_1107 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/legendet21.txt._COPYING_
2018-09-29 13:14:41,184 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/legendet21.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:41,205 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741932_1108, replicas=127.0.0.1:50010 for /user/Lag/ABU/leibnitzdiv1.txt._COPYING_
2018-09-29 13:14:41,222 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741932_1108 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/leibnitzdiv1.txt._COPYING_
2018-09-29 13:14:41,624 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/leibnitzdiv1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:41,646 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741933_1109, replicas=127.0.0.1:50010 for /user/Lag/ABU/lejeu1.txt._COPYING_
2018-09-29 13:14:41,663 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741933_1109 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/lejeu1.txt._COPYING_
2018-09-29 13:14:42,065 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/lejeu1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:42,087 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741934_1110, replicas=127.0.0.1:50010 for /user/Lag/ABU/lepetitchose1.txt._COPYING_
2018-09-29 13:14:42,106 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741934_1110 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/lepetitchose1.txt._COPYING_
2018-09-29 13:14:42,508 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/lepetitchose1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:42,531 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741935_1111, replicas=127.0.0.1:50010 for /user/Lag/ABU/lessoirees1.txt._COPYING_
2018-09-29 13:14:42,555 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741935_1111 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/lessoirees1.txt._COPYING_
2018-09-29 13:14:42,957 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/lessoirees1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:42,979 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741936_1112, replicas=127.0.0.1:50010 for /user/Lag/ABU/letphi1.txt._COPYING_
2018-09-29 13:14:42,997 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741936_1112 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/letphi1.txt._COPYING_
2018-09-29 13:14:43,399 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/letphi1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:43,420 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741937_1113, replicas=127.0.0.1:50010 for /user/Lag/ABU/lettresecrites1.txt._COPYING_
2018-09-29 13:14:43,438 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/lettresecrites1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:43,456 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741938_1114, replicas=127.0.0.1:50010 for /user/Lag/ABU/lettresjuives11.txt._COPYING_
2018-09-29 13:14:43,492 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741938_1114 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/lettresjuives11.txt._COPYING_
2018-09-29 13:14:43,894 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/lettresjuives11.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:43,916 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741939_1115, replicas=127.0.0.1:50010 for /user/Lag/ABU/lettresjuives231.txt._COPYING_
2018-09-29 13:14:43,940 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/lettresjuives231.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:43,983 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741940_1116, replicas=127.0.0.1:50010 for /user/Lag/ABU/lettresjuives451.txt._COPYING_
2018-09-29 13:14:44,028 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741940_1116 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/lettresjuives451.txt._COPYING_
2018-09-29 13:14:44,430 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/lettresjuives451.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:44,447 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741941_1117, replicas=127.0.0.1:50010 for /user/Lag/ABU/lettresjuives6781.txt._COPYING_
2018-09-29 13:14:44,472 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/lettresjuives6781.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:44,517 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741942_1118, replicas=127.0.0.1:50010 for /user/Lag/ABU/lettresreli2.txt._COPYING_
2018-09-29 13:14:44,534 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741942_1118 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/lettresreli2.txt._COPYING_
2018-09-29 13:14:44,935 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/lettresreli2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:44,953 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741943_1119, replicas=127.0.0.1:50010 for /user/Lag/ABU/levieux2.txt._COPYING_
2018-09-29 13:14:44,971 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/levieux2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:44,992 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741944_1120, replicas=127.0.0.1:50010 for /user/Lag/ABU/liaisons3.txt._COPYING_
2018-09-29 13:14:45,015 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741944_1120 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/liaisons3.txt._COPYING_
2018-09-29 13:14:45,416 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/liaisons3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:45,446 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741945_1121, replicas=127.0.0.1:50010 for /user/Lag/ABU/licong1.txt._COPYING_
2018-09-29 13:14:45,465 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/licong1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:45,484 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741946_1122, replicas=127.0.0.1:50010 for /user/Lag/ABU/lmain3.txt._COPYING_
2018-09-29 13:14:45,493 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741946_1122 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/lmain3.txt._COPYING_
2018-09-29 13:14:45,895 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/lmain3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:45,913 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741947_1123, replicas=127.0.0.1:50010 for /user/Lag/ABU/lutrin1.txt._COPYING_
2018-09-29 13:14:45,925 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/lutrin1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:45,942 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741948_1124, replicas=127.0.0.1:50010 for /user/Lag/ABU/m702douai2.txt._COPYING_
2018-09-29 13:14:45,953 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741948_1124 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/m702douai2.txt._COPYING_
2018-09-29 13:14:46,355 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/m702douai2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:46,373 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741949_1125, replicas=127.0.0.1:50010 for /user/Lag/ABU/machine3.txt._COPYING_
2018-09-29 13:14:46,383 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741949_1125 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/machine3.txt._COPYING_
2018-09-29 13:14:46,784 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/machine3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:46,797 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741950_1126, replicas=127.0.0.1:50010 for /user/Lag/ABU/manifeste2.txt._COPYING_
2018-09-29 13:14:46,804 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741950_1126 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/manifeste2.txt._COPYING_
2018-09-29 13:14:47,205 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/manifeste2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:47,219 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741951_1127, replicas=127.0.0.1:50010 for /user/Lag/ABU/manon2.txt._COPYING_
2018-09-29 13:14:47,231 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741951_1127 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/manon2.txt._COPYING_
2018-09-29 13:14:47,633 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/manon2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:47,650 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741952_1128, replicas=127.0.0.1:50010 for /user/Lag/ABU/marie1.txt._COPYING_
2018-09-29 13:14:47,665 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/marie1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:47,688 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741953_1129, replicas=127.0.0.1:50010 for /user/Lag/ABU/maximes2.txt._COPYING_
2018-09-29 13:14:47,705 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741953_1129 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/maximes2.txt._COPYING_
2018-09-29 13:14:48,106 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/maximes2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:48,123 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741954_1130, replicas=127.0.0.1:50010 for /user/Lag/ABU/medipoet1.txt._COPYING_
2018-09-29 13:14:48,137 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741954_1130 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/medipoet1.txt._COPYING_
2018-09-29 13:14:48,538 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/medipoet1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:48,555 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741955_1131, replicas=127.0.0.1:50010 for /user/Lag/ABU/medit3.txt._COPYING_
2018-09-29 13:14:48,570 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741955_1131 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/medit3.txt._COPYING_
2018-09-29 13:14:48,971 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/medit3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:48,988 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741956_1132, replicas=127.0.0.1:50010 for /user/Lag/ABU/medita1.txt._COPYING_
2018-09-29 13:14:49,004 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741956_1132 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/medita1.txt._COPYING_
2018-09-29 13:14:49,405 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/medita1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:49,422 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741957_1133, replicas=127.0.0.1:50010 for /user/Lag/ABU/methode3.txt._COPYING_
2018-09-29 13:14:49,436 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741957_1133 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/methode3.txt._COPYING_
2018-09-29 13:14:49,837 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/methode3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:49,854 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741958_1134, replicas=127.0.0.1:50010 for /user/Lag/ABU/micromeg3.txt._COPYING_
2018-09-29 13:14:49,872 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741958_1134 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/micromeg3.txt._COPYING_
2018-09-29 13:14:50,274 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/micromeg3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:50,295 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741959_1135, replicas=127.0.0.1:50010 for /user/Lag/ABU/monadologie1.txt._COPYING_
2018-09-29 13:14:50,311 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741959_1135 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/monadologie1.txt._COPYING_
2018-09-29 13:14:50,712 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/monadologie1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:50,734 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741960_1136, replicas=127.0.0.1:50010 for /user/Lag/ABU/monde1.txt._COPYING_
2018-09-29 13:14:50,751 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741960_1136 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/monde1.txt._COPYING_
2018-09-29 13:14:51,153 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/monde1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:51,166 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741961_1137, replicas=127.0.0.1:50010 for /user/Lag/ABU/montbard1.txt._COPYING_
2018-09-29 13:14:51,181 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741961_1137 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/montbard1.txt._COPYING_
2018-09-29 13:14:51,582 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/montbard1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:51,604 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741962_1138, replicas=127.0.0.1:50010 for /user/Lag/ABU/morte2.txt._COPYING_
2018-09-29 13:14:51,621 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741962_1138 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/morte2.txt._COPYING_
2018-09-29 13:14:52,022 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/morte2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:52,040 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741963_1139, replicas=127.0.0.1:50010 for /user/Lag/ABU/mousque1.txt._COPYING_
2018-09-29 13:14:52,065 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741963_1139 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/mousque1.txt._COPYING_
2018-09-29 13:14:52,467 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/mousque1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:52,488 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741964_1140, replicas=127.0.0.1:50010 for /user/Lag/ABU/nddp1.txt._COPYING_
2018-09-29 13:14:52,510 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741964_1140 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/nddp1.txt._COPYING_
2018-09-29 13:14:52,911 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/nddp1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:52,928 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741965_1141, replicas=127.0.0.1:50010 for /user/Lag/ABU/neveu2.txt._COPYING_
2018-09-29 13:14:52,943 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741965_1141 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/neveu2.txt._COPYING_
2018-09-29 13:14:53,343 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/neveu2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:53,360 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741966_1142, replicas=127.0.0.1:50010 for /user/Lag/ABU/nouvmedi1.txt._COPYING_
2018-09-29 13:14:53,375 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/nouvmedi1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:53,407 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741967_1143, replicas=127.0.0.1:50010 for /user/Lag/ABU/odespolit1.txt._COPYING_
2018-09-29 13:14:53,422 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741967_1143 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/odespolit1.txt._COPYING_
2018-09-29 13:14:53,823 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/odespolit1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:53,840 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741968_1144, replicas=127.0.0.1:50010 for /user/Lag/ABU/opinions3.txt._COPYING_
2018-09-29 13:14:53,855 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741968_1144 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/opinions3.txt._COPYING_
2018-09-29 13:14:54,255 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/opinions3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:54,269 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741969_1145, replicas=127.0.0.1:50010 for /user/Lag/ABU/oriental1.txt._COPYING_
2018-09-29 13:14:54,283 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741969_1145 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/oriental1.txt._COPYING_
2018-09-29 13:14:54,684 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/oriental1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:54,749 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741970_1146, replicas=127.0.0.1:50010 for /user/Lag/ABU/paresse3.txt._COPYING_
2018-09-29 13:14:54,774 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/paresse3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:54,798 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741971_1147, replicas=127.0.0.1:50010 for /user/Lag/ABU/partiecamp2.txt._COPYING_
2018-09-29 13:14:54,823 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741971_1147 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/partiecamp2.txt._COPYING_
2018-09-29 13:14:55,224 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/partiecamp2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:55,254 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741972_1148, replicas=127.0.0.1:50010 for /user/Lag/ABU/pascaldiv1.txt._COPYING_
2018-09-29 13:14:55,279 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741972_1148 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/pascaldiv1.txt._COPYING_
2018-09-29 13:14:55,680 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/pascaldiv1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:55,700 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741973_1149, replicas=127.0.0.1:50010 for /user/Lag/ABU/pascalpetits1.txt._COPYING_
2018-09-29 13:14:55,720 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741973_1149 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/pascalpetits1.txt._COPYING_
2018-09-29 13:14:56,122 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/pascalpetits1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:56,142 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741974_1150, replicas=127.0.0.1:50010 for /user/Lag/ABU/penseesXX1.txt._COPYING_
2018-09-29 13:14:56,181 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741974_1150 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/penseesXX1.txt._COPYING_
2018-09-29 13:14:56,583 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/penseesXX1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:56,607 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741975_1151, replicas=127.0.0.1:50010 for /user/Lag/ABU/pensepict1.txt._COPYING_
2018-09-29 13:14:56,635 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741975_1151 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/pensepict1.txt._COPYING_
2018-09-29 13:14:57,036 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/pensepict1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:57,073 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741976_1152, replicas=127.0.0.1:50010 for /user/Lag/ABU/phedre2.txt._COPYING_
2018-09-29 13:14:57,093 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741976_1152 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/phedre2.txt._COPYING_
2018-09-29 13:14:57,495 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/phedre2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:57,518 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741977_1153, replicas=127.0.0.1:50010 for /user/Lag/ABU/plural3.txt._COPYING_
2018-09-29 13:14:57,548 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741977_1153 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/plural3.txt._COPYING_
2018-09-29 13:14:57,949 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/plural3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:57,970 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741978_1154, replicas=127.0.0.1:50010 for /user/Lag/ABU/poemallar1.txt._COPYING_
2018-09-29 13:14:57,989 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741978_1154 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/poemallar1.txt._COPYING_
2018-09-29 13:14:58,391 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/poemallar1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:58,416 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741979_1155, replicas=127.0.0.1:50010 for /user/Lag/ABU/poemesadv1.txt._COPYING_
2018-09-29 13:14:58,440 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741979_1155 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/poemesadv1.txt._COPYING_
2018-09-29 13:14:58,842 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/poemesadv1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:58,862 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741980_1156, replicas=127.0.0.1:50010 for /user/Lag/ABU/presseguerre1.txt._COPYING_
2018-09-29 13:14:58,885 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741980_1156 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/presseguerre1.txt._COPYING_
2018-09-29 13:14:59,287 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/presseguerre1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:59,307 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741981_1157, replicas=127.0.0.1:50010 for /user/Lag/ABU/preuves1.txt._COPYING_
2018-09-29 13:14:59,335 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741981_1157 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/preuves1.txt._COPYING_
2018-09-29 13:14:59,737 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/preuves1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:59,889 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741982_1158, replicas=127.0.0.1:50010 for /user/Lag/ABU/propriete1.txt._COPYING_
2018-09-29 13:14:59,930 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/propriete1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:14:59,952 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741983_1159, replicas=127.0.0.1:50010 for /user/Lag/ABU/pucelle1.txt._COPYING_
2018-09-29 13:14:59,978 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741983_1159 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/pucelle1.txt._COPYING_
2018-09-29 13:15:00,380 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/pucelle1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:00,402 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741984_1160, replicas=127.0.0.1:50010 for /user/Lag/ABU/quatrevt1.txt._COPYING_
2018-09-29 13:15:00,431 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741984_1160 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/quatrevt1.txt._COPYING_
2018-09-29 13:15:00,833 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/quatrevt1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:00,856 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741985_1161, replicas=127.0.0.1:50010 for /user/Lag/ABU/raison2.txt._COPYING_
2018-09-29 13:15:00,881 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741985_1161 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/raison2.txt._COPYING_
2018-09-29 13:15:01,282 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/raison2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:01,322 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741986_1162, replicas=127.0.0.1:50010 for /user/Lag/ABU/rayons1.txt._COPYING_
2018-09-29 13:15:01,339 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741986_1162 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/rayons1.txt._COPYING_
2018-09-29 13:15:01,741 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/rayons1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:01,764 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741987_1163, replicas=127.0.0.1:50010 for /user/Lag/ABU/ren05101.txt._COPYING_
2018-09-29 13:15:01,781 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741987_1163 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ren05101.txt._COPYING_
2018-09-29 13:15:02,182 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ren05101.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:02,201 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741988_1164, replicas=127.0.0.1:50010 for /user/Lag/ABU/ren87922.txt._COPYING_
2018-09-29 13:15:02,225 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741988_1164 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ren87922.txt._COPYING_
2018-09-29 13:15:02,626 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ren87922.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:02,641 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741989_1165, replicas=127.0.0.1:50010 for /user/Lag/ABU/ren93982.txt._COPYING_
2018-09-29 13:15:02,696 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741989_1165 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ren93982.txt._COPYING_
2018-09-29 13:15:03,098 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ren93982.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:03,123 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741990_1166, replicas=127.0.0.1:50010 for /user/Lag/ABU/ren99041.txt._COPYING_
2018-09-29 13:15:03,143 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741990_1166 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ren99041.txt._COPYING_
2018-09-29 13:15:03,544 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ren99041.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:03,563 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741991_1167, replicas=127.0.0.1:50010 for /user/Lag/ABU/reveries3.txt._COPYING_
2018-09-29 13:15:03,591 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741991_1167 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/reveries3.txt._COPYING_
2018-09-29 13:15:03,992 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/reveries3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:04,013 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741992_1168, replicas=127.0.0.1:50010 for /user/Lag/ABU/rldased3.txt._COPYING_
2018-09-29 13:15:04,030 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741992_1168 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/rldased3.txt._COPYING_
2018-09-29 13:15:04,432 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/rldased3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:04,454 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741993_1169, replicas=127.0.0.1:50010 for /user/Lag/ABU/robur1.txt._COPYING_
2018-09-29 13:15:04,472 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741993_1169 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/robur1.txt._COPYING_
2018-09-29 13:15:04,874 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/robur1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:05,005 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741994_1170, replicas=127.0.0.1:50010 for /user/Lag/ABU/roland2.txt._COPYING_
2018-09-29 13:15:05,023 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741994_1170 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/roland2.txt._COPYING_
2018-09-29 13:15:05,424 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/roland2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:05,442 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741995_1171, replicas=127.0.0.1:50010 for /user/Lag/ABU/roletrav2.txt._COPYING_
2018-09-29 13:15:05,470 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741995_1171 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/roletrav2.txt._COPYING_
2018-09-29 13:15:05,872 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/roletrav2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:05,891 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741996_1172, replicas=127.0.0.1:50010 for /user/Lag/ABU/rouge1.txt._COPYING_
2018-09-29 13:15:05,922 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741996_1172 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/rouge1.txt._COPYING_
2018-09-29 13:15:06,323 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/rouge1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:06,350 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741997_1173, replicas=127.0.0.1:50010 for /user/Lag/ABU/rousgene1.txt._COPYING_
2018-09-29 13:15:06,371 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741997_1173 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/rousgene1.txt._COPYING_
2018-09-29 13:15:06,773 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/rousgene1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:06,797 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741998_1174, replicas=127.0.0.1:50010 for /user/Lag/ABU/ruesboi1.txt._COPYING_
2018-09-29 13:15:06,823 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741998_1174 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ruesboi1.txt._COPYING_
2018-09-29 13:15:07,225 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ruesboi1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:07,245 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741999_1175, replicas=127.0.0.1:50010 for /user/Lag/ABU/salalion1.txt._COPYING_
2018-09-29 13:15:07,264 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741999_1175 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/salalion1.txt._COPYING_
2018-09-29 13:15:07,665 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/salalion1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:07,687 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742000_1176, replicas=127.0.0.1:50010 for /user/Lag/ABU/salammb1.txt._COPYING_
2018-09-29 13:15:07,713 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742000_1176 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/salammb1.txt._COPYING_
2018-09-29 13:15:08,115 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/salammb1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:08,140 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742001_1177, replicas=127.0.0.1:50010 for /user/Lag/ABU/satan1.txt._COPYING_
2018-09-29 13:15:08,331 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742001_1177 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/satan1.txt._COPYING_
2018-09-29 13:15:08,732 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/satan1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:08,750 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742002_1178, replicas=127.0.0.1:50010 for /user/Lag/ABU/scapin2.txt._COPYING_
2018-09-29 13:15:08,768 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742002_1178 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/scapin2.txt._COPYING_
2018-09-29 13:15:09,170 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/scapin2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:09,188 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742003_1179, replicas=127.0.0.1:50010 for /user/Lag/ABU/scihyp2.txt._COPYING_
2018-09-29 13:15:09,208 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742003_1179 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/scihyp2.txt._COPYING_
2018-09-29 13:15:09,610 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/scihyp2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:09,654 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742004_1180, replicas=127.0.0.1:50010 for /user/Lag/ABU/septfem2.txt._COPYING_
2018-09-29 13:15:09,678 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742004_1180 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/septfem2.txt._COPYING_
2018-09-29 13:15:10,079 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/septfem2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:10,145 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742005_1181, replicas=127.0.0.1:50010 for /user/Lag/ABU/smarra1.txt._COPYING_
2018-09-29 13:15:10,163 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742005_1181 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/smarra1.txt._COPYING_
2018-09-29 13:15:10,564 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/smarra1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:10,584 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742006_1182, replicas=127.0.0.1:50010 for /user/Lag/ABU/souveni1.txt._COPYING_
2018-09-29 13:15:10,608 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742006_1182 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/souveni1.txt._COPYING_
2018-09-29 13:15:11,010 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/souveni1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:11,033 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742007_1183, replicas=127.0.0.1:50010 for /user/Lag/ABU/supplem2.txt._COPYING_
2018-09-29 13:15:11,051 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742007_1183 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/supplem2.txt._COPYING_
2018-09-29 13:15:11,452 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/supplem2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:11,485 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742008_1184, replicas=127.0.0.1:50010 for /user/Lag/ABU/tartuf2.txt._COPYING_
2018-09-29 13:15:11,516 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742008_1184 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/tartuf2.txt._COPYING_
2018-09-29 13:15:11,917 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/tartuf2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:11,939 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742009_1185, replicas=127.0.0.1:50010 for /user/Lag/ABU/tdm80j2.txt._COPYING_
2018-09-29 13:15:11,958 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742009_1185 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/tdm80j2.txt._COPYING_
2018-09-29 13:15:12,359 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/tdm80j2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:12,383 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742010_1186, replicas=127.0.0.1:50010 for /user/Lag/ABU/theodicee1.txt._COPYING_
2018-09-29 13:15:12,403 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742010_1186 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/theodicee1.txt._COPYING_
2018-09-29 13:15:12,805 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/theodicee1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:12,830 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742011_1187, replicas=127.0.0.1:50010 for /user/Lag/ABU/theoriephys1.txt._COPYING_
2018-09-29 13:15:12,850 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742011_1187 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/theoriephys1.txt._COPYING_
2018-09-29 13:15:13,252 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/theoriephys1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:13,274 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742012_1188, replicas=127.0.0.1:50010 for /user/Lag/ABU/thotri1.txt._COPYING_
2018-09-29 13:15:13,298 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742012_1188 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/thotri1.txt._COPYING_
2018-09-29 13:15:13,699 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/thotri1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:13,719 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742013_1189, replicas=127.0.0.1:50010 for /user/Lag/ABU/tlun3.txt._COPYING_
2018-09-29 13:15:13,747 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742013_1189 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/tlun3.txt._COPYING_
2018-09-29 13:15:14,149 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/tlun3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:14,170 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742014_1190, replicas=127.0.0.1:50010 for /user/Lag/ABU/tome1malherbe1.txt._COPYING_
2018-09-29 13:15:14,196 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742014_1190 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/tome1malherbe1.txt._COPYING_
2018-09-29 13:15:14,598 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/tome1malherbe1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:14,624 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742015_1191, replicas=127.0.0.1:50010 for /user/Lag/ABU/troylion1.txt._COPYING_
2018-09-29 13:15:14,651 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742015_1191 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/troylion1.txt._COPYING_
2018-09-29 13:15:15,053 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/troylion1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:15,136 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742016_1192, replicas=127.0.0.1:50010 for /user/Lag/ABU/ulenspiegel1.txt._COPYING_
2018-09-29 13:15:15,160 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742016_1192 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/ulenspiegel1.txt._COPYING_
2018-09-29 13:15:15,562 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/ulenspiegel1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:15,580 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742017_1193, replicas=127.0.0.1:50010 for /user/Lag/ABU/uncoeur3.txt._COPYING_
2018-09-29 13:15:15,598 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742017_1193 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/uncoeur3.txt._COPYING_
2018-09-29 13:15:16,000 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/uncoeur3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:16,021 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742018_1194, replicas=127.0.0.1:50010 for /user/Lag/ABU/unevie2.txt._COPYING_
2018-09-29 13:15:16,041 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742018_1194 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/unevie2.txt._COPYING_
2018-09-29 13:15:16,442 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/unevie2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:16,466 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742019_1195, replicas=127.0.0.1:50010 for /user/Lag/ABU/utopique2.txt._COPYING_
2018-09-29 13:15:16,491 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742019_1195 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/utopique2.txt._COPYING_
2018-09-29 13:15:16,892 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/utopique2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:16,921 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742020_1196, replicas=127.0.0.1:50010 for /user/Lag/ABU/verhafin1.txt._COPYING_
2018-09-29 13:15:16,939 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742020_1196 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/verhafin1.txt._COPYING_
2018-09-29 13:15:17,341 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/verhafin1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:17,357 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742021_1197, replicas=127.0.0.1:50010 for /user/Lag/ABU/viemars3.txt._COPYING_
2018-09-29 13:15:17,375 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742021_1197 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/viemars3.txt._COPYING_
2018-09-29 13:15:17,776 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/viemars3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:17,797 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742022_1198, replicas=127.0.0.1:50010 for /user/Lag/ABU/vignypoesie1.txt._COPYING_
2018-09-29 13:15:17,814 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742022_1198 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/vignypoesie1.txt._COPYING_
2018-09-29 13:15:18,216 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/vignypoesie1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:18,237 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742023_1199, replicas=127.0.0.1:50010 for /user/Lag/ABU/volpofin1.txt._COPYING_
2018-09-29 13:15:18,256 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742023_1199 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/volpofin1.txt._COPYING_
2018-09-29 13:15:18,657 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/volpofin1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:18,681 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742024_1200, replicas=127.0.0.1:50010 for /user/Lag/ABU/voltgene1.txt._COPYING_
2018-09-29 13:15:18,708 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742024_1200 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/voltgene1.txt._COPYING_
2018-09-29 13:15:19,109 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/voltgene1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:19,128 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742025_1201, replicas=127.0.0.1:50010 for /user/Lag/ABU/voyfran1.txt._COPYING_
2018-09-29 13:15:19,157 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742025_1201 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/voyfran1.txt._COPYING_
2018-09-29 13:15:19,559 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/voyfran1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:15:19,580 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742026_1202, replicas=127.0.0.1:50010 for /user/Lag/ABU/voylun3.txt._COPYING_
2018-09-29 13:15:19,602 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742026_1202 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/Lag/ABU/voylun3.txt._COPYING_
2018-09-29 13:15:20,004 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/Lag/ABU/voylun3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1270029206_1
2018-09-29 13:16:23,869 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1135 Total time for transactions(ms): 83 Number of transactions batched in Syncs: 192 Number of syncs: 940 SyncTimes(ms): 5622 
2018-09-29 13:16:24,051 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call Call#4 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:42464: java.io.FileNotFoundException: Path is not a file: /user/Lag/ABU
2018-09-29 13:17:26,336 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1137 Total time for transactions(ms): 84 Number of transactions batched in Syncs: 195 Number of syncs: 942 SyncTimes(ms): 5627 
2018-09-29 13:20:35,297 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1140 Total time for transactions(ms): 85 Number of transactions batched in Syncs: 195 Number of syncs: 943 SyncTimes(ms): 5629 
2018-09-29 13:20:54,894 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742027_1203, replicas=127.0.0.1:50010 for /user/output2/_temporary/0/_temporary/attempt_local2108790959_0001_r_000000_0/part-r-00000
2018-09-29 13:20:55,092 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output2/_temporary/0/_temporary/attempt_local2108790959_0001_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_1208335743_1
2018-09-29 13:20:56,442 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742028_1204, replicas=127.0.0.1:50010 for /user/output2/_temporary/0/_temporary/attempt_local2108790959_0001_r_000001_0/part-r-00001
2018-09-29 13:20:56,581 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742028_1204 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/output2/_temporary/0/_temporary/attempt_local2108790959_0001_r_000001_0/part-r-00001
2018-09-29 13:20:56,983 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output2/_temporary/0/_temporary/attempt_local2108790959_0001_r_000001_0/part-r-00001 is closed by DFSClient_NONMAPREDUCE_1208335743_1
2018-09-29 13:20:58,340 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742029_1205, replicas=127.0.0.1:50010 for /user/output2/_temporary/0/_temporary/attempt_local2108790959_0001_r_000002_0/part-r-00002
2018-09-29 13:20:58,472 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742029_1205 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/output2/_temporary/0/_temporary/attempt_local2108790959_0001_r_000002_0/part-r-00002
2018-09-29 13:20:58,873 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output2/_temporary/0/_temporary/attempt_local2108790959_0001_r_000002_0/part-r-00002 is closed by DFSClient_NONMAPREDUCE_1208335743_1
2018-09-29 13:20:58,970 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output2/_SUCCESS is closed by DFSClient_NONMAPREDUCE_1208335743_1
2018-09-29 13:49:16,777 INFO BlockStateChange: BLOCK* processReport 0x647d9f1e6fd4b244: from storage DS-011e18f7-a2a2-4dea-8098-8361e16537ba node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=e0a9757c-a277-47ac-b700-5f0e9eb7f9ae, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-88860281-4205-4492-aa60-05a152b2daf2;nsid=599153523;c=1538219587368), blocks: 205, hasStaleStorage: false, processing time: 3 msecs, invalidatedBlocks: 0
2018-09-29 14:14:25,905 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-09-29 14:14:25,906 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-09-29 14:14:25,906 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 88, 1255
2018-09-29 14:14:25,906 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1169 Total time for transactions(ms): 85 Number of transactions batched in Syncs: 204 Number of syncs: 965 SyncTimes(ms): 5764 
2018-09-29 14:14:25,917 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1169 Total time for transactions(ms): 85 Number of transactions batched in Syncs: 204 Number of syncs: 966 SyncTimes(ms): 5775 
2018-09-29 14:14:25,918 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000000088 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000088-0000000000000001256
2018-09-29 14:14:25,918 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1257
2018-09-29 14:14:26,258 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000088-0000000000000001256, fileSize: 110389. Sent total: 110389 bytes. Size of last segment intended to send: -1 bytes.
2018-09-29 14:14:26,586 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Combined time for fsimage download and fsync to all disks took 0.00s. The fsimage download took 0.00s at 7500.00 KB/s. Synchronous (fsync) write to disk of /tmp/hadoop-lag/dfs/name/current/fsimage.ckpt_0000000000000001256 took 0.00s.
2018-09-29 14:14:26,586 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000001256 size 15906 bytes.
2018-09-29 14:14:26,600 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 87
2018-09-29 14:14:26,601 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2018-09-29 15:14:26,907 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-09-29 15:14:26,907 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-09-29 15:14:26,907 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1257, 1257
2018-09-29 15:14:26,907 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 308 
2018-09-29 15:14:26,909 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 310 
2018-09-29 15:14:26,911 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000001257 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000001257-0000000000000001258
2018-09-29 15:14:26,911 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1259
2018-09-29 15:14:27,008 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/edits_0000000000000001257-0000000000000001258, fileSize: 42. Sent total: 42 bytes. Size of last segment intended to send: -1 bytes.
2018-09-29 15:14:27,072 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Combined time for fsimage download and fsync to all disks took 0.00s. The fsimage download took 0.00s at 15000.00 KB/s. Synchronous (fsync) write to disk of /tmp/hadoop-lag/dfs/name/current/fsimage.ckpt_0000000000000001258 took 0.00s.
2018-09-29 15:14:27,072 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000001258 size 15906 bytes.
2018-09-29 15:14:27,079 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 1256
2018-09-29 15:14:27,080 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000087, cpktTxId=0000000000000000087)
2018-09-29 16:14:27,377 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-09-29 16:14:27,377 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-09-29 16:14:27,377 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1259, 1259
2018-09-29 16:14:27,377 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 74 
2018-09-29 16:14:27,380 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 76 
2018-09-29 16:14:27,380 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000001259 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000001259-0000000000000001260
2018-09-29 16:14:27,380 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1261
2018-09-29 16:14:27,479 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/edits_0000000000000001259-0000000000000001260, fileSize: 42. Sent total: 42 bytes. Size of last segment intended to send: -1 bytes.
2018-09-29 16:14:27,530 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Combined time for fsimage download and fsync to all disks took 0.00s. The fsimage download took 0.00s at 15000.00 KB/s. Synchronous (fsync) write to disk of /tmp/hadoop-lag/dfs/name/current/fsimage.ckpt_0000000000000001260 took 0.00s.
2018-09-29 16:14:27,530 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000001260 size 15906 bytes.
2018-09-29 16:14:27,537 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 1258
2018-09-29 16:14:27,537 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000001256, cpktTxId=0000000000000001256)
2018-09-29 17:14:27,839 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-09-29 17:14:27,839 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-09-29 17:14:27,839 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1261, 1261
2018-09-29 17:14:27,840 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 83 
2018-09-29 17:14:27,842 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 85 
2018-09-29 17:14:27,843 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000001261 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000001261-0000000000000001262
2018-09-29 17:14:27,843 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1263
2018-09-29 17:14:27,919 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/edits_0000000000000001261-0000000000000001262, fileSize: 42. Sent total: 42 bytes. Size of last segment intended to send: -1 bytes.
2018-09-29 17:14:27,960 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Combined time for fsimage download and fsync to all disks took 0.00s. The fsimage download took 0.00s at 15000.00 KB/s. Synchronous (fsync) write to disk of /tmp/hadoop-lag/dfs/name/current/fsimage.ckpt_0000000000000001262 took 0.00s.
2018-09-29 17:14:27,960 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000001262 size 15906 bytes.
2018-09-29 17:14:27,968 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 1260
2018-09-29 17:14:27,968 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000001258, cpktTxId=0000000000000001258)
2018-09-29 18:14:28,236 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-09-29 18:14:28,236 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-09-29 18:14:28,236 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1263, 1263
2018-09-29 18:14:28,237 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 61 
2018-09-29 18:14:28,239 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 63 
2018-09-29 18:14:28,240 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000001263 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000001263-0000000000000001264
2018-09-29 18:14:28,240 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1265
2018-09-29 18:14:28,338 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/edits_0000000000000001263-0000000000000001264, fileSize: 42. Sent total: 42 bytes. Size of last segment intended to send: -1 bytes.
2018-09-29 18:14:28,399 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Combined time for fsimage download and fsync to all disks took 0.00s. The fsimage download took 0.00s at 15000.00 KB/s. Synchronous (fsync) write to disk of /tmp/hadoop-lag/dfs/name/current/fsimage.ckpt_0000000000000001264 took 0.00s.
2018-09-29 18:14:28,399 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000001264 size 15906 bytes.
2018-09-29 18:14:28,415 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 1262
2018-09-29 18:14:28,415 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000001260, cpktTxId=0000000000000001260)
2018-09-29 19:14:28,639 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-09-29 19:14:28,639 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-09-29 19:14:28,639 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1265, 1265
2018-09-29 19:14:28,640 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 77 
2018-09-29 19:14:28,642 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 79 
2018-09-29 19:14:28,643 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000001265 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000001265-0000000000000001266
2018-09-29 19:14:28,643 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1267
2018-09-29 19:14:28,744 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/edits_0000000000000001265-0000000000000001266, fileSize: 42. Sent total: 42 bytes. Size of last segment intended to send: -1 bytes.
2018-09-29 19:14:28,809 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Combined time for fsimage download and fsync to all disks took 0.00s. The fsimage download took 0.00s at 15000.00 KB/s. Synchronous (fsync) write to disk of /tmp/hadoop-lag/dfs/name/current/fsimage.ckpt_0000000000000001266 took 0.00s.
2018-09-29 19:14:28,809 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000001266 size 15906 bytes.
2018-09-29 19:14:28,817 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 1264
2018-09-29 19:14:28,817 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000001262, cpktTxId=0000000000000001262)
2018-09-29 19:49:15,380 INFO BlockStateChange: BLOCK* processReport 0x647d9f1e6fd4b245: from storage DS-011e18f7-a2a2-4dea-8098-8361e16537ba node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=e0a9757c-a277-47ac-b700-5f0e9eb7f9ae, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-88860281-4205-4492-aa60-05a152b2daf2;nsid=599153523;c=1538219587368), blocks: 205, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
2018-09-29 20:14:29,032 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-09-29 20:14:29,032 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-09-29 20:14:29,032 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1267, 1267
2018-09-29 20:14:29,033 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 85 
2018-09-29 20:14:29,045 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 96 
2018-09-29 20:14:29,046 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000001267 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000001267-0000000000000001268
2018-09-29 20:14:29,046 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1269
2018-09-29 20:14:29,105 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/edits_0000000000000001267-0000000000000001268, fileSize: 42. Sent total: 42 bytes. Size of last segment intended to send: -1 bytes.
2018-09-29 20:14:29,150 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Combined time for fsimage download and fsync to all disks took 0.00s. The fsimage download took 0.00s at 15000.00 KB/s. Synchronous (fsync) write to disk of /tmp/hadoop-lag/dfs/name/current/fsimage.ckpt_0000000000000001268 took 0.00s.
2018-09-29 20:14:29,151 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000001268 size 15906 bytes.
2018-09-29 20:14:29,158 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 1266
2018-09-29 20:14:29,158 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000001264, cpktTxId=0000000000000001264)
2018-09-29 20:34:29,267 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2018-09-29 20:34:29,269 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at lag-Predator-G3-571/127.0.1.1
************************************************************/
2018-10-01 15:40:23,701 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = lag-Predator-G3-571/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.9.1
STARTUP_MSG:   classpath = /home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/etc/hadoop:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/activation-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-auth-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jettison-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/junit-4.11.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-annotations-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsch-0.1.54.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-nfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/fst-2.50.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-core-1.8.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-1.7.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-framework-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsp-api-2.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/gson-2.2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-recipes-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jcip-annotations-1.0-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-digester-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-net-3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-smart-1.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsch-0.1.54.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-router-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-registry-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-api-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.9.1.jar:/usr/lib/jvm/java-8-openjdk-amd64//lib/tools.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e30710aea4e6e55e69372929106cf119af06fd0e; compiled by 'root' on 2018-04-16T09:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-10-01 15:40:23,715 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-10-01 15:40:23,717 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2018-10-01 15:40:23,820 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-10-01 15:40:23,875 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-10-01 15:40:23,875 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2018-10-01 15:40:23,898 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2018-10-01 15:40:23,900 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2018-10-01 15:40:23,993 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-10-01 15:40:24,034 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2018-10-01 15:40:24,112 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-10-01 15:40:24,117 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-10-01 15:40:24,134 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2018-10-01 15:40:24,138 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-10-01 15:40:24,140 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2018-10-01 15:40:24,140 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-10-01 15:40:24,140 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-10-01 15:40:24,263 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2018-10-01 15:40:24,263 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2018-10-01 15:40:24,273 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2018-10-01 15:40:24,273 INFO org.mortbay.log: jetty-6.1.26
2018-10-01 15:40:24,435 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2018-10-01 15:40:24,453 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-10-01 15:40:24,453 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-10-01 15:40:24,481 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2018-10-01 15:40:24,490 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2018-10-01 15:40:24,491 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2018-10-01 15:40:24,492 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2018-10-01 15:40:24,495 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = lag (auth:SIMPLE)
2018-10-01 15:40:24,495 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2018-10-01 15:40:24,495 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2018-10-01 15:40:24,495 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2018-10-01 15:40:24,531 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-10-01 15:40:24,551 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2018-10-01 15:40:24,551 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2018-10-01 15:40:24,553 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2018-10-01 15:40:24,553 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2018 Oct 01 15:40:24
2018-10-01 15:40:24,554 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2018-10-01 15:40:24,554 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-10-01 15:40:24,555 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2018-10-01 15:40:24,555 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2018-10-01 15:40:24,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2018-10-01 15:40:24,564 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS
2018-10-01 15:40:24,566 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2018-10-01 15:40:24,566 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2018-10-01 15:40:24,566 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2018-10-01 15:40:24,566 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2018-10-01 15:40:24,566 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2018-10-01 15:40:24,566 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2018-10-01 15:40:24,566 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2018-10-01 15:40:24,567 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2018-10-01 15:40:24,567 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2018-10-01 15:40:24,567 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2018-10-01 15:40:24,567 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2018-10-01 15:40:24,567 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2018-10-01 15:40:24,620 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2018-10-01 15:40:24,620 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-10-01 15:40:24,620 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2018-10-01 15:40:24,621 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2018-10-01 15:40:24,621 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2018-10-01 15:40:24,621 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2018-10-01 15:40:24,621 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2018-10-01 15:40:24,623 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false
2018-10-01 15:40:24,626 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2018-10-01 15:40:24,626 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-10-01 15:40:24,626 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2018-10-01 15:40:24,626 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2018-10-01 15:40:24,628 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2018-10-01 15:40:24,628 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2018-10-01 15:40:24,628 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2018-10-01 15:40:24,630 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2018-10-01 15:40:24,631 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2018-10-01 15:40:24,632 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2018-10-01 15:40:24,632 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-10-01 15:40:24,632 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2018-10-01 15:40:24,632 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2018-10-01 15:40:24,633 WARN org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-lag/dfs/name does not exist
2018-10-01 15:40:24,634 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-lag/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:375)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:226)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1048)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:681)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:666)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:728)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:953)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:932)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1741)
2018-10-01 15:40:24,636 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2018-10-01 15:40:24,737 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2018-10-01 15:40:24,738 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2018-10-01 15:40:24,738 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2018-10-01 15:40:24,739 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-lag/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:375)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:226)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1048)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:681)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:666)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:728)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:953)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:932)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1741)
2018-10-01 15:40:24,742 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1: org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-lag/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
2018-10-01 15:40:24,745 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at lag-Predator-G3-571/127.0.1.1
************************************************************/
2018-10-01 15:41:36,240 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = lag-Predator-G3-571/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.9.1
STARTUP_MSG:   classpath = /home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/etc/hadoop:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/activation-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-auth-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jettison-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/junit-4.11.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-annotations-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsch-0.1.54.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-nfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/fst-2.50.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-core-1.8.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-1.7.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-framework-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsp-api-2.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/gson-2.2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-recipes-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jcip-annotations-1.0-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-digester-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-net-3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-smart-1.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsch-0.1.54.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-router-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-registry-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-api-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.9.1.jar:/usr/lib/jvm/java-8-openjdk-amd64//lib/tools.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e30710aea4e6e55e69372929106cf119af06fd0e; compiled by 'root' on 2018-04-16T09:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-10-01 15:41:36,245 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-10-01 15:41:36,248 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2018-10-01 15:41:36,341 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-10-01 15:41:36,400 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-10-01 15:41:36,400 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2018-10-01 15:41:36,414 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2018-10-01 15:41:36,416 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2018-10-01 15:41:36,508 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-10-01 15:41:36,515 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2018-10-01 15:41:36,545 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-10-01 15:41:36,550 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-10-01 15:41:36,565 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2018-10-01 15:41:36,569 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-10-01 15:41:36,570 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2018-10-01 15:41:36,570 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-10-01 15:41:36,570 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-10-01 15:41:36,640 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2018-10-01 15:41:36,640 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2018-10-01 15:41:36,648 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2018-10-01 15:41:36,648 INFO org.mortbay.log: jetty-6.1.26
2018-10-01 15:41:36,759 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2018-10-01 15:41:36,779 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-10-01 15:41:36,779 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-10-01 15:41:36,821 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2018-10-01 15:41:36,826 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2018-10-01 15:41:36,827 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2018-10-01 15:41:36,828 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2018-10-01 15:41:36,831 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = lag (auth:SIMPLE)
2018-10-01 15:41:36,831 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2018-10-01 15:41:36,831 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2018-10-01 15:41:36,831 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2018-10-01 15:41:36,846 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-10-01 15:41:36,856 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2018-10-01 15:41:36,856 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2018-10-01 15:41:36,858 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2018-10-01 15:41:36,858 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2018 Oct 01 15:41:36
2018-10-01 15:41:36,859 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2018-10-01 15:41:36,859 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-10-01 15:41:36,860 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2018-10-01 15:41:36,860 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2018-10-01 15:41:36,867 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2018-10-01 15:41:36,868 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS
2018-10-01 15:41:36,870 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2018-10-01 15:41:36,870 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2018-10-01 15:41:36,870 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2018-10-01 15:41:36,870 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2018-10-01 15:41:36,870 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2018-10-01 15:41:36,870 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2018-10-01 15:41:36,870 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2018-10-01 15:41:36,870 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2018-10-01 15:41:36,870 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2018-10-01 15:41:36,870 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2018-10-01 15:41:36,870 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2018-10-01 15:41:36,871 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2018-10-01 15:41:36,907 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2018-10-01 15:41:36,907 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-10-01 15:41:36,907 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2018-10-01 15:41:36,907 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2018-10-01 15:41:36,907 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2018-10-01 15:41:36,907 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2018-10-01 15:41:36,907 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2018-10-01 15:41:36,910 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false
2018-10-01 15:41:36,913 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2018-10-01 15:41:36,913 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-10-01 15:41:36,913 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2018-10-01 15:41:36,913 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2018-10-01 15:41:36,915 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2018-10-01 15:41:36,915 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2018-10-01 15:41:36,915 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2018-10-01 15:41:36,917 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2018-10-01 15:41:36,917 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2018-10-01 15:41:36,919 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2018-10-01 15:41:36,919 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-10-01 15:41:36,919 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2018-10-01 15:41:36,919 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2018-10-01 15:41:36,973 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-lag/dfs/name/in_use.lock acquired by nodename 7879@lag-Predator-G3-571
2018-10-01 15:41:37,007 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-lag/dfs/name/current
2018-10-01 15:41:37,008 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2018-10-01 15:41:37,008 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2018-10-01 15:41:37,066 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2018-10-01 15:41:37,082 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2018-10-01 15:41:37,082 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000
2018-10-01 15:41:37,085 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2018-10-01 15:41:37,085 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2018-10-01 15:41:37,198 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2018-10-01 15:41:37,199 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 278 msecs
2018-10-01 15:41:37,401 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2018-10-01 15:41:37,404 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-10-01 15:41:37,410 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2018-10-01 15:41:37,442 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2018-10-01 15:41:37,455 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2018-10-01 15:41:37,460 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues
2018-10-01 15:41:37,460 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2018-10-01 15:41:37,460 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2018-10-01 15:41:37,460 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2018-10-01 15:41:37,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2018-10-01 15:41:37,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2018-10-01 15:41:37,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2018-10-01 15:41:37,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2018-10-01 15:41:37,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2018-10-01 15:41:37,468 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 8 msec
2018-10-01 15:41:37,479 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-10-01 15:41:37,479 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2018-10-01 15:41:37,481 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2018-10-01 15:41:37,483 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2018-10-01 15:41:37,483 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2018-10-01 15:41:37,485 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 2 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2018-10-01 15:41:37,486 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2018-10-01 15:41:41,930 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=8ca0fc22-95d3-4df4-894b-2422baee26f4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-a777b8f1-3585-4228-96aa-cd08fe28d2e5;nsid=758020841;c=1538401289851) storage 8ca0fc22-95d3-4df4-894b-2422baee26f4
2018-10-01 15:41:41,931 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2018-10-01 15:41:41,931 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 8ca0fc22-95d3-4df4-894b-2422baee26f4 (127.0.0.1:50010).
2018-10-01 15:41:41,966 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-ac8fde1a-4e4d-418d-b8c3-5649c3ae68c2 for DN 127.0.0.1:50010
2018-10-01 15:41:41,990 INFO BlockStateChange: BLOCK* processReport 0xa9f0b0e3e5765506: Processing first storage report for DS-ac8fde1a-4e4d-418d-b8c3-5649c3ae68c2 from datanode 8ca0fc22-95d3-4df4-894b-2422baee26f4
2018-10-01 15:41:41,991 INFO BlockStateChange: BLOCK* processReport 0xa9f0b0e3e5765506: from storage DS-ac8fde1a-4e4d-418d-b8c3-5649c3ae68c2 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=8ca0fc22-95d3-4df4-894b-2422baee26f4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-a777b8f1-3585-4228-96aa-cd08fe28d2e5;nsid=758020841;c=1538401289851), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2018-10-01 15:42:26,577 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=127.0.0.1:50010 for /user/ABU/adelaide2.txt._COPYING_
2018-10-01 15:42:26,714 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741825_1001 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/adelaide2.txt._COPYING_
2018-10-01 15:42:27,118 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/adelaide2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:27,142 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=127.0.0.1:50010 for /user/ABU/aiglon1.txt._COPYING_
2018-10-01 15:42:27,160 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/aiglon1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:27,182 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=127.0.0.1:50010 for /user/ABU/amoursjaunes1.txt._COPYING_
2018-10-01 15:42:27,206 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/amoursjaunes1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:27,235 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=127.0.0.1:50010 for /user/ABU/andromaque1.txt._COPYING_
2018-10-01 15:42:27,257 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/andromaque1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:27,283 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=127.0.0.1:50010 for /user/ABU/ane1.txt._COPYING_
2018-10-01 15:42:27,307 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ane1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:27,323 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=127.0.0.1:50010 for /user/ABU/annee1.txt._COPYING_
2018-10-01 15:42:27,341 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/annee1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:27,360 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=127.0.0.1:50010 for /user/ABU/antiquites1.txt._COPYING_
2018-10-01 15:42:27,380 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/antiquites1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:27,403 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008, replicas=127.0.0.1:50010 for /user/ABU/archiduc1.txt._COPYING_
2018-10-01 15:42:27,427 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/archiduc1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:27,474 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009, replicas=127.0.0.1:50010 for /user/ABU/arebours1.txt._COPYING_
2018-10-01 15:42:27,500 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/arebours1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:27,525 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010, replicas=127.0.0.1:50010 for /user/ABU/argent1.txt._COPYING_
2018-10-01 15:42:27,546 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/argent1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:27,564 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011, replicas=127.0.0.1:50010 for /user/ABU/armance2.txt._COPYING_
2018-10-01 15:42:27,581 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741835_1011 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/armance2.txt._COPYING_
2018-10-01 15:42:27,983 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/armance2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:28,004 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012, replicas=127.0.0.1:50010 for /user/ABU/arriamar2.txt._COPYING_
2018-10-01 15:42:28,023 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/arriamar2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:28,046 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013, replicas=127.0.0.1:50010 for /user/ABU/artgrdp1.txt._COPYING_
2018-10-01 15:42:28,064 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/artgrdp1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:28,084 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014, replicas=127.0.0.1:50010 for /user/ABU/athalie2.txt._COPYING_
2018-10-01 15:42:28,108 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/athalie2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:28,135 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015, replicas=127.0.0.1:50010 for /user/ABU/avare2.txt._COPYING_
2018-10-01 15:42:28,158 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/avare2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:28,185 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016, replicas=127.0.0.1:50010 for /user/ABU/aziyade1.txt._COPYING_
2018-10-01 15:42:28,209 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/aziyade1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:28,251 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017, replicas=127.0.0.1:50010 for /user/ABU/ballades1.txt._COPYING_
2018-10-01 15:42:28,273 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ballades1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:28,297 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018, replicas=127.0.0.1:50010 for /user/ABU/ballon1.txt._COPYING_
2018-10-01 15:42:28,323 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ballon1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:28,345 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019, replicas=127.0.0.1:50010 for /user/ABU/becass2.txt._COPYING_
2018-10-01 15:42:28,368 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/becass2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:28,392 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020, replicas=127.0.0.1:50010 for /user/ABU/begum2.txt._COPYING_
2018-10-01 15:42:28,417 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741844_1020 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/begum2.txt._COPYING_
2018-10-01 15:42:28,820 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/begum2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:28,842 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021, replicas=127.0.0.1:50010 for /user/ABU/belami2.txt._COPYING_
2018-10-01 15:42:28,870 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741845_1021 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/belami2.txt._COPYING_
2018-10-01 15:42:29,273 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/belami2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:30,027 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022, replicas=127.0.0.1:50010 for /user/ABU/bertris1.txt._COPYING_
2018-10-01 15:42:30,175 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/bertris1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:30,438 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023, replicas=127.0.0.1:50010 for /user/ABU/blocus3.txt._COPYING_
2018-10-01 15:42:30,580 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/blocus3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:31,260 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024, replicas=127.0.0.1:50010 for /user/ABU/boule2.txt._COPYING_
2018-10-01 15:42:31,382 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/boule2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:31,577 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025, replicas=127.0.0.1:50010 for /user/ABU/bounty1.txt._COPYING_
2018-10-01 15:42:31,695 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/bounty1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:31,865 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741850_1026, replicas=127.0.0.1:50010 for /user/ABU/bouvard2.txt._COPYING_
2018-10-01 15:42:31,993 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/bouvard2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:32,223 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741851_1027, replicas=127.0.0.1:50010 for /user/ABU/bovary3.txt._COPYING_
2018-10-01 15:42:32,346 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/bovary3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:32,532 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741852_1028, replicas=127.0.0.1:50010 for /user/ABU/bretagne1.txt._COPYING_
2018-10-01 15:42:32,683 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/bretagne1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:32,860 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741853_1029, replicas=127.0.0.1:50010 for /user/ABU/britannicus1.txt._COPYING_
2018-10-01 15:42:32,936 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/britannicus1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:32,959 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741854_1030, replicas=127.0.0.1:50010 for /user/ABU/bugjarg1.txt._COPYING_
2018-10-01 15:42:32,981 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/bugjarg1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:33,003 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741855_1031, replicas=127.0.0.1:50010 for /user/ABU/candide3.txt._COPYING_
2018-10-01 15:42:33,023 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/candide3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:33,075 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741856_1032, replicas=127.0.0.1:50010 for /user/ABU/caribou1.txt._COPYING_
2018-10-01 15:42:33,095 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/caribou1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:33,118 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741857_1033, replicas=127.0.0.1:50010 for /user/ABU/centurion1.txt._COPYING_
2018-10-01 15:42:33,139 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/centurion1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:33,160 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741858_1034, replicas=127.0.0.1:50010 for /user/ABU/chabert3.txt._COPYING_
2018-10-01 15:42:33,177 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/chabert3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:33,196 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741859_1035, replicas=127.0.0.1:50010 for /user/ABU/chambre2.txt._COPYING_
2018-10-01 15:42:33,213 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/chambre2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:33,236 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741860_1036, replicas=127.0.0.1:50010 for /user/ABU/champi1.txt._COPYING_
2018-10-01 15:42:33,257 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/champi1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:33,282 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741861_1037, replicas=127.0.0.1:50010 for /user/ABU/chartre1.txt._COPYING_
2018-10-01 15:42:33,312 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/chartre1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:33,337 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741862_1038, replicas=127.0.0.1:50010 for /user/ABU/chef2.txt._COPYING_
2018-10-01 15:42:33,357 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/chef2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:33,396 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741863_1039, replicas=127.0.0.1:50010 for /user/ABU/cinna2.txt._COPYING_
2018-10-01 15:42:33,416 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/cinna2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:33,437 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741864_1040, replicas=127.0.0.1:50010 for /user/ABU/cleves2.txt._COPYING_
2018-10-01 15:42:33,460 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/cleves2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:33,486 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741865_1041, replicas=127.0.0.1:50010 for /user/ABU/cocuage2.txt._COPYING_
2018-10-01 15:42:33,505 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/cocuage2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:33,531 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741866_1042, replicas=127.0.0.1:50010 for /user/ABU/colliergriffes1.txt._COPYING_
2018-10-01 15:42:33,550 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/colliergriffes1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:33,575 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741867_1043, replicas=127.0.0.1:50010 for /user/ABU/colomba1.txt._COPYING_
2018-10-01 15:42:33,595 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/colomba1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:33,616 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741868_1044, replicas=127.0.0.1:50010 for /user/ABU/commerce1.txt._COPYING_
2018-10-01 15:42:33,636 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/commerce1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:33,662 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741869_1045, replicas=127.0.0.1:50010 for /user/ABU/confessions1.txt._COPYING_
2018-10-01 15:42:33,695 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/confessions1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:33,724 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741870_1046, replicas=127.0.0.1:50010 for /user/ABU/conscrit2.txt._COPYING_
2018-10-01 15:42:33,746 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/conscrit2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:33,767 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741871_1047, replicas=127.0.0.1:50010 for /user/ABU/consider1.txt._COPYING_
2018-10-01 15:42:33,787 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/consider1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:33,980 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741872_1048, replicas=127.0.0.1:50010 for /user/ABU/contemplA2.txt._COPYING_
2018-10-01 15:42:34,000 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/contemplA2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:34,022 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741873_1049, replicas=127.0.0.1:50010 for /user/ABU/contemplB2.txt._COPYING_
2018-10-01 15:42:34,044 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/contemplB2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:34,068 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741874_1050, replicas=127.0.0.1:50010 for /user/ABU/contrat1.txt._COPYING_
2018-10-01 15:42:34,088 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/contrat1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:34,113 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741875_1051, replicas=127.0.0.1:50010 for /user/ABU/crainque1.txt._COPYING_
2018-10-01 15:42:34,133 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/crainque1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:34,154 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741876_1052, replicas=127.0.0.1:50010 for /user/ABU/curee2.txt._COPYING_
2018-10-01 15:42:34,178 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/curee2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:34,216 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741877_1053, replicas=127.0.0.1:50010 for /user/ABU/cyrano1.txt._COPYING_
2018-10-01 15:42:34,235 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/cyrano1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:34,260 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741878_1054, replicas=127.0.0.1:50010 for /user/ABU/daphnis1.txt._COPYING_
2018-10-01 15:42:34,275 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/daphnis1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:34,293 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741879_1055, replicas=127.0.0.1:50010 for /user/ABU/ddhc3.txt._COPYING_
2018-10-01 15:42:34,305 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ddhc3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:34,325 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741880_1056, replicas=127.0.0.1:50010 for /user/ABU/desespere1.txt._COPYING_
2018-10-01 15:42:34,348 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/desespere1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:34,369 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741881_1057, replicas=127.0.0.1:50010 for /user/ABU/diableam3.txt._COPYING_
2018-10-01 15:42:34,385 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/diableam3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:34,405 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741882_1058, replicas=127.0.0.1:50010 for /user/ABU/diablecor2.txt._COPYING_
2018-10-01 15:42:34,424 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/diablecor2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:34,448 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741883_1059, replicas=127.0.0.1:50010 for /user/ABU/dicohisto1.txt._COPYING_
2018-10-01 15:42:34,469 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/dicohisto1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:34,493 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741884_1060, replicas=127.0.0.1:50010 for /user/ABU/dicotypo1.txt._COPYING_
2018-10-01 15:42:34,509 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741884_1060 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/dicotypo1.txt._COPYING_
2018-10-01 15:42:34,911 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/dicotypo1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:34,945 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741885_1061, replicas=127.0.0.1:50010 for /user/ABU/domi3.txt._COPYING_
2018-10-01 15:42:34,962 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741885_1061 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/domi3.txt._COPYING_
2018-10-01 15:42:35,364 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/domi3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:35,466 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741886_1062, replicas=127.0.0.1:50010 for /user/ABU/domjuan2.txt._COPYING_
2018-10-01 15:42:35,485 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/domjuan2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:35,505 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741887_1063, replicas=127.0.0.1:50010 for /user/ABU/drageoir2.txt._COPYING_
2018-10-01 15:42:35,524 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/drageoir2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:35,544 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741888_1064, replicas=127.0.0.1:50010 for /user/ABU/drole2.txt._COPYING_
2018-10-01 15:42:35,560 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/drole2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:35,582 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741889_1065, replicas=127.0.0.1:50010 for /user/ABU/ecole1.txt._COPYING_
2018-10-01 15:42:35,599 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741889_1065 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/ecole1.txt._COPYING_
2018-10-01 15:42:36,001 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ecole1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:36,106 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741890_1066, replicas=127.0.0.1:50010 for /user/ABU/educati1.txt._COPYING_
2018-10-01 15:42:36,131 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741890_1066 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/educati1.txt._COPYING_
2018-10-01 15:42:36,533 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/educati1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:36,612 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741891_1067, replicas=127.0.0.1:50010 for /user/ABU/elixir2.txt._COPYING_
2018-10-01 15:42:36,633 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/elixir2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:36,653 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741892_1068, replicas=127.0.0.1:50010 for /user/ABU/erenouv1.txt._COPYING_
2018-10-01 15:42:36,671 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/erenouv1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:36,694 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741893_1069, replicas=127.0.0.1:50010 for /user/ABU/espece1.txt._COPYING_
2018-10-01 15:42:36,721 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/espece1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:36,762 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741894_1070, replicas=127.0.0.1:50010 for /user/ABU/etuit1.txt._COPYING_
2018-10-01 15:42:36,782 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/etuit1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:36,803 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741895_1071, replicas=127.0.0.1:50010 for /user/ABU/excentlang1.txt._COPYING_
2018-10-01 15:42:36,824 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/excentlang1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:36,824 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 428 Total time for transactions(ms): 88 Number of transactions batched in Syncs: 72 Number of syncs: 356 SyncTimes(ms): 4963 
2018-10-01 15:42:36,848 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741896_1072, replicas=127.0.0.1:50010 for /user/ABU/fabulistes1.txt._COPYING_
2018-10-01 15:42:36,864 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/fabulistes1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:36,884 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741897_1073, replicas=127.0.0.1:50010 for /user/ABU/fadette1.txt._COPYING_
2018-10-01 15:42:36,899 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/fadette1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:36,929 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741898_1074, replicas=127.0.0.1:50010 for /user/ABU/farcefran1.txt._COPYING_
2018-10-01 15:42:36,948 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741898_1074 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/farcefran1.txt._COPYING_
2018-10-01 15:42:37,350 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/farcefran1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:37,414 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741899_1075, replicas=127.0.0.1:50010 for /user/ABU/femtin1.txt._COPYING_
2018-10-01 15:42:37,443 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741899_1075 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/femtin1.txt._COPYING_
2018-10-01 15:42:37,845 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/femtin1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:37,871 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741900_1076, replicas=127.0.0.1:50010 for /user/ABU/feuilles1.txt._COPYING_
2018-10-01 15:42:37,887 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741900_1076 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/feuilles1.txt._COPYING_
2018-10-01 15:42:38,289 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/feuilles1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:38,453 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741901_1077, replicas=127.0.0.1:50010 for /user/ABU/figchose1.txt._COPYING_
2018-10-01 15:42:38,471 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741901_1077 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/figchose1.txt._COPYING_
2018-10-01 15:42:38,873 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/figchose1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:38,906 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741902_1078, replicas=127.0.0.1:50010 for /user/ABU/flandeux1.txt._COPYING_
2018-10-01 15:42:38,920 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741902_1078 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/flandeux1.txt._COPYING_
2018-10-01 15:42:39,322 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/flandeux1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:39,360 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741903_1079, replicas=127.0.0.1:50010 for /user/ABU/football1.txt._COPYING_
2018-10-01 15:42:39,379 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/football1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:39,395 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741904_1080, replicas=127.0.0.1:50010 for /user/ABU/gargantua2.txt._COPYING_
2018-10-01 15:42:39,407 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/gargantua2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:39,424 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741905_1081, replicas=127.0.0.1:50010 for /user/ABU/gaspard2.txt._COPYING_
2018-10-01 15:42:39,436 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/gaspard2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:39,454 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741906_1082, replicas=127.0.0.1:50010 for /user/ABU/germinal1.txt._COPYING_
2018-10-01 15:42:39,471 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/germinal1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:39,683 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741907_1083, replicas=127.0.0.1:50010 for /user/ABU/germinie3.txt._COPYING_
2018-10-01 15:42:39,707 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/germinie3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:39,729 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741908_1084, replicas=127.0.0.1:50010 for /user/ABU/gilblas1.txt._COPYING_
2018-10-01 15:42:39,752 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/gilblas1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:39,769 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741909_1085, replicas=127.0.0.1:50010 for /user/ABU/guizeur1.txt._COPYING_
2018-10-01 15:42:39,787 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/guizeur1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:39,805 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741910_1086, replicas=127.0.0.1:50010 for /user/ABU/hamlet1.txt._COPYING_
2018-10-01 15:42:39,831 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741910_1086 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/hamlet1.txt._COPYING_
2018-10-01 15:42:40,233 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/hamlet1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:40,265 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741911_1087, replicas=127.0.0.1:50010 for /user/ABU/harmonie1.txt._COPYING_
2018-10-01 15:42:40,283 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741911_1087 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/harmonie1.txt._COPYING_
2018-10-01 15:42:40,686 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/harmonie1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:40,709 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741912_1088, replicas=127.0.0.1:50010 for /user/ABU/historiettes2.txt._COPYING_
2018-10-01 15:42:40,726 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/historiettes2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:40,745 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741913_1089, replicas=127.0.0.1:50010 for /user/ABU/homecu1.txt._COPYING_
2018-10-01 15:42:40,761 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741913_1089 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/homecu1.txt._COPYING_
2018-10-01 15:42:41,163 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/homecu1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:41,208 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741914_1090, replicas=127.0.0.1:50010 for /user/ABU/horla3.txt._COPYING_
2018-10-01 15:42:41,228 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/horla3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:41,246 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741915_1091, replicas=127.0.0.1:50010 for /user/ABU/hugoshak1.txt._COPYING_
2018-10-01 15:42:41,264 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/hugoshak1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:41,294 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741916_1092, replicas=127.0.0.1:50010 for /user/ABU/humilite3.txt._COPYING_
2018-10-01 15:42:41,313 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741916_1092 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/humilite3.txt._COPYING_
2018-10-01 15:42:41,715 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/humilite3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:41,750 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741917_1093, replicas=127.0.0.1:50010 for /user/ABU/ideolo1.txt._COPYING_
2018-10-01 15:42:41,768 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ideolo1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:41,786 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741918_1094, replicas=127.0.0.1:50010 for /user/ABU/illusion1.txt._COPYING_
2018-10-01 15:42:41,797 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/illusion1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:41,815 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741919_1095, replicas=127.0.0.1:50010 for /user/ABU/inquisit2.txt._COPYING_
2018-10-01 15:42:41,826 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741919_1095 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/inquisit2.txt._COPYING_
2018-10-01 15:42:42,228 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/inquisit2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:42,251 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741920_1096, replicas=127.0.0.1:50010 for /user/ABU/interpret2.txt._COPYING_
2018-10-01 15:42:42,259 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741920_1096 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/interpret2.txt._COPYING_
2018-10-01 15:42:42,661 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/interpret2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:42,685 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741921_1097, replicas=127.0.0.1:50010 for /user/ABU/iphigenie1.txt._COPYING_
2018-10-01 15:42:42,692 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741921_1097 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/iphigenie1.txt._COPYING_
2018-10-01 15:42:43,094 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/iphigenie1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:43,130 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741922_1098, replicas=127.0.0.1:50010 for /user/ABU/irnois2.txt._COPYING_
2018-10-01 15:42:43,142 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/irnois2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:43,154 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741923_1099, replicas=127.0.0.1:50010 for /user/ABU/jaccuse3.txt._COPYING_
2018-10-01 15:42:43,161 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/jaccuse3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:43,178 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741924_1100, replicas=127.0.0.1:50010 for /user/ABU/jacques1.txt._COPYING_
2018-10-01 15:42:43,191 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/jacques1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:43,213 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741925_1101, replicas=127.0.0.1:50010 for /user/ABU/journalism1.txt._COPYING_
2018-10-01 15:42:43,235 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/journalism1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:43,256 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741926_1102, replicas=127.0.0.1:50010 for /user/ABU/journbloy1.txt._COPYING_
2018-10-01 15:42:43,276 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741926_1102 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/journbloy1.txt._COPYING_
2018-10-01 15:42:43,677 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/journbloy1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:43,703 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741927_1103, replicas=127.0.0.1:50010 for /user/ABU/justice1.txt._COPYING_
2018-10-01 15:42:43,718 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741927_1103 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/justice1.txt._COPYING_
2018-10-01 15:42:44,119 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/justice1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:44,146 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741928_1104, replicas=127.0.0.1:50010 for /user/ABU/lafille2.txt._COPYING_
2018-10-01 15:42:44,160 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741928_1104 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/lafille2.txt._COPYING_
2018-10-01 15:42:44,561 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lafille2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:44,606 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741929_1105, replicas=127.0.0.1:50010 for /user/ABU/lecid1.txt._COPYING_
2018-10-01 15:42:44,624 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741929_1105 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/lecid1.txt._COPYING_
2018-10-01 15:42:45,026 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lecid1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:45,061 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741930_1106, replicas=127.0.0.1:50010 for /user/ABU/legend1.txt._COPYING_
2018-10-01 15:42:45,076 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741930_1106 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/legend1.txt._COPYING_
2018-10-01 15:42:45,478 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/legend1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:45,515 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741931_1107, replicas=127.0.0.1:50010 for /user/ABU/legendet21.txt._COPYING_
2018-10-01 15:42:45,527 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741931_1107 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/legendet21.txt._COPYING_
2018-10-01 15:42:45,929 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/legendet21.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:45,957 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741932_1108, replicas=127.0.0.1:50010 for /user/ABU/leibnitzdiv1.txt._COPYING_
2018-10-01 15:42:45,974 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741932_1108 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/leibnitzdiv1.txt._COPYING_
2018-10-01 15:42:46,376 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/leibnitzdiv1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:46,403 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741933_1109, replicas=127.0.0.1:50010 for /user/ABU/lejeu1.txt._COPYING_
2018-10-01 15:42:46,418 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741933_1109 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/lejeu1.txt._COPYING_
2018-10-01 15:42:46,820 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lejeu1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:46,851 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741934_1110, replicas=127.0.0.1:50010 for /user/ABU/lepetitchose1.txt._COPYING_
2018-10-01 15:42:46,872 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741934_1110 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/lepetitchose1.txt._COPYING_
2018-10-01 15:42:47,274 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lepetitchose1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:47,304 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741935_1111, replicas=127.0.0.1:50010 for /user/ABU/lessoirees1.txt._COPYING_
2018-10-01 15:42:47,330 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lessoirees1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:47,368 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741936_1112, replicas=127.0.0.1:50010 for /user/ABU/letphi1.txt._COPYING_
2018-10-01 15:42:47,560 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/letphi1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:47,577 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741937_1113, replicas=127.0.0.1:50010 for /user/ABU/lettresecrites1.txt._COPYING_
2018-10-01 15:42:47,598 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lettresecrites1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:47,620 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741938_1114, replicas=127.0.0.1:50010 for /user/ABU/lettresjuives11.txt._COPYING_
2018-10-01 15:42:47,643 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lettresjuives11.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:47,665 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741939_1115, replicas=127.0.0.1:50010 for /user/ABU/lettresjuives231.txt._COPYING_
2018-10-01 15:42:47,687 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741939_1115 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/lettresjuives231.txt._COPYING_
2018-10-01 15:42:48,089 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lettresjuives231.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:48,125 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741940_1116, replicas=127.0.0.1:50010 for /user/ABU/lettresjuives451.txt._COPYING_
2018-10-01 15:42:48,168 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741940_1116 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/lettresjuives451.txt._COPYING_
2018-10-01 15:42:48,570 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lettresjuives451.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:48,607 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741941_1117, replicas=127.0.0.1:50010 for /user/ABU/lettresjuives6781.txt._COPYING_
2018-10-01 15:42:48,629 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741941_1117 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/lettresjuives6781.txt._COPYING_
2018-10-01 15:42:49,032 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lettresjuives6781.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:49,061 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741942_1118, replicas=127.0.0.1:50010 for /user/ABU/lettresreli2.txt._COPYING_
2018-10-01 15:42:49,074 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741942_1118 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/lettresreli2.txt._COPYING_
2018-10-01 15:42:49,476 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lettresreli2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:49,511 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741943_1119, replicas=127.0.0.1:50010 for /user/ABU/levieux2.txt._COPYING_
2018-10-01 15:42:49,528 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741943_1119 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/levieux2.txt._COPYING_
2018-10-01 15:42:49,930 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/levieux2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:49,972 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741944_1120, replicas=127.0.0.1:50010 for /user/ABU/liaisons3.txt._COPYING_
2018-10-01 15:42:49,993 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741944_1120 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/liaisons3.txt._COPYING_
2018-10-01 15:42:50,394 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/liaisons3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:50,435 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741945_1121, replicas=127.0.0.1:50010 for /user/ABU/licong1.txt._COPYING_
2018-10-01 15:42:50,454 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741945_1121 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/licong1.txt._COPYING_
2018-10-01 15:42:50,856 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/licong1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:50,892 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741946_1122, replicas=127.0.0.1:50010 for /user/ABU/lmain3.txt._COPYING_
2018-10-01 15:42:50,904 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lmain3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:50,918 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741947_1123, replicas=127.0.0.1:50010 for /user/ABU/lutrin1.txt._COPYING_
2018-10-01 15:42:50,925 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741947_1123 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/lutrin1.txt._COPYING_
2018-10-01 15:42:51,326 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lutrin1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:51,346 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741948_1124, replicas=127.0.0.1:50010 for /user/ABU/m702douai2.txt._COPYING_
2018-10-01 15:42:51,353 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741948_1124 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/m702douai2.txt._COPYING_
2018-10-01 15:42:51,755 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/m702douai2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:51,789 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741949_1125, replicas=127.0.0.1:50010 for /user/ABU/machine3.txt._COPYING_
2018-10-01 15:42:51,796 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741949_1125 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/machine3.txt._COPYING_
2018-10-01 15:42:52,197 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/machine3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:52,233 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741950_1126, replicas=127.0.0.1:50010 for /user/ABU/manifeste2.txt._COPYING_
2018-10-01 15:42:52,244 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741950_1126 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/manifeste2.txt._COPYING_
2018-10-01 15:42:52,646 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/manifeste2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:52,673 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741951_1127, replicas=127.0.0.1:50010 for /user/ABU/manon2.txt._COPYING_
2018-10-01 15:42:52,692 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741951_1127 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/manon2.txt._COPYING_
2018-10-01 15:42:53,094 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/manon2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:53,129 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741952_1128, replicas=127.0.0.1:50010 for /user/ABU/marie1.txt._COPYING_
2018-10-01 15:42:53,142 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741952_1128 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/marie1.txt._COPYING_
2018-10-01 15:42:53,544 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/marie1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:53,580 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741953_1129, replicas=127.0.0.1:50010 for /user/ABU/maximes2.txt._COPYING_
2018-10-01 15:42:53,595 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741953_1129 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/maximes2.txt._COPYING_
2018-10-01 15:42:53,997 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/maximes2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:54,035 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741954_1130, replicas=127.0.0.1:50010 for /user/ABU/medipoet1.txt._COPYING_
2018-10-01 15:42:54,050 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741954_1130 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/medipoet1.txt._COPYING_
2018-10-01 15:42:54,452 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/medipoet1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:54,485 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741955_1131, replicas=127.0.0.1:50010 for /user/ABU/medit3.txt._COPYING_
2018-10-01 15:42:54,504 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741955_1131 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/medit3.txt._COPYING_
2018-10-01 15:42:54,906 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/medit3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:54,939 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741956_1132, replicas=127.0.0.1:50010 for /user/ABU/medita1.txt._COPYING_
2018-10-01 15:42:54,958 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741956_1132 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/medita1.txt._COPYING_
2018-10-01 15:42:55,360 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/medita1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:55,391 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741957_1133, replicas=127.0.0.1:50010 for /user/ABU/methode3.txt._COPYING_
2018-10-01 15:42:55,409 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741957_1133 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/methode3.txt._COPYING_
2018-10-01 15:42:55,811 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/methode3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:55,843 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741958_1134, replicas=127.0.0.1:50010 for /user/ABU/micromeg3.txt._COPYING_
2018-10-01 15:42:55,860 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741958_1134 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/micromeg3.txt._COPYING_
2018-10-01 15:42:56,263 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/micromeg3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:56,296 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741959_1135, replicas=127.0.0.1:50010 for /user/ABU/monadologie1.txt._COPYING_
2018-10-01 15:42:56,313 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741959_1135 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/monadologie1.txt._COPYING_
2018-10-01 15:42:56,715 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/monadologie1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:56,834 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741960_1136, replicas=127.0.0.1:50010 for /user/ABU/monde1.txt._COPYING_
2018-10-01 15:42:56,849 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741960_1136 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/monde1.txt._COPYING_
2018-10-01 15:42:57,251 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/monde1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:57,378 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741961_1137, replicas=127.0.0.1:50010 for /user/ABU/montbard1.txt._COPYING_
2018-10-01 15:42:57,396 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741961_1137 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/montbard1.txt._COPYING_
2018-10-01 15:42:57,798 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/montbard1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:57,920 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741962_1138, replicas=127.0.0.1:50010 for /user/ABU/morte2.txt._COPYING_
2018-10-01 15:42:57,938 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741962_1138 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/morte2.txt._COPYING_
2018-10-01 15:42:58,341 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/morte2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:58,539 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741963_1139, replicas=127.0.0.1:50010 for /user/ABU/mousque1.txt._COPYING_
2018-10-01 15:42:58,565 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741963_1139 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/mousque1.txt._COPYING_
2018-10-01 15:42:58,966 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/mousque1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:59,094 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741964_1140, replicas=127.0.0.1:50010 for /user/ABU/nddp1.txt._COPYING_
2018-10-01 15:42:59,166 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741964_1140 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/nddp1.txt._COPYING_
2018-10-01 15:42:59,568 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/nddp1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:59,693 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741965_1141, replicas=127.0.0.1:50010 for /user/ABU/neveu2.txt._COPYING_
2018-10-01 15:42:59,734 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/neveu2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:42:59,746 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741966_1142, replicas=127.0.0.1:50010 for /user/ABU/nouvmedi1.txt._COPYING_
2018-10-01 15:42:59,761 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741966_1142 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/nouvmedi1.txt._COPYING_
2018-10-01 15:43:00,163 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/nouvmedi1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:00,343 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741967_1143, replicas=127.0.0.1:50010 for /user/ABU/odespolit1.txt._COPYING_
2018-10-01 15:43:00,362 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741967_1143 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/odespolit1.txt._COPYING_
2018-10-01 15:43:00,764 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/odespolit1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:00,888 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741968_1144, replicas=127.0.0.1:50010 for /user/ABU/opinions3.txt._COPYING_
2018-10-01 15:43:00,904 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741968_1144 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/opinions3.txt._COPYING_
2018-10-01 15:43:01,306 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/opinions3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:01,435 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741969_1145, replicas=127.0.0.1:50010 for /user/ABU/oriental1.txt._COPYING_
2018-10-01 15:43:01,454 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741969_1145 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/oriental1.txt._COPYING_
2018-10-01 15:43:01,856 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/oriental1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:01,985 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741970_1146, replicas=127.0.0.1:50010 for /user/ABU/paresse3.txt._COPYING_
2018-10-01 15:43:02,004 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741970_1146 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/paresse3.txt._COPYING_
2018-10-01 15:43:02,406 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/paresse3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:02,425 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741971_1147, replicas=127.0.0.1:50010 for /user/ABU/partiecamp2.txt._COPYING_
2018-10-01 15:43:02,439 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741971_1147 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/partiecamp2.txt._COPYING_
2018-10-01 15:43:02,841 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/partiecamp2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:02,855 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741972_1148, replicas=127.0.0.1:50010 for /user/ABU/pascaldiv1.txt._COPYING_
2018-10-01 15:43:02,870 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/pascaldiv1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:02,894 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741973_1149, replicas=127.0.0.1:50010 for /user/ABU/pascalpetits1.txt._COPYING_
2018-10-01 15:43:02,913 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/pascalpetits1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:02,942 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741974_1150, replicas=127.0.0.1:50010 for /user/ABU/penseesXX1.txt._COPYING_
2018-10-01 15:43:02,961 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741974_1150 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/penseesXX1.txt._COPYING_
2018-10-01 15:43:03,363 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/penseesXX1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:03,386 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741975_1151, replicas=127.0.0.1:50010 for /user/ABU/pensepict1.txt._COPYING_
2018-10-01 15:43:03,408 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741975_1151 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/pensepict1.txt._COPYING_
2018-10-01 15:43:03,810 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/pensepict1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:03,837 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741976_1152, replicas=127.0.0.1:50010 for /user/ABU/phedre2.txt._COPYING_
2018-10-01 15:43:03,862 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741976_1152 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/phedre2.txt._COPYING_
2018-10-01 15:43:04,263 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/phedre2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:04,281 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741977_1153, replicas=127.0.0.1:50010 for /user/ABU/plural3.txt._COPYING_
2018-10-01 15:43:04,302 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741977_1153 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/plural3.txt._COPYING_
2018-10-01 15:43:04,704 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/plural3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:04,727 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741978_1154, replicas=127.0.0.1:50010 for /user/ABU/poemallar1.txt._COPYING_
2018-10-01 15:43:04,744 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741978_1154 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/poemallar1.txt._COPYING_
2018-10-01 15:43:05,146 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/poemallar1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:05,163 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741979_1155, replicas=127.0.0.1:50010 for /user/ABU/poemesadv1.txt._COPYING_
2018-10-01 15:43:05,187 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741979_1155 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/poemesadv1.txt._COPYING_
2018-10-01 15:43:05,588 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/poemesadv1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:05,611 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741980_1156, replicas=127.0.0.1:50010 for /user/ABU/presseguerre1.txt._COPYING_
2018-10-01 15:43:05,629 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741980_1156 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/presseguerre1.txt._COPYING_
2018-10-01 15:43:06,031 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/presseguerre1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:06,052 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741981_1157, replicas=127.0.0.1:50010 for /user/ABU/preuves1.txt._COPYING_
2018-10-01 15:43:06,073 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741981_1157 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/preuves1.txt._COPYING_
2018-10-01 15:43:06,475 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/preuves1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:06,496 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741982_1158, replicas=127.0.0.1:50010 for /user/ABU/propriete1.txt._COPYING_
2018-10-01 15:43:06,513 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741982_1158 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/propriete1.txt._COPYING_
2018-10-01 15:43:06,914 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/propriete1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:06,979 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741983_1159, replicas=127.0.0.1:50010 for /user/ABU/pucelle1.txt._COPYING_
2018-10-01 15:43:06,998 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741983_1159 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/pucelle1.txt._COPYING_
2018-10-01 15:43:07,400 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/pucelle1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:07,418 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741984_1160, replicas=127.0.0.1:50010 for /user/ABU/quatrevt1.txt._COPYING_
2018-10-01 15:43:07,440 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741984_1160 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/quatrevt1.txt._COPYING_
2018-10-01 15:43:07,842 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/quatrevt1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:07,865 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741985_1161, replicas=127.0.0.1:50010 for /user/ABU/raison2.txt._COPYING_
2018-10-01 15:43:07,886 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741985_1161 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/raison2.txt._COPYING_
2018-10-01 15:43:08,288 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/raison2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:08,311 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741986_1162, replicas=127.0.0.1:50010 for /user/ABU/rayons1.txt._COPYING_
2018-10-01 15:43:08,331 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741986_1162 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/rayons1.txt._COPYING_
2018-10-01 15:43:08,734 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/rayons1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:08,753 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741987_1163, replicas=127.0.0.1:50010 for /user/ABU/ren05101.txt._COPYING_
2018-10-01 15:43:08,774 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741987_1163 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/ren05101.txt._COPYING_
2018-10-01 15:43:09,176 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ren05101.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:09,199 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741988_1164, replicas=127.0.0.1:50010 for /user/ABU/ren87922.txt._COPYING_
2018-10-01 15:43:09,218 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741988_1164 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/ren87922.txt._COPYING_
2018-10-01 15:43:09,620 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ren87922.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:09,643 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741989_1165, replicas=127.0.0.1:50010 for /user/ABU/ren93982.txt._COPYING_
2018-10-01 15:43:09,670 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741989_1165 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/ren93982.txt._COPYING_
2018-10-01 15:43:10,071 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ren93982.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:10,091 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741990_1166, replicas=127.0.0.1:50010 for /user/ABU/ren99041.txt._COPYING_
2018-10-01 15:43:10,114 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741990_1166 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/ren99041.txt._COPYING_
2018-10-01 15:43:10,516 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ren99041.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:10,537 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741991_1167, replicas=127.0.0.1:50010 for /user/ABU/reveries3.txt._COPYING_
2018-10-01 15:43:10,557 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741991_1167 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/reveries3.txt._COPYING_
2018-10-01 15:43:10,958 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/reveries3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:10,980 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741992_1168, replicas=127.0.0.1:50010 for /user/ABU/rldased3.txt._COPYING_
2018-10-01 15:43:11,004 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741992_1168 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/rldased3.txt._COPYING_
2018-10-01 15:43:11,406 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/rldased3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:11,429 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741993_1169, replicas=127.0.0.1:50010 for /user/ABU/robur1.txt._COPYING_
2018-10-01 15:43:11,453 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741993_1169 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/robur1.txt._COPYING_
2018-10-01 15:43:11,855 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/robur1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:11,986 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741994_1170, replicas=127.0.0.1:50010 for /user/ABU/roland2.txt._COPYING_
2018-10-01 15:43:12,015 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741994_1170 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/roland2.txt._COPYING_
2018-10-01 15:43:12,417 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/roland2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:12,444 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741995_1171, replicas=127.0.0.1:50010 for /user/ABU/roletrav2.txt._COPYING_
2018-10-01 15:43:12,462 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741995_1171 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/roletrav2.txt._COPYING_
2018-10-01 15:43:12,864 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/roletrav2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:12,885 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741996_1172, replicas=127.0.0.1:50010 for /user/ABU/rouge1.txt._COPYING_
2018-10-01 15:43:12,905 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741996_1172 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/rouge1.txt._COPYING_
2018-10-01 15:43:13,307 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/rouge1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:13,329 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741997_1173, replicas=127.0.0.1:50010 for /user/ABU/rousgene1.txt._COPYING_
2018-10-01 15:43:13,350 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741997_1173 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/rousgene1.txt._COPYING_
2018-10-01 15:43:13,753 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/rousgene1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:13,774 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741998_1174, replicas=127.0.0.1:50010 for /user/ABU/ruesboi1.txt._COPYING_
2018-10-01 15:43:13,797 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741998_1174 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/ruesboi1.txt._COPYING_
2018-10-01 15:43:14,199 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ruesboi1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:14,221 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741999_1175, replicas=127.0.0.1:50010 for /user/ABU/salalion1.txt._COPYING_
2018-10-01 15:43:14,416 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741999_1175 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/salalion1.txt._COPYING_
2018-10-01 15:43:14,817 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/salalion1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:14,837 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742000_1176, replicas=127.0.0.1:50010 for /user/ABU/salammb1.txt._COPYING_
2018-10-01 15:43:14,857 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742000_1176 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/salammb1.txt._COPYING_
2018-10-01 15:43:15,259 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/salammb1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:15,281 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742001_1177, replicas=127.0.0.1:50010 for /user/ABU/satan1.txt._COPYING_
2018-10-01 15:43:15,302 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742001_1177 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/satan1.txt._COPYING_
2018-10-01 15:43:15,704 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/satan1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:15,733 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742002_1178, replicas=127.0.0.1:50010 for /user/ABU/scapin2.txt._COPYING_
2018-10-01 15:43:15,753 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742002_1178 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/scapin2.txt._COPYING_
2018-10-01 15:43:16,154 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/scapin2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:16,176 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742003_1179, replicas=127.0.0.1:50010 for /user/ABU/scihyp2.txt._COPYING_
2018-10-01 15:43:16,199 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742003_1179 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/scihyp2.txt._COPYING_
2018-10-01 15:43:16,600 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/scihyp2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:16,664 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742004_1180, replicas=127.0.0.1:50010 for /user/ABU/septfem2.txt._COPYING_
2018-10-01 15:43:16,686 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742004_1180 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/septfem2.txt._COPYING_
2018-10-01 15:43:17,088 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/septfem2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:17,153 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742005_1181, replicas=127.0.0.1:50010 for /user/ABU/smarra1.txt._COPYING_
2018-10-01 15:43:17,175 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742005_1181 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/smarra1.txt._COPYING_
2018-10-01 15:43:17,577 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/smarra1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:17,599 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742006_1182, replicas=127.0.0.1:50010 for /user/ABU/souveni1.txt._COPYING_
2018-10-01 15:43:17,619 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742006_1182 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/souveni1.txt._COPYING_
2018-10-01 15:43:18,021 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/souveni1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:18,042 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742007_1183, replicas=127.0.0.1:50010 for /user/ABU/supplem2.txt._COPYING_
2018-10-01 15:43:18,063 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742007_1183 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/supplem2.txt._COPYING_
2018-10-01 15:43:18,465 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/supplem2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:18,486 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742008_1184, replicas=127.0.0.1:50010 for /user/ABU/tartuf2.txt._COPYING_
2018-10-01 15:43:18,514 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742008_1184 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/tartuf2.txt._COPYING_
2018-10-01 15:43:18,916 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/tartuf2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:18,945 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742009_1185, replicas=127.0.0.1:50010 for /user/ABU/tdm80j2.txt._COPYING_
2018-10-01 15:43:18,968 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742009_1185 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/tdm80j2.txt._COPYING_
2018-10-01 15:43:19,370 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/tdm80j2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:19,391 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742010_1186, replicas=127.0.0.1:50010 for /user/ABU/theodicee1.txt._COPYING_
2018-10-01 15:43:19,412 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742010_1186 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/theodicee1.txt._COPYING_
2018-10-01 15:43:19,814 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/theodicee1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:19,835 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742011_1187, replicas=127.0.0.1:50010 for /user/ABU/theoriephys1.txt._COPYING_
2018-10-01 15:43:19,858 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742011_1187 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/theoriephys1.txt._COPYING_
2018-10-01 15:43:20,259 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/theoriephys1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:20,280 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742012_1188, replicas=127.0.0.1:50010 for /user/ABU/thotri1.txt._COPYING_
2018-10-01 15:43:20,300 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742012_1188 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/thotri1.txt._COPYING_
2018-10-01 15:43:20,702 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/thotri1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:20,730 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742013_1189, replicas=127.0.0.1:50010 for /user/ABU/tlun3.txt._COPYING_
2018-10-01 15:43:20,749 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742013_1189 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/tlun3.txt._COPYING_
2018-10-01 15:43:21,151 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/tlun3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:21,172 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742014_1190, replicas=127.0.0.1:50010 for /user/ABU/tome1malherbe1.txt._COPYING_
2018-10-01 15:43:21,206 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742014_1190 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/tome1malherbe1.txt._COPYING_
2018-10-01 15:43:21,608 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/tome1malherbe1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:21,629 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742015_1191, replicas=127.0.0.1:50010 for /user/ABU/troylion1.txt._COPYING_
2018-10-01 15:43:21,649 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742015_1191 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/troylion1.txt._COPYING_
2018-10-01 15:43:22,051 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/troylion1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:22,129 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742016_1192, replicas=127.0.0.1:50010 for /user/ABU/ulenspiegel1.txt._COPYING_
2018-10-01 15:43:22,156 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742016_1192 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/ulenspiegel1.txt._COPYING_
2018-10-01 15:43:22,558 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ulenspiegel1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:22,578 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742017_1193, replicas=127.0.0.1:50010 for /user/ABU/uncoeur3.txt._COPYING_
2018-10-01 15:43:22,597 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742017_1193 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/uncoeur3.txt._COPYING_
2018-10-01 15:43:22,999 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/uncoeur3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:23,017 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742018_1194, replicas=127.0.0.1:50010 for /user/ABU/unevie2.txt._COPYING_
2018-10-01 15:43:23,040 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742018_1194 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/unevie2.txt._COPYING_
2018-10-01 15:43:23,442 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/unevie2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:23,464 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742019_1195, replicas=127.0.0.1:50010 for /user/ABU/utopique2.txt._COPYING_
2018-10-01 15:43:23,482 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742019_1195 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/utopique2.txt._COPYING_
2018-10-01 15:43:23,884 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/utopique2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:23,906 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742020_1196, replicas=127.0.0.1:50010 for /user/ABU/verhafin1.txt._COPYING_
2018-10-01 15:43:23,924 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742020_1196 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/verhafin1.txt._COPYING_
2018-10-01 15:43:24,326 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/verhafin1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:24,345 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742021_1197, replicas=127.0.0.1:50010 for /user/ABU/viemars3.txt._COPYING_
2018-10-01 15:43:24,363 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742021_1197 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/viemars3.txt._COPYING_
2018-10-01 15:43:24,765 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/viemars3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:24,791 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742022_1198, replicas=127.0.0.1:50010 for /user/ABU/vignypoesie1.txt._COPYING_
2018-10-01 15:43:24,814 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742022_1198 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/vignypoesie1.txt._COPYING_
2018-10-01 15:43:25,216 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/vignypoesie1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:25,234 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742023_1199, replicas=127.0.0.1:50010 for /user/ABU/volpofin1.txt._COPYING_
2018-10-01 15:43:25,262 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742023_1199 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/volpofin1.txt._COPYING_
2018-10-01 15:43:25,663 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/volpofin1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:25,683 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742024_1200, replicas=127.0.0.1:50010 for /user/ABU/voltgene1.txt._COPYING_
2018-10-01 15:43:25,698 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742024_1200 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/voltgene1.txt._COPYING_
2018-10-01 15:43:26,099 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/voltgene1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:26,911 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742025_1201, replicas=127.0.0.1:50010 for /user/ABU/voyfran1.txt._COPYING_
2018-10-01 15:43:27,001 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742025_1201 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/voyfran1.txt._COPYING_
2018-10-01 15:43:27,403 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/voyfran1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 15:43:27,753 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742026_1202, replicas=127.0.0.1:50010 for /user/ABU/voylun3.txt._COPYING_
2018-10-01 15:43:27,795 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742026_1202 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/voylun3.txt._COPYING_
2018-10-01 15:43:28,197 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/voylun3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_324416373_1
2018-10-01 16:09:26,496 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1218 Total time for transactions(ms): 133 Number of transactions batched in Syncs: 204 Number of syncs: 1012 SyncTimes(ms): 11137 
2018-10-01 16:09:52,382 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742027_1203, replicas=127.0.0.1:50010 for /user/output/_temporary/0/_temporary/attempt_local22797520_0001_r_000000_0/part-r-00000
2018-10-01 16:09:56,567 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output/_temporary/0/_temporary/attempt_local22797520_0001_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_572760056_1
2018-10-01 16:09:56,654 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_572760056_1
2018-10-01 16:13:28,401 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1233 Total time for transactions(ms): 134 Number of transactions batched in Syncs: 208 Number of syncs: 1023 SyncTimes(ms): 11689 
2018-10-01 16:13:55,256 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output2/_temporary/0/_temporary/attempt_local1898735794_0001_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_-1488792394_1
2018-10-01 16:13:56,369 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output2/_temporary/0/_temporary/attempt_local1898735794_0001_r_000001_0/part-r-00001 is closed by DFSClient_NONMAPREDUCE_-1488792394_1
2018-10-01 16:13:57,416 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output2/_temporary/0/_temporary/attempt_local1898735794_0001_r_000002_0/part-r-00002 is closed by DFSClient_NONMAPREDUCE_-1488792394_1
2018-10-01 16:13:57,464 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output2/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1488792394_1
2018-10-01 16:18:19,919 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1253 Total time for transactions(ms): 134 Number of transactions batched in Syncs: 213 Number of syncs: 1040 SyncTimes(ms): 11764 
2018-10-01 16:18:48,909 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742028_1204, replicas=127.0.0.1:50010 for /user/output3/_temporary/0/_temporary/attempt_local1484406025_0001_r_000000_0/part-r-00000
2018-10-01 16:18:49,091 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output3/_temporary/0/_temporary/attempt_local1484406025_0001_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_-1470193032_1
2018-10-01 16:18:51,192 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742029_1205, replicas=127.0.0.1:50010 for /user/output3/_temporary/0/_temporary/attempt_local1484406025_0001_r_000001_0/part-r-00001
2018-10-01 16:18:51,306 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742029_1205 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/output3/_temporary/0/_temporary/attempt_local1484406025_0001_r_000001_0/part-r-00001
2018-10-01 16:18:51,708 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output3/_temporary/0/_temporary/attempt_local1484406025_0001_r_000001_0/part-r-00001 is closed by DFSClient_NONMAPREDUCE_-1470193032_1
2018-10-01 16:18:53,851 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742030_1206, replicas=127.0.0.1:50010 for /user/output3/_temporary/0/_temporary/attempt_local1484406025_0001_r_000002_0/part-r-00002
2018-10-01 16:18:53,959 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742030_1206 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/output3/_temporary/0/_temporary/attempt_local1484406025_0001_r_000002_0/part-r-00002
2018-10-01 16:18:54,360 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output3/_temporary/0/_temporary/attempt_local1484406025_0001_r_000002_0/part-r-00002 is closed by DFSClient_NONMAPREDUCE_-1470193032_1
2018-10-01 16:18:54,439 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output3/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1470193032_1
2018-10-01 16:24:46,542 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-10-01 16:24:46,542 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-10-01 16:24:46,542 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1, 1283
2018-10-01 16:24:46,543 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1284 Total time for transactions(ms): 135 Number of transactions batched in Syncs: 219 Number of syncs: 1065 SyncTimes(ms): 11922 
2018-10-01 16:24:46,582 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1284 Total time for transactions(ms): 135 Number of transactions batched in Syncs: 219 Number of syncs: 1066 SyncTimes(ms): 11961 
2018-10-01 16:24:46,584 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000001-0000000000000001284
2018-10-01 16:24:46,604 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1285
2018-10-01 16:24:47,521 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000, fileSize: 320. Sent total: 320 bytes. Size of last segment intended to send: -1 bytes.
2018-10-01 16:24:48,028 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000001-0000000000000001284, fileSize: 118340. Sent total: 118340 bytes. Size of last segment intended to send: -1 bytes.
2018-10-01 16:24:49,159 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Combined time for fsimage download and fsync to all disks took 0.00s. The fsimage download took 0.00s at 7500.00 KB/s. Synchronous (fsync) write to disk of /tmp/hadoop-lag/dfs/name/current/fsimage.ckpt_0000000000000001284 took 0.00s.
2018-10-01 16:24:49,159 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000001284 size 16153 bytes.
2018-10-01 16:24:49,431 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2018-10-01 16:36:00,790 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 388 
2018-10-01 16:37:24,540 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 7 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 2 Number of syncs: 3 SyncTimes(ms): 390 
2018-10-01 16:37:25,434 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742031_1207, replicas=127.0.0.1:50010 for /user/output4/_temporary/0/_temporary/attempt_local1396838859_0001_r_000000_0/part-r-00000
2018-10-01 16:37:29,996 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output4/_temporary/0/_temporary/attempt_local1396838859_0001_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_-503029694_1
2018-10-01 16:37:30,831 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output4/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-503029694_1
2018-10-01 16:46:57,059 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2018-10-01 16:46:57,061 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at lag-Predator-G3-571/127.0.1.1
************************************************************/
2018-10-01 21:51:11,953 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = lag-Predator-G3-571/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.9.1
STARTUP_MSG:   classpath = /home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/etc/hadoop:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/activation-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-auth-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jettison-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/junit-4.11.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hadoop-annotations-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jsch-0.1.54.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-nfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-client-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/fst-2.50.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-core-1.8.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/nimbus-jose-jwt-4.41.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-beanutils-1.7.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-framework-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsp-api-2.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/gson-2.2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/curator-recipes-2.7.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/xmlenc-0.52.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jcip-annotations-1.0-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-digester-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-net-3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/json-smart-1.3.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jsch-0.1.54.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-client-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-router-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-registry-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-api-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/yarn/hadoop-yarn-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/avro-1.7.7.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/snappy-java-1.0.5.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.1-tests.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.9.1.jar:/home/lag/Git/All/BD/Hadoop/hadoop-2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.9.1.jar:/usr/lib/jvm/java-8-openjdk-amd64//lib/tools.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e30710aea4e6e55e69372929106cf119af06fd0e; compiled by 'root' on 2018-04-16T09:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-10-01 21:51:11,981 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-10-01 21:51:11,983 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2018-10-01 21:51:12,097 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-10-01 21:51:12,299 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-10-01 21:51:12,299 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2018-10-01 21:51:12,316 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2018-10-01 21:51:12,318 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2018-10-01 21:51:12,436 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-10-01 21:51:12,461 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2018-10-01 21:51:12,531 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-10-01 21:51:12,536 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-10-01 21:51:12,550 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2018-10-01 21:51:12,554 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-10-01 21:51:12,555 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2018-10-01 21:51:12,555 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-10-01 21:51:12,555 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-10-01 21:51:12,670 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2018-10-01 21:51:12,670 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2018-10-01 21:51:12,701 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2018-10-01 21:51:12,701 INFO org.mortbay.log: jetty-6.1.26
2018-10-01 21:51:13,055 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2018-10-01 21:51:13,078 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-10-01 21:51:13,079 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2018-10-01 21:51:13,106 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true
2018-10-01 21:51:13,113 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2018-10-01 21:51:13,113 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2018-10-01 21:51:13,114 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2018-10-01 21:51:13,117 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = lag (auth:SIMPLE)
2018-10-01 21:51:13,117 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2018-10-01 21:51:13,117 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2018-10-01 21:51:13,117 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2018-10-01 21:51:13,136 INFO org.apache.hadoop.hdfs.server.common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-10-01 21:51:13,148 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2018-10-01 21:51:13,148 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2018-10-01 21:51:13,150 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2018-10-01 21:51:13,150 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2018 Oct 01 21:51:13
2018-10-01 21:51:13,151 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2018-10-01 21:51:13,151 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-10-01 21:51:13,152 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2018-10-01 21:51:13,152 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2018-10-01 21:51:13,162 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2018-10-01 21:51:13,162 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS
2018-10-01 21:51:13,164 WARN org.apache.hadoop.conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2018-10-01 21:51:13,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2018-10-01 21:51:13,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2018-10-01 21:51:13,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2018-10-01 21:51:13,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2018-10-01 21:51:13,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2018-10-01 21:51:13,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2018-10-01 21:51:13,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2018-10-01 21:51:13,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2018-10-01 21:51:13,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2018-10-01 21:51:13,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2018-10-01 21:51:13,165 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2018-10-01 21:51:13,206 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2018-10-01 21:51:13,206 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-10-01 21:51:13,206 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2018-10-01 21:51:13,206 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2018-10-01 21:51:13,206 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2018-10-01 21:51:13,206 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2018-10-01 21:51:13,207 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2018-10-01 21:51:13,209 INFO org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false
2018-10-01 21:51:13,213 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2018-10-01 21:51:13,213 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-10-01 21:51:13,213 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2018-10-01 21:51:13,213 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2018-10-01 21:51:13,215 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2018-10-01 21:51:13,215 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2018-10-01 21:51:13,215 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2018-10-01 21:51:13,218 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2018-10-01 21:51:13,218 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2018-10-01 21:51:13,219 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2018-10-01 21:51:13,219 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2018-10-01 21:51:13,220 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2018-10-01 21:51:13,220 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2018-10-01 21:51:13,268 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-lag/dfs/name/in_use.lock acquired by nodename 7349@lag-Predator-G3-571
2018-10-01 21:51:13,280 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-lag/dfs/name/current
2018-10-01 21:51:13,280 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2018-10-01 21:51:13,280 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2018-10-01 21:51:13,353 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2018-10-01 21:51:13,369 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2018-10-01 21:51:13,369 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000
2018-10-01 21:51:13,372 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2018-10-01 21:51:13,372 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2018-10-01 21:51:13,520 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2018-10-01 21:51:13,520 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 298 msecs
2018-10-01 21:51:13,727 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2018-10-01 21:51:13,741 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-10-01 21:51:13,753 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2018-10-01 21:51:13,856 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2018-10-01 21:51:13,862 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2018-10-01 21:51:13,867 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues
2018-10-01 21:51:13,867 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2018-10-01 21:51:13,867 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2018-10-01 21:51:13,868 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2018-10-01 21:51:13,873 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2018-10-01 21:51:13,873 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2018-10-01 21:51:13,873 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2018-10-01 21:51:13,873 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2018-10-01 21:51:13,873 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2018-10-01 21:51:13,873 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 6 msec
2018-10-01 21:51:13,888 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-10-01 21:51:13,888 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2018-10-01 21:51:13,890 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2018-10-01 21:51:13,891 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2018-10-01 21:51:13,891 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
2018-10-01 21:51:13,893 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 2 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2018-10-01 21:51:13,896 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2018-10-01 21:51:17,730 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=b40c851a-007b-4318-977c-70243500ade3, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-7d3148c9-dcc7-444c-8096-e10ab44c02b7;nsid=1549815022;c=1538423468728) storage b40c851a-007b-4318-977c-70243500ade3
2018-10-01 21:51:17,731 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2018-10-01 21:51:17,731 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN b40c851a-007b-4318-977c-70243500ade3 (127.0.0.1:50010).
2018-10-01 21:51:17,775 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-a62d7fa5-6c95-48df-9fba-c84873acf8d2 for DN 127.0.0.1:50010
2018-10-01 21:51:17,799 INFO BlockStateChange: BLOCK* processReport 0x4fc94be17471eed: Processing first storage report for DS-a62d7fa5-6c95-48df-9fba-c84873acf8d2 from datanode b40c851a-007b-4318-977c-70243500ade3
2018-10-01 21:51:17,800 INFO BlockStateChange: BLOCK* processReport 0x4fc94be17471eed: from storage DS-a62d7fa5-6c95-48df-9fba-c84873acf8d2 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=b40c851a-007b-4318-977c-70243500ade3, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-7d3148c9-dcc7-444c-8096-e10ab44c02b7;nsid=1549815022;c=1538423468728), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2018-10-01 21:52:10,197 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=127.0.0.1:50010 for /user/ABU/adelaide2.txt._COPYING_
2018-10-01 21:52:10,333 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741825_1001 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/adelaide2.txt._COPYING_
2018-10-01 21:52:10,738 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/adelaide2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:10,764 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=127.0.0.1:50010 for /user/ABU/aiglon1.txt._COPYING_
2018-10-01 21:52:10,774 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/aiglon1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:10,789 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=127.0.0.1:50010 for /user/ABU/amoursjaunes1.txt._COPYING_
2018-10-01 21:52:10,799 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/amoursjaunes1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:10,815 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=127.0.0.1:50010 for /user/ABU/andromaque1.txt._COPYING_
2018-10-01 21:52:10,824 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/andromaque1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:10,839 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=127.0.0.1:50010 for /user/ABU/ane1.txt._COPYING_
2018-10-01 21:52:10,849 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ane1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:10,874 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=127.0.0.1:50010 for /user/ABU/annee1.txt._COPYING_
2018-10-01 21:52:10,886 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/annee1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:10,905 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=127.0.0.1:50010 for /user/ABU/antiquites1.txt._COPYING_
2018-10-01 21:52:10,921 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/antiquites1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:10,949 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008, replicas=127.0.0.1:50010 for /user/ABU/archiduc1.txt._COPYING_
2018-10-01 21:52:10,999 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/archiduc1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:11,027 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009, replicas=127.0.0.1:50010 for /user/ABU/arebours1.txt._COPYING_
2018-10-01 21:52:11,053 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/arebours1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:11,079 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010, replicas=127.0.0.1:50010 for /user/ABU/argent1.txt._COPYING_
2018-10-01 21:52:11,101 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741834_1010 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/argent1.txt._COPYING_
2018-10-01 21:52:11,503 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/argent1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:11,529 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011, replicas=127.0.0.1:50010 for /user/ABU/armance2.txt._COPYING_
2018-10-01 21:52:11,554 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/armance2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:11,579 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012, replicas=127.0.0.1:50010 for /user/ABU/arriamar2.txt._COPYING_
2018-10-01 21:52:11,601 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/arriamar2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:11,624 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013, replicas=127.0.0.1:50010 for /user/ABU/artgrdp1.txt._COPYING_
2018-10-01 21:52:11,647 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/artgrdp1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:11,670 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014, replicas=127.0.0.1:50010 for /user/ABU/athalie2.txt._COPYING_
2018-10-01 21:52:11,693 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/athalie2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:11,717 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015, replicas=127.0.0.1:50010 for /user/ABU/avare2.txt._COPYING_
2018-10-01 21:52:11,738 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/avare2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:11,767 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016, replicas=127.0.0.1:50010 for /user/ABU/aziyade1.txt._COPYING_
2018-10-01 21:52:11,789 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741840_1016 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/aziyade1.txt._COPYING_
2018-10-01 21:52:12,192 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/aziyade1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:12,215 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017, replicas=127.0.0.1:50010 for /user/ABU/ballades1.txt._COPYING_
2018-10-01 21:52:12,232 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741841_1017 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/ballades1.txt._COPYING_
2018-10-01 21:52:12,634 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ballades1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:12,658 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018, replicas=127.0.0.1:50010 for /user/ABU/ballon1.txt._COPYING_
2018-10-01 21:52:12,684 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ballon1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:12,728 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019, replicas=127.0.0.1:50010 for /user/ABU/becass2.txt._COPYING_
2018-10-01 21:52:12,752 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/becass2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:12,776 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020, replicas=127.0.0.1:50010 for /user/ABU/begum2.txt._COPYING_
2018-10-01 21:52:12,800 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741844_1020 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/begum2.txt._COPYING_
2018-10-01 21:52:13,202 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/begum2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:13,202 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 122 Total time for transactions(ms): 27 Number of transactions batched in Syncs: 20 Number of syncs: 102 SyncTimes(ms): 415 
2018-10-01 21:52:13,225 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021, replicas=127.0.0.1:50010 for /user/ABU/belami2.txt._COPYING_
2018-10-01 21:52:13,252 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/belami2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:13,291 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022, replicas=127.0.0.1:50010 for /user/ABU/bertris1.txt._COPYING_
2018-10-01 21:52:13,311 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741846_1022 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/bertris1.txt._COPYING_
2018-10-01 21:52:13,714 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/bertris1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:13,734 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023, replicas=127.0.0.1:50010 for /user/ABU/blocus3.txt._COPYING_
2018-10-01 21:52:13,751 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/blocus3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:13,778 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024, replicas=127.0.0.1:50010 for /user/ABU/boule2.txt._COPYING_
2018-10-01 21:52:13,800 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/boule2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:13,824 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025, replicas=127.0.0.1:50010 for /user/ABU/bounty1.txt._COPYING_
2018-10-01 21:52:13,845 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/bounty1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:13,869 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741850_1026, replicas=127.0.0.1:50010 for /user/ABU/bouvard2.txt._COPYING_
2018-10-01 21:52:13,896 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/bouvard2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:13,921 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741851_1027, replicas=127.0.0.1:50010 for /user/ABU/bovary3.txt._COPYING_
2018-10-01 21:52:13,945 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/bovary3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:13,964 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741852_1028, replicas=127.0.0.1:50010 for /user/ABU/bretagne1.txt._COPYING_
2018-10-01 21:52:13,984 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/bretagne1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:14,007 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741853_1029, replicas=127.0.0.1:50010 for /user/ABU/britannicus1.txt._COPYING_
2018-10-01 21:52:14,028 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/britannicus1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:14,074 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741854_1030, replicas=127.0.0.1:50010 for /user/ABU/bugjarg1.txt._COPYING_
2018-10-01 21:52:14,096 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/bugjarg1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:14,119 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741855_1031, replicas=127.0.0.1:50010 for /user/ABU/candide3.txt._COPYING_
2018-10-01 21:52:14,139 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/candide3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:14,164 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741856_1032, replicas=127.0.0.1:50010 for /user/ABU/caribou1.txt._COPYING_
2018-10-01 21:52:14,184 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/caribou1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:14,206 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741857_1033, replicas=127.0.0.1:50010 for /user/ABU/centurion1.txt._COPYING_
2018-10-01 21:52:14,227 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/centurion1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:14,247 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741858_1034, replicas=127.0.0.1:50010 for /user/ABU/chabert3.txt._COPYING_
2018-10-01 21:52:14,266 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/chabert3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:14,460 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741859_1035, replicas=127.0.0.1:50010 for /user/ABU/chambre2.txt._COPYING_
2018-10-01 21:52:14,480 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/chambre2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:14,505 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741860_1036, replicas=127.0.0.1:50010 for /user/ABU/champi1.txt._COPYING_
2018-10-01 21:52:14,523 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/champi1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:14,545 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741861_1037, replicas=127.0.0.1:50010 for /user/ABU/chartre1.txt._COPYING_
2018-10-01 21:52:14,574 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/chartre1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:14,617 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741862_1038, replicas=127.0.0.1:50010 for /user/ABU/chef2.txt._COPYING_
2018-10-01 21:52:14,637 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/chef2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:14,658 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741863_1039, replicas=127.0.0.1:50010 for /user/ABU/cinna2.txt._COPYING_
2018-10-01 21:52:14,678 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/cinna2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:14,704 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741864_1040, replicas=127.0.0.1:50010 for /user/ABU/cleves2.txt._COPYING_
2018-10-01 21:52:14,726 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/cleves2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:14,747 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741865_1041, replicas=127.0.0.1:50010 for /user/ABU/cocuage2.txt._COPYING_
2018-10-01 21:52:14,766 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/cocuage2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:14,791 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741866_1042, replicas=127.0.0.1:50010 for /user/ABU/colliergriffes1.txt._COPYING_
2018-10-01 21:52:14,810 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/colliergriffes1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:14,836 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741867_1043, replicas=127.0.0.1:50010 for /user/ABU/colomba1.txt._COPYING_
2018-10-01 21:52:14,855 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/colomba1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:14,880 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741868_1044, replicas=127.0.0.1:50010 for /user/ABU/commerce1.txt._COPYING_
2018-10-01 21:52:14,899 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/commerce1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:14,923 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741869_1045, replicas=127.0.0.1:50010 for /user/ABU/confessions1.txt._COPYING_
2018-10-01 21:52:14,949 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/confessions1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:14,967 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741870_1046, replicas=127.0.0.1:50010 for /user/ABU/conscrit2.txt._COPYING_
2018-10-01 21:52:14,983 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741870_1046 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/conscrit2.txt._COPYING_
2018-10-01 21:52:15,385 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/conscrit2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:15,408 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741871_1047, replicas=127.0.0.1:50010 for /user/ABU/consider1.txt._COPYING_
2018-10-01 21:52:15,423 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/consider1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:15,446 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741872_1048, replicas=127.0.0.1:50010 for /user/ABU/contemplA2.txt._COPYING_
2018-10-01 21:52:15,466 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/contemplA2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:15,487 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741873_1049, replicas=127.0.0.1:50010 for /user/ABU/contemplB2.txt._COPYING_
2018-10-01 21:52:15,507 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/contemplB2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:15,532 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741874_1050, replicas=127.0.0.1:50010 for /user/ABU/contrat1.txt._COPYING_
2018-10-01 21:52:15,551 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/contrat1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:15,572 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741875_1051, replicas=127.0.0.1:50010 for /user/ABU/crainque1.txt._COPYING_
2018-10-01 21:52:15,589 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/crainque1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:15,609 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741876_1052, replicas=127.0.0.1:50010 for /user/ABU/curee2.txt._COPYING_
2018-10-01 21:52:15,626 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/curee2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:15,645 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741877_1053, replicas=127.0.0.1:50010 for /user/ABU/cyrano1.txt._COPYING_
2018-10-01 21:52:15,665 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/cyrano1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:15,686 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741878_1054, replicas=127.0.0.1:50010 for /user/ABU/daphnis1.txt._COPYING_
2018-10-01 21:52:15,705 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/daphnis1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:15,727 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741879_1055, replicas=127.0.0.1:50010 for /user/ABU/ddhc3.txt._COPYING_
2018-10-01 21:52:15,746 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ddhc3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:15,770 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741880_1056, replicas=127.0.0.1:50010 for /user/ABU/desespere1.txt._COPYING_
2018-10-01 21:52:15,803 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/desespere1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:15,823 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741881_1057, replicas=127.0.0.1:50010 for /user/ABU/diableam3.txt._COPYING_
2018-10-01 21:52:15,837 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/diableam3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:15,856 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741882_1058, replicas=127.0.0.1:50010 for /user/ABU/diablecor2.txt._COPYING_
2018-10-01 21:52:15,871 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/diablecor2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:15,889 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741883_1059, replicas=127.0.0.1:50010 for /user/ABU/dicohisto1.txt._COPYING_
2018-10-01 21:52:15,905 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/dicohisto1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:15,925 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741884_1060, replicas=127.0.0.1:50010 for /user/ABU/dicotypo1.txt._COPYING_
2018-10-01 21:52:15,945 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/dicotypo1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:15,990 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741885_1061, replicas=127.0.0.1:50010 for /user/ABU/domi3.txt._COPYING_
2018-10-01 21:52:16,012 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/domi3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:16,050 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741886_1062, replicas=127.0.0.1:50010 for /user/ABU/domjuan2.txt._COPYING_
2018-10-01 21:52:16,069 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/domjuan2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:16,092 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741887_1063, replicas=127.0.0.1:50010 for /user/ABU/drageoir2.txt._COPYING_
2018-10-01 21:52:16,110 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741887_1063 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/drageoir2.txt._COPYING_
2018-10-01 21:52:16,512 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/drageoir2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:16,533 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741888_1064, replicas=127.0.0.1:50010 for /user/ABU/drole2.txt._COPYING_
2018-10-01 21:52:16,552 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/drole2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:16,572 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741889_1065, replicas=127.0.0.1:50010 for /user/ABU/ecole1.txt._COPYING_
2018-10-01 21:52:16,588 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ecole1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:16,608 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741890_1066, replicas=127.0.0.1:50010 for /user/ABU/educati1.txt._COPYING_
2018-10-01 21:52:16,629 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/educati1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:16,648 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741891_1067, replicas=127.0.0.1:50010 for /user/ABU/elixir2.txt._COPYING_
2018-10-01 21:52:16,662 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741891_1067 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/elixir2.txt._COPYING_
2018-10-01 21:52:17,063 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/elixir2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:17,084 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741892_1068, replicas=127.0.0.1:50010 for /user/ABU/erenouv1.txt._COPYING_
2018-10-01 21:52:17,101 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/erenouv1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:17,121 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741893_1069, replicas=127.0.0.1:50010 for /user/ABU/espece1.txt._COPYING_
2018-10-01 21:52:17,147 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/espece1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:17,186 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741894_1070, replicas=127.0.0.1:50010 for /user/ABU/etuit1.txt._COPYING_
2018-10-01 21:52:17,204 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/etuit1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:17,225 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741895_1071, replicas=127.0.0.1:50010 for /user/ABU/excentlang1.txt._COPYING_
2018-10-01 21:52:17,246 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/excentlang1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:17,268 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741896_1072, replicas=127.0.0.1:50010 for /user/ABU/fabulistes1.txt._COPYING_
2018-10-01 21:52:17,285 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/fabulistes1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:17,303 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741897_1073, replicas=127.0.0.1:50010 for /user/ABU/fadette1.txt._COPYING_
2018-10-01 21:52:17,318 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/fadette1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:17,528 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741898_1074, replicas=127.0.0.1:50010 for /user/ABU/farcefran1.txt._COPYING_
2018-10-01 21:52:17,544 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741898_1074 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/farcefran1.txt._COPYING_
2018-10-01 21:52:17,945 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/farcefran1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:17,979 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741899_1075, replicas=127.0.0.1:50010 for /user/ABU/femtin1.txt._COPYING_
2018-10-01 21:52:17,996 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741899_1075 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/femtin1.txt._COPYING_
2018-10-01 21:52:18,398 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/femtin1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:18,433 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741900_1076, replicas=127.0.0.1:50010 for /user/ABU/feuilles1.txt._COPYING_
2018-10-01 21:52:18,452 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/feuilles1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:18,471 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741901_1077, replicas=127.0.0.1:50010 for /user/ABU/figchose1.txt._COPYING_
2018-10-01 21:52:18,491 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/figchose1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:18,513 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741902_1078, replicas=127.0.0.1:50010 for /user/ABU/flandeux1.txt._COPYING_
2018-10-01 21:52:18,532 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/flandeux1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:18,553 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741903_1079, replicas=127.0.0.1:50010 for /user/ABU/football1.txt._COPYING_
2018-10-01 21:52:18,570 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741903_1079 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/football1.txt._COPYING_
2018-10-01 21:52:18,971 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/football1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:19,007 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741904_1080, replicas=127.0.0.1:50010 for /user/ABU/gargantua2.txt._COPYING_
2018-10-01 21:52:19,019 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/gargantua2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:19,037 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741905_1081, replicas=127.0.0.1:50010 for /user/ABU/gaspard2.txt._COPYING_
2018-10-01 21:52:19,048 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/gaspard2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:19,066 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741906_1082, replicas=127.0.0.1:50010 for /user/ABU/germinal1.txt._COPYING_
2018-10-01 21:52:19,082 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741906_1082 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/germinal1.txt._COPYING_
2018-10-01 21:52:19,484 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/germinal1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:19,521 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741907_1083, replicas=127.0.0.1:50010 for /user/ABU/germinie3.txt._COPYING_
2018-10-01 21:52:19,543 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/germinie3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:19,566 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741908_1084, replicas=127.0.0.1:50010 for /user/ABU/gilblas1.txt._COPYING_
2018-10-01 21:52:19,588 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/gilblas1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:19,609 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741909_1085, replicas=127.0.0.1:50010 for /user/ABU/guizeur1.txt._COPYING_
2018-10-01 21:52:19,632 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/guizeur1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:19,683 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741910_1086, replicas=127.0.0.1:50010 for /user/ABU/hamlet1.txt._COPYING_
2018-10-01 21:52:19,703 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/hamlet1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:19,725 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741911_1087, replicas=127.0.0.1:50010 for /user/ABU/harmonie1.txt._COPYING_
2018-10-01 21:52:19,742 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741911_1087 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/harmonie1.txt._COPYING_
2018-10-01 21:52:20,144 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/harmonie1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:20,176 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741912_1088, replicas=127.0.0.1:50010 for /user/ABU/historiettes2.txt._COPYING_
2018-10-01 21:52:20,198 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/historiettes2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:20,217 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741913_1089, replicas=127.0.0.1:50010 for /user/ABU/homecu1.txt._COPYING_
2018-10-01 21:52:20,231 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741913_1089 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/homecu1.txt._COPYING_
2018-10-01 21:52:20,632 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/homecu1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:20,654 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741914_1090, replicas=127.0.0.1:50010 for /user/ABU/horla3.txt._COPYING_
2018-10-01 21:52:20,671 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741914_1090 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/horla3.txt._COPYING_
2018-10-01 21:52:21,073 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/horla3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:21,095 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741915_1091, replicas=127.0.0.1:50010 for /user/ABU/hugoshak1.txt._COPYING_
2018-10-01 21:52:21,112 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741915_1091 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/hugoshak1.txt._COPYING_
2018-10-01 21:52:21,514 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/hugoshak1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:21,535 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741916_1092, replicas=127.0.0.1:50010 for /user/ABU/humilite3.txt._COPYING_
2018-10-01 21:52:21,550 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741916_1092 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/humilite3.txt._COPYING_
2018-10-01 21:52:21,951 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/humilite3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:21,973 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741917_1093, replicas=127.0.0.1:50010 for /user/ABU/ideolo1.txt._COPYING_
2018-10-01 21:52:21,990 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741917_1093 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/ideolo1.txt._COPYING_
2018-10-01 21:52:22,151 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-10-01 21:52:22,151 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-10-01 21:52:22,151 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1, 559
2018-10-01 21:52:22,164 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 560 Total time for transactions(ms): 60 Number of transactions batched in Syncs: 93 Number of syncs: 468 SyncTimes(ms): 2241 
2018-10-01 21:52:22,165 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000001-0000000000000000560
2018-10-01 21:52:22,182 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 561
2018-10-01 21:52:22,392 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ideolo1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:22,477 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741918_1094, replicas=127.0.0.1:50010 for /user/ABU/illusion1.txt._COPYING_
2018-10-01 21:52:22,528 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/illusion1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:22,605 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741919_1095, replicas=127.0.0.1:50010 for /user/ABU/inquisit2.txt._COPYING_
2018-10-01 21:52:22,654 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/inquisit2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:22,728 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741920_1096, replicas=127.0.0.1:50010 for /user/ABU/interpret2.txt._COPYING_
2018-10-01 21:52:22,790 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/interpret2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:22,878 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741921_1097, replicas=127.0.0.1:50010 for /user/ABU/iphigenie1.txt._COPYING_
2018-10-01 21:52:22,922 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741921_1097 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/iphigenie1.txt._COPYING_
2018-10-01 21:52:23,077 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000, fileSize: 320. Sent total: 320 bytes. Size of last segment intended to send: -1 bytes.
2018-10-01 21:52:23,111 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000001-0000000000000000560, fileSize: 50113. Sent total: 50113 bytes. Size of last segment intended to send: -1 bytes.
2018-10-01 21:52:23,323 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/iphigenie1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:23,427 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741922_1098, replicas=127.0.0.1:50010 for /user/ABU/irnois2.txt._COPYING_
2018-10-01 21:52:23,462 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Combined time for fsimage download and fsync to all disks took 0.00s. The fsimage download took 0.00s at 7000.00 KB/s. Synchronous (fsync) write to disk of /tmp/hadoop-lag/dfs/name/current/fsimage.ckpt_0000000000000000560 took 0.00s.
2018-10-01 21:52:23,462 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000560 size 7377 bytes.
2018-10-01 21:52:23,487 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741922_1098 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/irnois2.txt._COPYING_
2018-10-01 21:52:23,488 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2018-10-01 21:52:23,888 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/irnois2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:23,975 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741923_1099, replicas=127.0.0.1:50010 for /user/ABU/jaccuse3.txt._COPYING_
2018-10-01 21:52:24,022 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741923_1099 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/jaccuse3.txt._COPYING_
2018-10-01 21:52:24,423 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/jaccuse3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:24,496 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741924_1100, replicas=127.0.0.1:50010 for /user/ABU/jacques1.txt._COPYING_
2018-10-01 21:52:24,545 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/jacques1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:24,611 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741925_1101, replicas=127.0.0.1:50010 for /user/ABU/journalism1.txt._COPYING_
2018-10-01 21:52:24,671 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/journalism1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:24,748 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741926_1102, replicas=127.0.0.1:50010 for /user/ABU/journbloy1.txt._COPYING_
2018-10-01 21:52:24,803 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741926_1102 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/journbloy1.txt._COPYING_
2018-10-01 21:52:25,204 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/journbloy1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:25,279 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741927_1103, replicas=127.0.0.1:50010 for /user/ABU/justice1.txt._COPYING_
2018-10-01 21:52:25,336 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741927_1103 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/justice1.txt._COPYING_
2018-10-01 21:52:25,737 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/justice1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:25,811 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741928_1104, replicas=127.0.0.1:50010 for /user/ABU/lafille2.txt._COPYING_
2018-10-01 21:52:25,872 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741928_1104 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/lafille2.txt._COPYING_
2018-10-01 21:52:26,274 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lafille2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:26,345 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741929_1105, replicas=127.0.0.1:50010 for /user/ABU/lecid1.txt._COPYING_
2018-10-01 21:52:26,400 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741929_1105 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/lecid1.txt._COPYING_
2018-10-01 21:52:26,802 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lecid1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:26,877 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741930_1106, replicas=127.0.0.1:50010 for /user/ABU/legend1.txt._COPYING_
2018-10-01 21:52:26,937 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/legend1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:27,014 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741931_1107, replicas=127.0.0.1:50010 for /user/ABU/legendet21.txt._COPYING_
2018-10-01 21:52:27,069 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/legendet21.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:27,319 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741932_1108, replicas=127.0.0.1:50010 for /user/ABU/leibnitzdiv1.txt._COPYING_
2018-10-01 21:52:27,381 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741932_1108 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/leibnitzdiv1.txt._COPYING_
2018-10-01 21:52:27,782 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/leibnitzdiv1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:27,858 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741933_1109, replicas=127.0.0.1:50010 for /user/ABU/lejeu1.txt._COPYING_
2018-10-01 21:52:27,918 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741933_1109 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/lejeu1.txt._COPYING_
2018-10-01 21:52:28,319 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lejeu1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:28,391 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741934_1110, replicas=127.0.0.1:50010 for /user/ABU/lepetitchose1.txt._COPYING_
2018-10-01 21:52:28,451 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741934_1110 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/lepetitchose1.txt._COPYING_
2018-10-01 21:52:28,852 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lepetitchose1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:28,923 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741935_1111, replicas=127.0.0.1:50010 for /user/ABU/lessoirees1.txt._COPYING_
2018-10-01 21:52:28,985 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741935_1111 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/lessoirees1.txt._COPYING_
2018-10-01 21:52:29,386 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lessoirees1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:29,467 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741936_1112, replicas=127.0.0.1:50010 for /user/ABU/letphi1.txt._COPYING_
2018-10-01 21:52:29,526 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/letphi1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:29,622 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741937_1113, replicas=127.0.0.1:50010 for /user/ABU/lettresecrites1.txt._COPYING_
2018-10-01 21:52:29,683 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lettresecrites1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:29,758 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741938_1114, replicas=127.0.0.1:50010 for /user/ABU/lettresjuives11.txt._COPYING_
2018-10-01 21:52:29,821 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lettresjuives11.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:29,895 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741939_1115, replicas=127.0.0.1:50010 for /user/ABU/lettresjuives231.txt._COPYING_
2018-10-01 21:52:29,956 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741939_1115 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/lettresjuives231.txt._COPYING_
2018-10-01 21:52:30,358 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lettresjuives231.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:30,451 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741940_1116, replicas=127.0.0.1:50010 for /user/ABU/lettresjuives451.txt._COPYING_
2018-10-01 21:52:30,533 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lettresjuives451.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:30,608 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741941_1117, replicas=127.0.0.1:50010 for /user/ABU/lettresjuives6781.txt._COPYING_
2018-10-01 21:52:30,668 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741941_1117 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/lettresjuives6781.txt._COPYING_
2018-10-01 21:52:31,070 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lettresjuives6781.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:31,151 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741942_1118, replicas=127.0.0.1:50010 for /user/ABU/lettresreli2.txt._COPYING_
2018-10-01 21:52:31,210 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lettresreli2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:31,287 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741943_1119, replicas=127.0.0.1:50010 for /user/ABU/levieux2.txt._COPYING_
2018-10-01 21:52:31,346 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741943_1119 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/levieux2.txt._COPYING_
2018-10-01 21:52:31,748 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/levieux2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:31,820 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741944_1120, replicas=127.0.0.1:50010 for /user/ABU/liaisons3.txt._COPYING_
2018-10-01 21:52:31,883 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/liaisons3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:31,956 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741945_1121, replicas=127.0.0.1:50010 for /user/ABU/licong1.txt._COPYING_
2018-10-01 21:52:32,016 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/licong1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:32,093 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741946_1122, replicas=127.0.0.1:50010 for /user/ABU/lmain3.txt._COPYING_
2018-10-01 21:52:32,140 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741946_1122 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/lmain3.txt._COPYING_
2018-10-01 21:52:32,542 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lmain3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:32,612 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741947_1123, replicas=127.0.0.1:50010 for /user/ABU/lutrin1.txt._COPYING_
2018-10-01 21:52:32,662 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741947_1123 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/lutrin1.txt._COPYING_
2018-10-01 21:52:33,063 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/lutrin1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:33,136 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741948_1124, replicas=127.0.0.1:50010 for /user/ABU/m702douai2.txt._COPYING_
2018-10-01 21:52:33,200 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741948_1124 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/m702douai2.txt._COPYING_
2018-10-01 21:52:33,601 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/m702douai2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:33,679 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741949_1125, replicas=127.0.0.1:50010 for /user/ABU/machine3.txt._COPYING_
2018-10-01 21:52:33,727 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741949_1125 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/machine3.txt._COPYING_
2018-10-01 21:52:34,128 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/machine3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:34,198 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741950_1126, replicas=127.0.0.1:50010 for /user/ABU/manifeste2.txt._COPYING_
2018-10-01 21:52:34,248 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741950_1126 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/manifeste2.txt._COPYING_
2018-10-01 21:52:34,650 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/manifeste2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:34,720 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741951_1127, replicas=127.0.0.1:50010 for /user/ABU/manon2.txt._COPYING_
2018-10-01 21:52:34,771 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741951_1127 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/manon2.txt._COPYING_
2018-10-01 21:52:35,173 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/manon2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:35,266 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741952_1128, replicas=127.0.0.1:50010 for /user/ABU/marie1.txt._COPYING_
2018-10-01 21:52:35,328 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741952_1128 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/marie1.txt._COPYING_
2018-10-01 21:52:35,730 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/marie1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:35,808 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741953_1129, replicas=127.0.0.1:50010 for /user/ABU/maximes2.txt._COPYING_
2018-10-01 21:52:35,867 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741953_1129 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/maximes2.txt._COPYING_
2018-10-01 21:52:36,269 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/maximes2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:36,339 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741954_1130, replicas=127.0.0.1:50010 for /user/ABU/medipoet1.txt._COPYING_
2018-10-01 21:52:36,395 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741954_1130 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/medipoet1.txt._COPYING_
2018-10-01 21:52:36,797 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/medipoet1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:36,872 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741955_1131, replicas=127.0.0.1:50010 for /user/ABU/medit3.txt._COPYING_
2018-10-01 21:52:36,932 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741955_1131 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/medit3.txt._COPYING_
2018-10-01 21:52:37,333 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/medit3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:37,406 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741956_1132, replicas=127.0.0.1:50010 for /user/ABU/medita1.txt._COPYING_
2018-10-01 21:52:37,465 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741956_1132 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/medita1.txt._COPYING_
2018-10-01 21:52:37,867 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/medita1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:37,938 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741957_1133, replicas=127.0.0.1:50010 for /user/ABU/methode3.txt._COPYING_
2018-10-01 21:52:38,004 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741957_1133 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/methode3.txt._COPYING_
2018-10-01 21:52:38,405 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/methode3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:38,478 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741958_1134, replicas=127.0.0.1:50010 for /user/ABU/micromeg3.txt._COPYING_
2018-10-01 21:52:38,536 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741958_1134 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/micromeg3.txt._COPYING_
2018-10-01 21:52:38,937 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/micromeg3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:39,010 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741959_1135, replicas=127.0.0.1:50010 for /user/ABU/monadologie1.txt._COPYING_
2018-10-01 21:52:39,068 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741959_1135 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/monadologie1.txt._COPYING_
2018-10-01 21:52:39,470 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/monadologie1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:39,542 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741960_1136, replicas=127.0.0.1:50010 for /user/ABU/monde1.txt._COPYING_
2018-10-01 21:52:39,601 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741960_1136 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/monde1.txt._COPYING_
2018-10-01 21:52:40,002 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/monde1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:40,075 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741961_1137, replicas=127.0.0.1:50010 for /user/ABU/montbard1.txt._COPYING_
2018-10-01 21:52:40,134 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741961_1137 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/montbard1.txt._COPYING_
2018-10-01 21:52:40,535 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/montbard1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:40,605 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741962_1138, replicas=127.0.0.1:50010 for /user/ABU/morte2.txt._COPYING_
2018-10-01 21:52:40,665 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741962_1138 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/morte2.txt._COPYING_
2018-10-01 21:52:41,067 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/morte2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:41,138 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741963_1139, replicas=127.0.0.1:50010 for /user/ABU/mousque1.txt._COPYING_
2018-10-01 21:52:41,201 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741963_1139 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/mousque1.txt._COPYING_
2018-10-01 21:52:41,603 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/mousque1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:41,683 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741964_1140, replicas=127.0.0.1:50010 for /user/ABU/nddp1.txt._COPYING_
2018-10-01 21:52:41,744 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741964_1140 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/nddp1.txt._COPYING_
2018-10-01 21:52:42,146 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/nddp1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:42,226 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741965_1141, replicas=127.0.0.1:50010 for /user/ABU/neveu2.txt._COPYING_
2018-10-01 21:52:42,285 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741965_1141 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/neveu2.txt._COPYING_
2018-10-01 21:52:42,687 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/neveu2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:42,781 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741966_1142, replicas=127.0.0.1:50010 for /user/ABU/nouvmedi1.txt._COPYING_
2018-10-01 21:52:42,840 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741966_1142 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/nouvmedi1.txt._COPYING_
2018-10-01 21:52:43,241 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/nouvmedi1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:43,317 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741967_1143, replicas=127.0.0.1:50010 for /user/ABU/odespolit1.txt._COPYING_
2018-10-01 21:52:43,379 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741967_1143 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/odespolit1.txt._COPYING_
2018-10-01 21:52:43,780 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/odespolit1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:43,959 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741968_1144, replicas=127.0.0.1:50010 for /user/ABU/opinions3.txt._COPYING_
2018-10-01 21:52:44,018 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741968_1144 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/opinions3.txt._COPYING_
2018-10-01 21:52:44,420 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/opinions3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:44,491 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741969_1145, replicas=127.0.0.1:50010 for /user/ABU/oriental1.txt._COPYING_
2018-10-01 21:52:44,550 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741969_1145 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/oriental1.txt._COPYING_
2018-10-01 21:52:44,951 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/oriental1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:45,023 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741970_1146, replicas=127.0.0.1:50010 for /user/ABU/paresse3.txt._COPYING_
2018-10-01 21:52:45,082 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741970_1146 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/paresse3.txt._COPYING_
2018-10-01 21:52:45,484 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/paresse3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:45,727 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741971_1147, replicas=127.0.0.1:50010 for /user/ABU/partiecamp2.txt._COPYING_
2018-10-01 21:52:45,790 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741971_1147 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/partiecamp2.txt._COPYING_
2018-10-01 21:52:46,192 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/partiecamp2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:46,262 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741972_1148, replicas=127.0.0.1:50010 for /user/ABU/pascaldiv1.txt._COPYING_
2018-10-01 21:52:46,321 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741972_1148 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/pascaldiv1.txt._COPYING_
2018-10-01 21:52:46,722 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/pascaldiv1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:46,795 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741973_1149, replicas=127.0.0.1:50010 for /user/ABU/pascalpetits1.txt._COPYING_
2018-10-01 21:52:46,854 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741973_1149 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/pascalpetits1.txt._COPYING_
2018-10-01 21:52:47,255 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/pascalpetits1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:47,329 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741974_1150, replicas=127.0.0.1:50010 for /user/ABU/penseesXX1.txt._COPYING_
2018-10-01 21:52:47,390 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741974_1150 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/penseesXX1.txt._COPYING_
2018-10-01 21:52:47,791 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/penseesXX1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:47,860 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741975_1151, replicas=127.0.0.1:50010 for /user/ABU/pensepict1.txt._COPYING_
2018-10-01 21:52:47,917 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741975_1151 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/pensepict1.txt._COPYING_
2018-10-01 21:52:48,318 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/pensepict1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:48,395 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741976_1152, replicas=127.0.0.1:50010 for /user/ABU/phedre2.txt._COPYING_
2018-10-01 21:52:48,449 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741976_1152 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/phedre2.txt._COPYING_
2018-10-01 21:52:48,850 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/phedre2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:49,150 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741977_1153, replicas=127.0.0.1:50010 for /user/ABU/plural3.txt._COPYING_
2018-10-01 21:52:49,209 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741977_1153 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/plural3.txt._COPYING_
2018-10-01 21:52:49,610 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/plural3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:49,682 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741978_1154, replicas=127.0.0.1:50010 for /user/ABU/poemallar1.txt._COPYING_
2018-10-01 21:52:49,787 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741978_1154 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/poemallar1.txt._COPYING_
2018-10-01 21:52:50,188 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/poemallar1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:50,271 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741979_1155, replicas=127.0.0.1:50010 for /user/ABU/poemesadv1.txt._COPYING_
2018-10-01 21:52:50,330 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741979_1155 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/poemesadv1.txt._COPYING_
2018-10-01 21:52:50,732 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/poemesadv1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:50,803 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741980_1156, replicas=127.0.0.1:50010 for /user/ABU/presseguerre1.txt._COPYING_
2018-10-01 21:52:50,862 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741980_1156 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/presseguerre1.txt._COPYING_
2018-10-01 21:52:51,264 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/presseguerre1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:51,334 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741981_1157, replicas=127.0.0.1:50010 for /user/ABU/preuves1.txt._COPYING_
2018-10-01 21:52:51,391 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741981_1157 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/preuves1.txt._COPYING_
2018-10-01 21:52:51,792 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/preuves1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:51,869 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741982_1158, replicas=127.0.0.1:50010 for /user/ABU/propriete1.txt._COPYING_
2018-10-01 21:52:51,930 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741982_1158 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/propriete1.txt._COPYING_
2018-10-01 21:52:52,331 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/propriete1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:52,411 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741983_1159, replicas=127.0.0.1:50010 for /user/ABU/pucelle1.txt._COPYING_
2018-10-01 21:52:52,472 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741983_1159 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/pucelle1.txt._COPYING_
2018-10-01 21:52:52,873 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/pucelle1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:52,944 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741984_1160, replicas=127.0.0.1:50010 for /user/ABU/quatrevt1.txt._COPYING_
2018-10-01 21:52:53,005 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741984_1160 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/quatrevt1.txt._COPYING_
2018-10-01 21:52:53,406 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/quatrevt1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:53,488 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741985_1161, replicas=127.0.0.1:50010 for /user/ABU/raison2.txt._COPYING_
2018-10-01 21:52:53,546 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741985_1161 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/raison2.txt._COPYING_
2018-10-01 21:52:53,948 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/raison2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:54,071 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741986_1162, replicas=127.0.0.1:50010 for /user/ABU/rayons1.txt._COPYING_
2018-10-01 21:52:54,208 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741986_1162 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/rayons1.txt._COPYING_
2018-10-01 21:52:54,609 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/rayons1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:54,682 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741987_1163, replicas=127.0.0.1:50010 for /user/ABU/ren05101.txt._COPYING_
2018-10-01 21:52:54,741 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741987_1163 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/ren05101.txt._COPYING_
2018-10-01 21:52:55,143 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ren05101.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:55,307 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741988_1164, replicas=127.0.0.1:50010 for /user/ABU/ren87922.txt._COPYING_
2018-10-01 21:52:55,366 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741988_1164 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/ren87922.txt._COPYING_
2018-10-01 21:52:55,768 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ren87922.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:55,839 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741989_1165, replicas=127.0.0.1:50010 for /user/ABU/ren93982.txt._COPYING_
2018-10-01 21:52:55,901 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741989_1165 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/ren93982.txt._COPYING_
2018-10-01 21:52:56,302 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ren93982.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:56,383 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741990_1166, replicas=127.0.0.1:50010 for /user/ABU/ren99041.txt._COPYING_
2018-10-01 21:52:56,443 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741990_1166 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/ren99041.txt._COPYING_
2018-10-01 21:52:56,844 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ren99041.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:56,916 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741991_1167, replicas=127.0.0.1:50010 for /user/ABU/reveries3.txt._COPYING_
2018-10-01 21:52:56,974 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741991_1167 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/reveries3.txt._COPYING_
2018-10-01 21:52:57,375 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/reveries3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:57,458 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741992_1168, replicas=127.0.0.1:50010 for /user/ABU/rldased3.txt._COPYING_
2018-10-01 21:52:57,517 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741992_1168 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/rldased3.txt._COPYING_
2018-10-01 21:52:57,919 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/rldased3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:57,991 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741993_1169, replicas=127.0.0.1:50010 for /user/ABU/robur1.txt._COPYING_
2018-10-01 21:52:58,047 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741993_1169 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/robur1.txt._COPYING_
2018-10-01 21:52:58,449 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/robur1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:58,520 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741994_1170, replicas=127.0.0.1:50010 for /user/ABU/roland2.txt._COPYING_
2018-10-01 21:52:58,579 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741994_1170 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/roland2.txt._COPYING_
2018-10-01 21:52:58,980 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/roland2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:59,150 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741995_1171, replicas=127.0.0.1:50010 for /user/ABU/roletrav2.txt._COPYING_
2018-10-01 21:52:59,266 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741995_1171 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/roletrav2.txt._COPYING_
2018-10-01 21:52:59,668 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/roletrav2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:52:59,740 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741996_1172, replicas=127.0.0.1:50010 for /user/ABU/rouge1.txt._COPYING_
2018-10-01 21:52:59,801 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741996_1172 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/rouge1.txt._COPYING_
2018-10-01 21:53:00,203 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/rouge1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:00,283 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741997_1173, replicas=127.0.0.1:50010 for /user/ABU/rousgene1.txt._COPYING_
2018-10-01 21:53:00,353 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741997_1173 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/rousgene1.txt._COPYING_
2018-10-01 21:53:00,755 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/rousgene1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:00,837 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741998_1174, replicas=127.0.0.1:50010 for /user/ABU/ruesboi1.txt._COPYING_
2018-10-01 21:53:00,913 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741998_1174 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/ruesboi1.txt._COPYING_
2018-10-01 21:53:01,314 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ruesboi1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:01,385 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741999_1175, replicas=127.0.0.1:50010 for /user/ABU/salalion1.txt._COPYING_
2018-10-01 21:53:01,445 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741999_1175 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/salalion1.txt._COPYING_
2018-10-01 21:53:01,847 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/salalion1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:01,919 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742000_1176, replicas=127.0.0.1:50010 for /user/ABU/salammb1.txt._COPYING_
2018-10-01 21:53:01,980 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742000_1176 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/salammb1.txt._COPYING_
2018-10-01 21:53:02,381 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/salammb1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:02,463 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742001_1177, replicas=127.0.0.1:50010 for /user/ABU/satan1.txt._COPYING_
2018-10-01 21:53:02,521 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742001_1177 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/satan1.txt._COPYING_
2018-10-01 21:53:02,923 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/satan1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:02,993 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742002_1178, replicas=127.0.0.1:50010 for /user/ABU/scapin2.txt._COPYING_
2018-10-01 21:53:03,053 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742002_1178 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/scapin2.txt._COPYING_
2018-10-01 21:53:03,455 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/scapin2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:03,527 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742003_1179, replicas=127.0.0.1:50010 for /user/ABU/scihyp2.txt._COPYING_
2018-10-01 21:53:03,586 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742003_1179 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/scihyp2.txt._COPYING_
2018-10-01 21:53:03,988 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/scihyp2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:04,059 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742004_1180, replicas=127.0.0.1:50010 for /user/ABU/septfem2.txt._COPYING_
2018-10-01 21:53:04,118 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742004_1180 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/septfem2.txt._COPYING_
2018-10-01 21:53:04,520 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/septfem2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:04,633 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742005_1181, replicas=127.0.0.1:50010 for /user/ABU/smarra1.txt._COPYING_
2018-10-01 21:53:04,692 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742005_1181 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/smarra1.txt._COPYING_
2018-10-01 21:53:05,094 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/smarra1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:05,166 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742006_1182, replicas=127.0.0.1:50010 for /user/ABU/souveni1.txt._COPYING_
2018-10-01 21:53:05,225 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742006_1182 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/souveni1.txt._COPYING_
2018-10-01 21:53:05,626 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/souveni1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:05,698 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742007_1183, replicas=127.0.0.1:50010 for /user/ABU/supplem2.txt._COPYING_
2018-10-01 21:53:05,757 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742007_1183 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/supplem2.txt._COPYING_
2018-10-01 21:53:06,159 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/supplem2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:06,419 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742008_1184, replicas=127.0.0.1:50010 for /user/ABU/tartuf2.txt._COPYING_
2018-10-01 21:53:06,482 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742008_1184 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/tartuf2.txt._COPYING_
2018-10-01 21:53:06,884 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/tartuf2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:06,956 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742009_1185, replicas=127.0.0.1:50010 for /user/ABU/tdm80j2.txt._COPYING_
2018-10-01 21:53:07,015 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742009_1185 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/tdm80j2.txt._COPYING_
2018-10-01 21:53:07,417 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/tdm80j2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:07,488 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742010_1186, replicas=127.0.0.1:50010 for /user/ABU/theodicee1.txt._COPYING_
2018-10-01 21:53:07,547 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742010_1186 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/theodicee1.txt._COPYING_
2018-10-01 21:53:07,948 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/theodicee1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:08,021 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742011_1187, replicas=127.0.0.1:50010 for /user/ABU/theoriephys1.txt._COPYING_
2018-10-01 21:53:08,077 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742011_1187 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/theoriephys1.txt._COPYING_
2018-10-01 21:53:08,478 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/theoriephys1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:08,553 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742012_1188, replicas=127.0.0.1:50010 for /user/ABU/thotri1.txt._COPYING_
2018-10-01 21:53:08,620 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/thotri1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:08,685 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742013_1189, replicas=127.0.0.1:50010 for /user/ABU/tlun3.txt._COPYING_
2018-10-01 21:53:08,746 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742013_1189 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/tlun3.txt._COPYING_
2018-10-01 21:53:09,148 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/tlun3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:09,219 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742014_1190, replicas=127.0.0.1:50010 for /user/ABU/tome1malherbe1.txt._COPYING_
2018-10-01 21:53:09,282 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/tome1malherbe1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:09,355 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742015_1191, replicas=127.0.0.1:50010 for /user/ABU/troylion1.txt._COPYING_
2018-10-01 21:53:09,444 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742015_1191 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/troylion1.txt._COPYING_
2018-10-01 21:53:09,846 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/troylion1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:09,939 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742016_1192, replicas=127.0.0.1:50010 for /user/ABU/ulenspiegel1.txt._COPYING_
2018-10-01 21:53:10,043 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742016_1192 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/ulenspiegel1.txt._COPYING_
2018-10-01 21:53:10,444 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/ulenspiegel1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:10,524 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742017_1193, replicas=127.0.0.1:50010 for /user/ABU/uncoeur3.txt._COPYING_
2018-10-01 21:53:10,582 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742017_1193 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/uncoeur3.txt._COPYING_
2018-10-01 21:53:10,984 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/uncoeur3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:11,055 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742018_1194, replicas=127.0.0.1:50010 for /user/ABU/unevie2.txt._COPYING_
2018-10-01 21:53:11,115 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742018_1194 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/unevie2.txt._COPYING_
2018-10-01 21:53:11,516 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/unevie2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:11,588 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742019_1195, replicas=127.0.0.1:50010 for /user/ABU/utopique2.txt._COPYING_
2018-10-01 21:53:11,646 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742019_1195 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/utopique2.txt._COPYING_
2018-10-01 21:53:12,048 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/utopique2.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:12,121 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742020_1196, replicas=127.0.0.1:50010 for /user/ABU/verhafin1.txt._COPYING_
2018-10-01 21:53:12,179 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742020_1196 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/verhafin1.txt._COPYING_
2018-10-01 21:53:12,580 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/verhafin1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:12,653 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742021_1197, replicas=127.0.0.1:50010 for /user/ABU/viemars3.txt._COPYING_
2018-10-01 21:53:12,711 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742021_1197 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/viemars3.txt._COPYING_
2018-10-01 21:53:13,112 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/viemars3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:13,185 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742022_1198, replicas=127.0.0.1:50010 for /user/ABU/vignypoesie1.txt._COPYING_
2018-10-01 21:53:13,244 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742022_1198 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/vignypoesie1.txt._COPYING_
2018-10-01 21:53:13,645 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/vignypoesie1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:13,716 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742023_1199, replicas=127.0.0.1:50010 for /user/ABU/volpofin1.txt._COPYING_
2018-10-01 21:53:13,776 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742023_1199 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/volpofin1.txt._COPYING_
2018-10-01 21:53:14,178 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/volpofin1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:14,261 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742024_1200, replicas=127.0.0.1:50010 for /user/ABU/voltgene1.txt._COPYING_
2018-10-01 21:53:14,320 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742024_1200 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/voltgene1.txt._COPYING_
2018-10-01 21:53:14,721 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/voltgene1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:14,811 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742025_1201, replicas=127.0.0.1:50010 for /user/ABU/voyfran1.txt._COPYING_
2018-10-01 21:53:14,872 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742025_1201 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/voyfran1.txt._COPYING_
2018-10-01 21:53:15,273 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/voyfran1.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:53:15,344 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742026_1202, replicas=127.0.0.1:50010 for /user/ABU/voylun3.txt._COPYING_
2018-10-01 21:53:15,403 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742026_1202 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/ABU/voylun3.txt._COPYING_
2018-10-01 21:53:15,804 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/ABU/voylun3.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1243360548_1
2018-10-01 21:55:10,657 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 660 Total time for transactions(ms): 49 Number of transactions batched in Syncs: 109 Number of syncs: 549 SyncTimes(ms): 14258 
2018-10-01 21:55:34,183 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742027_1203, replicas=127.0.0.1:50010 for /user/output/_temporary/0/_temporary/attempt_local1921099901_0001_r_000000_0/part-r-00000
2018-10-01 21:55:34,745 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output/_temporary/0/_temporary/attempt_local1921099901_0001_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_-1324885987_1
2018-10-01 21:55:38,218 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742028_1204, replicas=127.0.0.1:50010 for /user/output/_temporary/0/_temporary/attempt_local1921099901_0001_r_000001_0/part-r-00001
2018-10-01 21:55:38,714 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742028_1204 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/output/_temporary/0/_temporary/attempt_local1921099901_0001_r_000001_0/part-r-00001
2018-10-01 21:55:39,115 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output/_temporary/0/_temporary/attempt_local1921099901_0001_r_000001_0/part-r-00001 is closed by DFSClient_NONMAPREDUCE_-1324885987_1
2018-10-01 21:55:43,053 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742029_1205, replicas=127.0.0.1:50010 for /user/output/_temporary/0/_temporary/attempt_local1921099901_0001_r_000002_0/part-r-00002
2018-10-01 21:55:43,571 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742029_1205 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/output/_temporary/0/_temporary/attempt_local1921099901_0001_r_000002_0/part-r-00002
2018-10-01 21:55:43,972 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output/_temporary/0/_temporary/attempt_local1921099901_0001_r_000002_0/part-r-00002 is closed by DFSClient_NONMAPREDUCE_-1324885987_1
2018-10-01 21:55:44,691 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1324885987_1
2018-10-01 22:08:49,501 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 691 Total time for transactions(ms): 51 Number of transactions batched in Syncs: 116 Number of syncs: 573 SyncTimes(ms): 15998 
2018-10-01 22:09:30,943 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742030_1206, replicas=127.0.0.1:50010 for /user/output2/_temporary/0/_temporary/attempt_local840492516_0001_r_000000_0/part-r-00000
2018-10-01 22:09:31,579 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output2/_temporary/0/_temporary/attempt_local840492516_0001_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_-367946290_1
2018-10-01 22:09:47,844 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742031_1207, replicas=127.0.0.1:50010 for /user/output2/_temporary/0/_temporary/attempt_local840492516_0001_r_000001_0/part-r-00001
2018-10-01 22:09:48,498 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742031_1207 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/output2/_temporary/0/_temporary/attempt_local840492516_0001_r_000001_0/part-r-00001
2018-10-01 22:09:48,900 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output2/_temporary/0/_temporary/attempt_local840492516_0001_r_000001_0/part-r-00001 is closed by DFSClient_NONMAPREDUCE_-367946290_1
2018-10-01 22:10:00,388 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 708 Total time for transactions(ms): 51 Number of transactions batched in Syncs: 122 Number of syncs: 585 SyncTimes(ms): 16171 
2018-10-01 22:10:01,352 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742032_1208, replicas=127.0.0.1:50010 for /user/output2/_temporary/0/_temporary/attempt_local840492516_0001_r_000002_0/part-r-00002
2018-10-01 22:10:01,858 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742032_1208 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/output2/_temporary/0/_temporary/attempt_local840492516_0001_r_000002_0/part-r-00002
2018-10-01 22:10:02,260 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output2/_temporary/0/_temporary/attempt_local840492516_0001_r_000002_0/part-r-00002 is closed by DFSClient_NONMAPREDUCE_-367946290_1
2018-10-01 22:10:02,570 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output2/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-367946290_1
2018-10-01 22:11:56,682 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 724 Total time for transactions(ms): 51 Number of transactions batched in Syncs: 126 Number of syncs: 597 SyncTimes(ms): 17404 
2018-10-01 22:11:56,761 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742033_1209, replicas=127.0.0.1:50010 for /user/output3/_temporary/0/_temporary/attempt_local265685709_0001_r_000000_0/part-r-00000
2018-10-01 22:11:57,544 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output3/_temporary/0/_temporary/attempt_local265685709_0001_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_-1923203303_1
2018-10-01 22:12:01,941 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742034_1210, replicas=127.0.0.1:50010 for /user/output3/_temporary/0/_temporary/attempt_local265685709_0001_r_000001_0/part-r-00001
2018-10-01 22:12:02,390 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073742034_1210 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/output3/_temporary/0/_temporary/attempt_local265685709_0001_r_000001_0/part-r-00001
2018-10-01 22:12:02,791 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output3/_temporary/0/_temporary/attempt_local265685709_0001_r_000001_0/part-r-00001 is closed by DFSClient_NONMAPREDUCE_-1923203303_1
2018-10-01 22:12:07,859 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742035_1211, replicas=127.0.0.1:50010 for /user/output3/_temporary/0/_temporary/attempt_local265685709_0001_r_000002_0/part-r-00002
2018-10-01 22:12:08,968 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output3/_temporary/0/_temporary/attempt_local265685709_0001_r_000002_0/part-r-00002 is closed by DFSClient_NONMAPREDUCE_-1923203303_1
2018-10-01 22:12:09,985 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/output3/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1923203303_1
2018-10-01 22:52:23,839 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2018-10-01 22:52:23,839 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2018-10-01 22:52:23,839 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 561, 1310
2018-10-01 22:52:23,839 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 751 Total time for transactions(ms): 51 Number of transactions batched in Syncs: 132 Number of syncs: 619 SyncTimes(ms): 21148 
2018-10-01 22:52:23,841 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 751 Total time for transactions(ms): 51 Number of transactions batched in Syncs: 132 Number of syncs: 620 SyncTimes(ms): 21150 
2018-10-01 22:52:23,843 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-lag/dfs/name/current/edits_inprogress_0000000000000000561 -> /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000561-0000000000000001311
2018-10-01 22:52:23,843 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1312
2018-10-01 22:52:23,929 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-lag/dfs/name/current/edits_0000000000000000561-0000000000000001311, fileSize: 71213. Sent total: 71213 bytes. Size of last segment intended to send: -1 bytes.
2018-10-01 22:52:24,040 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Combined time for fsimage download and fsync to all disks took 0.00s. The fsimage download took 0.00s at 15000.00 KB/s. Synchronous (fsync) write to disk of /tmp/hadoop-lag/dfs/name/current/fsimage.ckpt_0000000000000001311 took 0.00s.
2018-10-01 22:52:24,040 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000001311 size 16352 bytes.
2018-10-01 22:52:24,048 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 560
2018-10-01 22:52:24,049 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-lag/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2018-10-01 23:43:17,704 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2018-10-01 23:43:17,733 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at lag-Predator-G3-571/127.0.1.1
************************************************************/
